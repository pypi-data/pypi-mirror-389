"""
é›†æˆå­¦ä¹ æ–¹æ³•æ¨¡å—
åŒ…å«éšæœºæ£®æ—å’Œæ¢¯åº¦æå‡æ ‘å›å½’ç®—æ³•
"""

import numpy as np
from typing import List, Optional
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

from .ml_models import RandomForestResult, GradientBoostingResult


def random_forest_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    n_estimators: int = 100,
    max_depth: Optional[int] = None,
    random_state: int = 42
) -> RandomForestResult:
    """
    éšæœºæ£®æ—å›å½’
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    ä½¿ç”¨éšæœºæ£®æ—ç®—æ³•è¿›è¡Œå›å½’åˆ†æï¼Œé€‚ç”¨äºéçº¿æ€§å…³ç³»å’Œå¤æ‚äº¤äº’æ•ˆåº”ã€‚
    
    ğŸ“ˆ ç®—æ³•ç‰¹ç‚¹ï¼š
    - é›†æˆå­¦ä¹ ï¼šå¤šä¸ªå†³ç­–æ ‘çš„ç»„åˆ
    - æŠ—è¿‡æ‹Ÿåˆï¼šé€šè¿‡è¢‹å¤–æ ·æœ¬å’Œç‰¹å¾éšæœºé€‰æ‹©
    - éçº¿æ€§å»ºæ¨¡ï¼šèƒ½å¤Ÿæ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»
    - ç‰¹å¾é‡è¦æ€§ï¼šæä¾›ç‰¹å¾é‡è¦æ€§æ’åº
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - å¤æ‚éçº¿æ€§å…³ç³»å»ºæ¨¡
    - ç‰¹å¾é‡è¦æ€§åˆ†æ
    - é«˜ç»´æ•°æ®å›å½’
    - ç¨³å¥é¢„æµ‹å»ºæ¨¡
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - è®¡ç®—å¤æ‚åº¦è¾ƒé«˜
    - éœ€è¦è°ƒæ•´è¶…å‚æ•°ï¼ˆn_estimators, max_depthï¼‰
    - å¯¹å¼‚å¸¸å€¼ç›¸å¯¹ç¨³å¥
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        n_estimators: æ ‘çš„æ•°é‡ï¼Œé»˜è®¤100
        max_depth: æœ€å¤§æ·±åº¦ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        random_state: éšæœºç§å­
    
    Returns:
        RandomForestResult: éšæœºæ£®æ—å›å½’ç»“æœ
    """
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # ç‰¹å¾åç§°å¤„ç†
    if feature_names is None:
        feature_names = [f"x{i}" for i in range(X.shape[1])]
    elif len(feature_names) != X.shape[1]:
        raise ValueError(f"ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸è‡ªå˜é‡æ•°é‡({X.shape[1]})ä¸åŒ¹é…")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    rf_model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        oob_score=True
    )
    rf_model.fit(X_scaled, y)
    
    # é¢„æµ‹
    y_pred = rf_model.predict(X_scaled)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(feature_names, rf_model.feature_importances_))
    
    return RandomForestResult(
        model_type="random_forest",
        r2_score=r2,
        mse=mse,
        mae=mae,
        n_obs=len(y),
        feature_names=feature_names,
        feature_importance=feature_importance,
        n_estimators=n_estimators,
        max_depth=max_depth if max_depth is not None else 0,  # 0è¡¨ç¤ºæ— é™åˆ¶
        oob_score=rf_model.oob_score_ if hasattr(rf_model, 'oob_score_') else None
    )


def gradient_boosting_regression(
    y_data: List[float],
    x_data: List[List[float]],
    feature_names: Optional[List[str]] = None,
    n_estimators: int = 100,
    learning_rate: float = 0.1,
    max_depth: int = 3,
    random_state: int = 42
) -> GradientBoostingResult:
    """
    æ¢¯åº¦æå‡æ ‘å›å½’
    
    ğŸ“Š åŠŸèƒ½è¯´æ˜ï¼š
    ä½¿ç”¨æ¢¯åº¦æå‡ç®—æ³•è¿›è¡Œå›å½’åˆ†æï¼Œé€šè¿‡é€æ­¥ä¼˜åŒ–æ®‹å·®æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚
    
    ğŸ“ˆ ç®—æ³•ç‰¹ç‚¹ï¼š
    - é€æ­¥ä¼˜åŒ–ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™é€æ­¥æ”¹è¿›æ¨¡å‹
    - é«˜ç²¾åº¦ï¼šé€šå¸¸æ¯”éšæœºæ£®æ—æœ‰æ›´å¥½çš„é¢„æµ‹ç²¾åº¦
    - æ­£åˆ™åŒ–ï¼šé€šè¿‡å­¦ä¹ ç‡å’Œæ ‘æ·±åº¦æ§åˆ¶è¿‡æ‹Ÿåˆ
    - ç‰¹å¾é‡è¦æ€§ï¼šæä¾›ç‰¹å¾é‡è¦æ€§æ’åº
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯ï¼š
    - é«˜ç²¾åº¦é¢„æµ‹éœ€æ±‚
    - ç»“æ„åŒ–æ•°æ®å»ºæ¨¡
    - ç«èµ›å’Œå®é™…åº”ç”¨
    - éœ€è¦ç²¾ç»†è°ƒä¼˜çš„åœºæ™¯
    
    âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
    - å¯¹è¶…å‚æ•°æ•æ„Ÿ
    - è®­ç»ƒæ—¶é—´è¾ƒé•¿
    - å®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆéœ€è¦ä»”ç»†è°ƒå‚ï¼‰
    
    Args:
        y_data: å› å˜é‡æ•°æ®
        x_data: è‡ªå˜é‡æ•°æ®ï¼ŒäºŒç»´åˆ—è¡¨æ ¼å¼
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        n_estimators: æ ‘çš„æ•°é‡ï¼Œé»˜è®¤100
        learning_rate: å­¦ä¹ ç‡ï¼Œé»˜è®¤0.1
        max_depth: æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤3
        random_state: éšæœºç§å­
    
    Returns:
        GradientBoostingResult: æ¢¯åº¦æå‡æ ‘å›å½’ç»“æœ
    """
    # æ•°æ®éªŒè¯
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    if len(y_data) != len(x_data):
        raise ValueError(f"å› å˜é‡å’Œè‡ªå˜é‡çš„è§‚æµ‹æ•°é‡ä¸ä¸€è‡´: y_data={len(y_data)}, x_data={len(x_data)}")
    
    # å‡†å¤‡æ•°æ®
    X = np.array(x_data)
    y = np.array(y_data)
    
    # ç‰¹å¾åç§°å¤„ç†
    if feature_names is None:
        feature_names = [f"x{i}" for i in range(X.shape[1])]
    elif len(feature_names) != X.shape[1]:
        raise ValueError(f"ç‰¹å¾åç§°æ•°é‡({len(feature_names)})ä¸è‡ªå˜é‡æ•°é‡({X.shape[1]})ä¸åŒ¹é…")
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # è®­ç»ƒæ¢¯åº¦æå‡æ ‘æ¨¡å‹
    gb_model = GradientBoostingRegressor(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        random_state=random_state
    )
    gb_model.fit(X_scaled, y)
    
    # é¢„æµ‹
    y_pred = gb_model.predict(X_scaled)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(feature_names, gb_model.feature_importances_))
    
    return GradientBoostingResult(
        model_type="gradient_boosting",
        r2_score=r2,
        mse=mse,
        mae=mae,
        n_obs=len(y),
        feature_names=feature_names,
        feature_importance=feature_importance,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth
    )