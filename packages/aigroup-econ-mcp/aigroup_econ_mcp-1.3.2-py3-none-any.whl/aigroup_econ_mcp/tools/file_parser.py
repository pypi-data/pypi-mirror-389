"""
æ–‡ä»¶è§£æä¸è¾“å…¥å¤„ç†æ¨¡å—
æ•´åˆäº†æ–‡ä»¶è§£æã€æ•°æ®è½¬æ¢å’Œè¾“å…¥å¤„ç†åŠŸèƒ½
"""

import json
import csv
from typing import Dict, List, Any, Union, Tuple, Optional, Callable
from pathlib import Path
from functools import wraps
import io
import base64


class FileParser:
    """æ–‡ä»¶è§£æå™¨ï¼Œæ”¯æŒCSVã€JSONå’ŒTXTæ ¼å¼"""
    
    @staticmethod
    def parse_file_path(
        file_path: str,
        file_format: str = "auto"
    ) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶è·¯å¾„è§£ææ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰
            file_format: æ–‡ä»¶æ ¼å¼ ("csv", "json", "auto")
        
        Returns:
            è§£æåçš„æ•°æ®å­—å…¸
        """
        path = Path(file_path)
        
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not path.is_file():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}")
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆåŸºäºæ–‡ä»¶æ‰©å±•åï¼‰
        if file_format == "auto":
            ext = path.suffix.lower()
            if ext == '.csv':
                file_format = "csv"
            elif ext in ['.json', '.jsonl']:
                file_format = "json"
            elif ext == '.txt':
                file_format = 'txt'
            else:
                # å°è¯•ä»å†…å®¹æ£€æµ‹
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return FileParser.parse_file_content(content, "auto")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return FileParser.parse_file_content(content, file_format)
    
    @staticmethod
    def parse_file_content(
        content: str,
        file_format: str = "auto"
    ) -> Dict[str, Any]:
        """
        è§£ææ–‡ä»¶å†…å®¹
        
        Args:
            content: æ–‡ä»¶å†…å®¹ï¼ˆbase64ç¼–ç çš„å­—ç¬¦ä¸²æˆ–ç›´æ¥æ–‡æœ¬ï¼‰
            file_format: æ–‡ä»¶æ ¼å¼ ("csv", "json", "auto")
        
        Returns:
            è§£æåçš„æ•°æ®å­—å…¸ï¼ŒåŒ…å«ï¼š
            - data: æ•°æ®å†…å®¹
            - variables: å˜é‡ååˆ—è¡¨
            - format: æ£€æµ‹åˆ°çš„æ ¼å¼
            - data_type: æ•°æ®ç±»å‹ï¼ˆ'univariate', 'multivariate', 'time_series', 'panel'ï¼‰
        """
        # å°è¯•æ£€æµ‹æ˜¯å¦ä¸ºbase64ç¼–ç 
        try:
            decoded_content = base64.b64decode(content).decode('utf-8')
        except:
            decoded_content = content
        
        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
        if file_format == "auto":
            file_format = FileParser._detect_format(decoded_content)
        
        if file_format == "csv":
            return FileParser._parse_csv(decoded_content)
        elif file_format == "txt":
            return FileParser._parse_txt(decoded_content)
        elif file_format == "json":
            return FileParser._parse_json(decoded_content)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_format}")
    
    @staticmethod
    def _detect_format(content: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        # å°è¯•è§£æJSON
        try:
            json.loads(content.strip())
            return "json"
        except:
            pass
        
        # æ£€æµ‹CSVç‰¹å¾

        # æ£€æµ‹TXTç‰¹å¾
        lines = content.strip().split('\n')
        if lines:
            first_line = lines[0].strip()
            # æ£€æŸ¥æ˜¯å¦ä¸ºé”®å€¼å¯¹æ ¼å¼
            if ':' in first_line or '=' in first_line:
                return "txt"
            
            # å°è¯•è§£æä¸ºçº¯æ•°å€¼
            try:
                float(first_line)
                return "txt"
            except ValueError:
                # å°è¯•æŒ‰ç©ºæ ¼æ‹†åˆ†å¹¶æ£€æŸ¥
                parts = first_line.split()
                if parts:
                    try:
                        for part in parts:
                            float(part)
                        return "txt"
                    except ValueError:
                        pass
        
        if ',' in content or '\t' in content:
            return "csv"
        
        raise ValueError("æ— æ³•è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼Œè¯·æ˜ç¡®æŒ‡å®š")
    
    @staticmethod
    def _parse_csv(content: str) -> Dict[str, Any]:
        """
        è§£æCSVæ–‡ä»¶
        
        æ”¯æŒçš„æ ¼å¼ï¼š
        1. å¸¦è¡¨å¤´çš„åˆ—æ•°æ®
        2. æ— è¡¨å¤´çš„çº¯æ•°å€¼æ•°æ®
        """
        lines = content.strip().split('\n')
        if not lines:
            raise ValueError("CSVæ–‡ä»¶ä¸ºç©º")
        
        # æ£€æµ‹åˆ†éš”ç¬¦
        delimiter = FileParser._detect_delimiter(lines[0])
        
        # ä½¿ç”¨csv.readerè§£æ
        reader = csv.reader(io.StringIO(content), delimiter=delimiter)
        rows = list(reader)
        
        if not rows:
            raise ValueError("CSVæ–‡ä»¶æ²¡æœ‰æ•°æ®")
        
        # æ£€æµ‹æ˜¯å¦æœ‰è¡¨å¤´
        has_header = FileParser._has_header(rows)
        
        if has_header:
            headers = rows[0]
            data_rows = rows[1:]
        else:
            # è‡ªåŠ¨ç”Ÿæˆåˆ—å
            headers = [f"var{i+1}" for i in range(len(rows[0]))]
            data_rows = rows
        
        # è½¬æ¢ä¸ºæ•°å€¼æ•°æ®
        parsed_data = {}
        for i, header in enumerate(headers):
            column_data = []
            for row in data_rows:
                if i < len(row):
                    try:
                        # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                        value = float(row[i].strip())
                        column_data.append(value)
                    except ValueError:
                        # å¦‚æœæ— æ³•è½¬æ¢ï¼Œä¿ç•™åŸå­—ç¬¦ä¸²ï¼ˆç”¨äºIDåˆ—ï¼‰
                        column_data.append(row[i].strip())
            
            if column_data:  # åªä¿ç•™æœ‰æ•°æ®çš„åˆ—
                parsed_data[header.strip()] = column_data
        
        if not parsed_data:
            raise ValueError("CSVæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
        
        # æ£€æµ‹æ•°æ®ç±»å‹
        data_type = FileParser._detect_data_type(parsed_data)
        
        return {
            "data": parsed_data,
            "variables": list(parsed_data.keys()),
            "format": "csv",
            "data_type": data_type,
            "n_variables": len(parsed_data),
            "n_observations": len(next(iter(parsed_data.values())))
        }
    
    @staticmethod
    def _parse_json(content: str) -> Dict[str, Any]:
        """
        è§£æJSONæ–‡ä»¶
        
        æ”¯æŒçš„æ ¼å¼ï¼š
        1. {"å˜é‡å": [æ•°æ®åˆ—è¡¨], ...}
        2. [{"å˜é‡1": å€¼, "å˜é‡2": å€¼, ...}, ...]
        3. {"data": {...}, "metadata": {...}}
        """
        try:
            json_data = json.loads(content)
        except json.JSONDecodeError as e:
            raise ValueError(f"JSONæ ¼å¼é”™è¯¯: {str(e)}")
        
        # æ ¼å¼1: ç›´æ¥çš„å˜é‡-æ•°æ®å­—å…¸
        if isinstance(json_data, dict) and all(
            isinstance(v, list) for v in json_data.values()
        ):
            # ä¿ç•™æ‰€æœ‰åˆ—ï¼ˆåŒ…æ‹¬å­—ç¬¦ä¸²ç±»å‹çš„IDå’Œæ—¶é—´åˆ—ï¼‰
            parsed_data = {}
            for key, values in json_data.items():
                if key.lower() in ['metadata', 'info', 'description']:
                    continue  # è·³è¿‡å…ƒæ•°æ®å­—æ®µ
                
                # æ™ºèƒ½è½¬æ¢ï¼šå°è¯•è½¬æ•°å€¼ï¼Œå¤±è´¥åˆ™ä¿ç•™åŸå§‹ç±»å‹
                converted_values = []
                for v in values:
                    try:
                        # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                        converted_values.append(float(v))
                    except (ValueError, TypeError):
                        # æ— æ³•è½¬æ¢åˆ™ä¿ç•™åŸå§‹å€¼ï¼ˆå­—ç¬¦ä¸²ç­‰ï¼‰
                        converted_values.append(v)
                
                parsed_data[key] = converted_values
            
            if parsed_data:
                data_type = FileParser._detect_data_type(parsed_data)
                return {
                    "data": parsed_data,
                    "variables": list(parsed_data.keys()),
                    "format": "json",
                    "data_type": data_type,
                    "n_variables": len(parsed_data),
                    "n_observations": len(next(iter(parsed_data.values())))
                }
        
        # æ ¼å¼2: è®°å½•æ•°ç»„æ ¼å¼
        elif isinstance(json_data, list) and json_data and isinstance(json_data[0], dict):
            # è½¬æ¢ä¸ºå˜é‡-æ•°æ®å­—å…¸ï¼Œä¿ç•™å­—ç¬¦ä¸²ç±»å‹
            parsed_data = {}
            for record in json_data:
                for key, value in record.items():
                    if key not in parsed_data:
                        parsed_data[key] = []
                    # æ™ºèƒ½è½¬æ¢ï¼šå°è¯•è½¬æ•°å€¼ï¼Œå¤±è´¥åˆ™ä¿ç•™åŸå§‹ç±»å‹
                    try:
                        parsed_data[key].append(float(value))
                    except (ValueError, TypeError):
                        # ä¿ç•™åŸå§‹å€¼ï¼ˆå­—ç¬¦ä¸²ç­‰ï¼‰
                        parsed_data[key].append(value)
            
            if parsed_data:
                data_type = FileParser._detect_data_type(parsed_data)
                return {
                    "data": parsed_data,
                    "variables": list(parsed_data.keys()),
                    "format": "json",
                    "data_type": data_type,
                    "n_variables": len(parsed_data),
                    "n_observations": len(next(iter(parsed_data.values())))
                }
        
        # æ ¼å¼3: åŒ…å«dataå­—æ®µçš„ç»“æ„
        elif isinstance(json_data, dict) and "data" in json_data:
            return FileParser._parse_json(json.dumps(json_data["data"]))
        
        raise ValueError("ä¸æ”¯æŒçš„JSONæ•°æ®æ ¼å¼")

    
    @staticmethod
    def _parse_txt(content: str) -> Dict[str, Any]:
        """
        è§£æTXTæ–‡ä»¶
        
        æ”¯æŒçš„æ ¼å¼ï¼š
        1. å•åˆ—æ•°å€¼ï¼ˆæ¯è¡Œä¸€ä¸ªæ•°å€¼ï¼‰
        2. ç©ºæ ¼/åˆ¶è¡¨ç¬¦åˆ†éš”çš„å¤šåˆ—æ•°å€¼
        3. å¸¦æ ‡æ³¨çš„é”®å€¼å¯¹ï¼ˆå˜é‡å: æ•°å€¼ æˆ– å˜é‡å æ•°å€¼ï¼‰
        """
        lines = [line.strip() for line in content.strip().split('\n') if line.strip()]
        if not lines:
            raise ValueError("TXTæ–‡ä»¶ä¸ºç©º")
        
        # æ£€æµ‹æ ¼å¼ç±»å‹
        first_line = lines[0]
        
        # æ ¼å¼1: æ£€æµ‹æ˜¯å¦ä¸ºé”®å€¼å¯¹æ ¼å¼ï¼ˆåŒ…å«å†’å·æˆ–ç­‰å·ï¼‰
        if ':' in first_line or '=' in first_line:
            return FileParser._parse_txt_keyvalue(lines)
        
        # æ ¼å¼2: æ£€æµ‹æ˜¯å¦åŒ…å«ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ï¼ˆå¤šåˆ—æ•°æ®ï¼‰
        if ' ' in first_line or '\t' in first_line:
            return FileParser._parse_txt_multicolumn(lines)
        
        # æ ¼å¼3: å•åˆ—æ•°å€¼
        return FileParser._parse_txt_single_column(lines)
    
    @staticmethod
    def _parse_txt_single_column(lines: List[str]) -> Dict[str, Any]:
        """è§£æå•åˆ—TXTæ•°æ®ï¼ˆæ¯è¡Œä¸€ä¸ªæ•°å€¼ï¼‰"""
        data_list = []
        for i, line in enumerate(lines, 1):
            try:
                value = float(line)
                data_list.append(value)
            except ValueError:
                raise ValueError(f"ç¬¬{i}è¡Œæ— æ³•è§£æä¸ºæ•°å€¼: {line}")
        
        if not data_list:
            raise ValueError("TXTæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼æ•°æ®")
        
        parsed_data = {"data": data_list}
        
        return {
            "data": parsed_data,
            "variables": ["data"],
            "format": "txt",
            "data_type": "univariate",
            "n_variables": 1,
            "n_observations": len(data_list)
        }
    
    @staticmethod
    def _parse_txt_multicolumn(lines: List[str]) -> Dict[str, Any]:
        """è§£æå¤šåˆ—TXTæ•°æ®ï¼ˆç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦åˆ†éš”ï¼‰"""
        # æ£€æµ‹åˆ†éš”ç¬¦
        first_line = lines[0]
        if '\t' in first_line:
            delimiter = '\t'
        else:
            delimiter = None  # ä½¿ç”¨split()é»˜è®¤è¡Œä¸º
        
        # è§£ææ‰€æœ‰è¡Œ
        all_rows = []
        for line in lines:
            if delimiter:
                parts = line.split(delimiter)
            else:
                parts = line.split()
            all_rows.append([p.strip() for p in parts if p.strip()])
        
        if not all_rows:
            raise ValueError("TXTæ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®")
        
        # æ£€æµ‹æ˜¯å¦æœ‰è¡¨å¤´
        has_header = False
        first_row = all_rows[0]
        for cell in first_row:
            try:
                float(cell)
            except ValueError:
                has_header = True
                break
        
        if has_header:
            headers = first_row
            data_rows = all_rows[1:]
        else:
            n_cols = len(first_row)
            headers = [f"var{i+1}" for i in range(n_cols)]
            data_rows = all_rows
        
        # è½¬æ¢ä¸ºæ•°å€¼æ•°æ®
        parsed_data = {}
        for i, header in enumerate(headers):
            column_data = []
            for row in data_rows:
                if i < len(row):
                    try:
                        value = float(row[i])
                        column_data.append(value)
                    except ValueError:
                        column_data.append(row[i])
            
            if column_data:
                parsed_data[header] = column_data
        
        if not parsed_data:
            raise ValueError("TXTæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
        
        data_type = FileParser._detect_data_type(parsed_data)
        
        return {
            "data": parsed_data,
            "variables": list(parsed_data.keys()),
            "format": "txt",
            "data_type": data_type,
            "n_variables": len(parsed_data),
            "n_observations": len(next(iter(parsed_data.values())))
        }
    
    @staticmethod
    def _parse_txt_keyvalue(lines: List[str]) -> Dict[str, Any]:
        """è§£æé”®å€¼å¯¹æ ¼å¼çš„TXTæ•°æ®"""
        parsed_data = {}
        
        for line in lines:
            if ':' in line:
                separator = ':'
            elif '=' in line:
                separator = '='
            else:
                continue
            
            parts = line.split(separator, 1)
            if len(parts) != 2:
                continue
            
            var_name = parts[0].strip()
            value_str = parts[1].strip()
            values = value_str.split()
            
            try:
                if len(values) == 1:
                    parsed_data[var_name] = [float(values[0])]
                else:
                    parsed_data[var_name] = [float(v) for v in values]
            except ValueError:
                if len(values) == 1:
                    parsed_data[var_name] = [values[0]]
                else:
                    parsed_data[var_name] = values
        
        if not parsed_data:
            raise ValueError("TXTæ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„é”®å€¼å¯¹æ•°æ®")
        
        data_type = FileParser._detect_data_type(parsed_data)
        n_observations = max(len(v) for v in parsed_data.values())
        
        return {
            "data": parsed_data,
            "variables": list(parsed_data.keys()),
            "format": "txt",
            "data_type": data_type,
            "n_variables": len(parsed_data),
            "n_observations": n_observations
        }
    
    @staticmethod
    def _detect_delimiter(line: str) -> str:
        """æ£€æµ‹CSVåˆ†éš”ç¬¦"""
        # å¸¸è§åˆ†éš”ç¬¦
        delimiters = [',', '\t', ';', '|']
        counts = {d: line.count(d) for d in delimiters}
        # è¿”å›å‡ºç°æ¬¡æ•°æœ€å¤šçš„åˆ†éš”ç¬¦
        return max(counts.items(), key=lambda x: x[1])[0]
    
    @staticmethod
    def _has_header(rows: List[List[str]]) -> bool:
        """æ£€æµ‹CSVæ˜¯å¦æœ‰è¡¨å¤´"""
        if len(rows) < 2:
            return False
        
        # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦åŒ…å«éæ•°å€¼å­—ç¬¦ä¸²
        first_row = rows[0]
        
        # å¦‚æœç¬¬ä¸€è¡Œæœ‰ä»»ä½•å…ƒç´ æ— æ³•è½¬æ¢ä¸ºæ•°å­—ï¼Œè®¤ä¸ºæœ‰è¡¨å¤´
        for cell in first_row:
            try:
                float(cell.strip())
            except ValueError:
                return True
        
        return False
    
    @staticmethod
    def _detect_data_type(data: Dict[str, List]) -> str:
        """
        æ£€æµ‹æ•°æ®ç±»å‹
        
        Returns:
            - 'univariate': å•å˜é‡
            - 'multivariate': å¤šå˜é‡
            - 'time_series': æ—¶é—´åºåˆ—ï¼ˆé€šè¿‡å˜é‡åæ¨æ–­ï¼‰
            - 'panel': é¢æ¿æ•°æ®ï¼ˆé€šè¿‡å˜é‡åæ¨æ–­ï¼‰
        """
        n_vars = len(data)
        var_names = [v.lower() for v in data.keys()]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´/æ—¥æœŸç›¸å…³çš„å˜é‡å
        time_keywords = ['time', 'date', 'year', 'month', 'day', 'period', 'quarter']
        has_time_var = any(any(kw in var for kw in time_keywords) for var in var_names)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®ä½“/IDç›¸å…³çš„å˜é‡å
        entity_keywords = ['id', 'entity', 'firm', 'company', 'country', 'region']
        has_entity_var = any(any(kw in var for kw in entity_keywords) for var in var_names)
        
        if n_vars == 1:
            return 'univariate'
        elif has_entity_var and has_time_var:
            return 'panel'
        elif has_time_var or n_vars >= 2:
            return 'time_series'
        else:
            return 'multivariate'
    
    @staticmethod
    def convert_to_tool_format(
        parsed_data: Dict[str, Any],
        tool_type: str
    ) -> Dict[str, Any]:
        """
        å°†è§£æåçš„æ•°æ®è½¬æ¢ä¸ºå·¥å…·æ‰€éœ€çš„æ ¼å¼
        
        Args:
            parsed_data: parse_file_contentè¿”å›çš„æ•°æ®
            tool_type: å·¥å…·ç±»å‹
                - 'single_var': å•å˜é‡ (List[float])
                - 'multi_var_dict': å¤šå˜é‡å­—å…¸ (Dict[str, List[float]])
                - 'multi_var_matrix': å¤šå˜é‡çŸ©é˜µ (List[List[float]])
                - 'regression': å›å½’åˆ†æ (y_data, x_data)
                - 'panel': é¢æ¿æ•°æ® (y_data, x_data, entity_ids, time_periods)
        
        Returns:
            è½¬æ¢åçš„æ•°æ®å­—å…¸
        """
        data = parsed_data["data"]
        variables = parsed_data["variables"]
        
        if tool_type == 'single_var':
            # è¿”å›ç¬¬ä¸€ä¸ªå˜é‡çš„æ•°æ®
            var_data = data[variables[0]]
            return {
                "data": var_data
            }
        
        elif tool_type == 'multi_var_dict':
            # ç›´æ¥è¿”å›å­—å…¸æ ¼å¼
            return {"data": data}
        
        elif tool_type == 'time_series':
            # æ—¶é—´åºåˆ—ç±»å‹ï¼Œä¸multi_var_dictç›¸åŒï¼Œè¿”å›å­—å…¸æ ¼å¼
            return {"data": data}
        
        elif tool_type == 'multi_var_matrix':
            # è½¬æ¢ä¸ºçŸ©é˜µæ ¼å¼ (List[List[float]])
            n_obs = len(data[variables[0]])
            matrix = []
            for i in range(n_obs):
                row = [data[var][i] for var in variables]
                matrix.append(row)
            
            return {
                "data": matrix,
                "feature_names": variables
            }
        
        elif tool_type == 'regression':
            # å‡è®¾æœ€åä¸€ä¸ªå˜é‡æ˜¯å› å˜é‡ï¼Œå…¶ä½™æ˜¯è‡ªå˜é‡
            if len(variables) < 2:
                raise ValueError("å›å½’åˆ†æè‡³å°‘éœ€è¦2ä¸ªå˜é‡ï¼ˆ1ä¸ªå› å˜é‡å’Œè‡³å°‘1ä¸ªè‡ªå˜é‡ï¼‰")
            
            y_var = variables[-1]
            x_vars = variables[:-1]
            
            y_data = data[y_var]
            n_obs = len(y_data)
            
            # æ„å»ºx_dataçŸ©é˜µ
            x_data = []
            for i in range(n_obs):
                row = [data[var][i] for var in x_vars]
                x_data.append(row)
            
            return {
                "y_data": y_data,
                "x_data": x_data,
                "feature_names": x_vars
            }
        
        elif tool_type == 'panel':
            # è¯†åˆ«å®ä½“IDã€æ—¶é—´æ ‡è¯†å’Œæ•°æ®å˜é‡
            entity_var = None
            time_var = None
            data_vars = []
            
            entity_keywords = ['id', 'entity', 'firm', 'company', 'country', 'region']
            time_keywords = ['time', 'date', 'year', 'month', 'day', 'period', 'quarter']
            
            # æ›´è¯¦ç»†çš„æ£€æµ‹é€»è¾‘
            print(f"Debug: å¼€å§‹æ£€æµ‹é¢æ¿æ•°æ®åˆ—...")
            for var in variables:
                var_lower = var.lower()
                print(f"Debug: æ£€æŸ¥å˜é‡ '{var}' (å°å†™: '{var_lower}')")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®ä½“IDåˆ—
                is_entity = any(kw in var_lower for kw in entity_keywords)
                is_time = any(kw in var_lower for kw in time_keywords)
                
                if is_entity and entity_var is None:
                    entity_var = var
                    print(f"Debug: è¯†åˆ«ä¸ºå®ä½“IDåˆ—: {var}")
                elif is_time and time_var is None:
                    time_var = var
                    print(f"Debug: è¯†åˆ«ä¸ºæ—¶é—´åˆ—: {var}")
                else:
                    data_vars.append(var)
                    print(f"Debug: è¯†åˆ«ä¸ºæ•°æ®åˆ—: {var}")
            
            print(f"Debug: entity_var={entity_var}, time_var={time_var}, data_vars={data_vars}")
            
            if not entity_var or not time_var:
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                available_vars = ', '.join(variables)
                error_msg = f"é¢æ¿æ•°æ®éœ€è¦åŒ…å«å®ä½“IDå’Œæ—¶é—´æ ‡è¯†å˜é‡ã€‚\n"
                error_msg += f"å¯ç”¨åˆ—: {available_vars}\n"
                error_msg += f"æ£€æµ‹åˆ°çš„å®ä½“IDåˆ—: {entity_var if entity_var else 'æœªæ£€æµ‹åˆ°'}\n"
                error_msg += f"æ£€æµ‹åˆ°çš„æ—¶é—´åˆ—: {time_var if time_var else 'æœªæ£€æµ‹åˆ°'}\n"
                error_msg += f"å®ä½“IDå…³é”®è¯: {entity_keywords}\n"
                error_msg += f"æ—¶é—´å…³é”®è¯: {time_keywords}"
                raise ValueError(error_msg)
            
            if len(data_vars) < 1:
                raise ValueError(f"é¢æ¿æ•°æ®è‡³å°‘éœ€è¦1ä¸ªæ•°æ®å˜é‡ã€‚å½“å‰æ•°æ®åˆ—: {data_vars}")
            
            # è½¬æ¢IDå’Œæ—¶é—´ï¼ˆä¿æŒåŸç±»å‹ï¼Œå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰
            entity_ids = [str(x) for x in data[entity_var]]
            time_periods = [str(int(x)) if isinstance(x, float) and x == int(x) else str(x) for x in data[time_var]]
            
            print(f"Debug: entity_idsæ ·æœ¬: {entity_ids[:5]}")
            print(f"Debug: time_periodsæ ·æœ¬: {time_periods[:5]}")
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ•°æ®å˜é‡ï¼Œå°†å…¶ä½œä¸ºå› å˜é‡
            if len(data_vars) == 1:
                y_var = data_vars[0]
                y_data = data[y_var]
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿè‡ªå˜é‡ï¼ˆå¸¸æ•°é¡¹ï¼‰
                n_obs = len(y_data)
                x_data = [[1.0] for _ in range(n_obs)]
                x_vars = ['const']
            else:
                # å‡è®¾æœ€åä¸€ä¸ªæ•°æ®å˜é‡æ˜¯å› å˜é‡
                y_var = data_vars[-1]
                x_vars = data_vars[:-1]
                
                y_data = data[y_var]
                n_obs = len(y_data)
                
                # æ„å»ºx_dataçŸ©é˜µ
                x_data = []
                for i in range(n_obs):
                    row = [data[var][i] for var in x_vars]
                    x_data.append(row)
            
            return {
                "y_data": y_data,
                "x_data": x_data,
                "entity_ids": entity_ids,
                "time_periods": time_periods,
                "feature_names": x_vars
            }
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å·¥å…·ç±»å‹: {tool_type}")
    
    @staticmethod
    def auto_detect_tool_params(parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è‡ªåŠ¨æ£€æµ‹å¹¶æ¨èé€‚åˆçš„å·¥å…·å‚æ•°
        
        Args:
            parsed_data: parse_file_contentè¿”å›çš„æ•°æ®
        
        Returns:
            æ¨èçš„å·¥å…·å’Œå‚æ•°
        """
        data_type = parsed_data["data_type"]
        n_vars = parsed_data["n_variables"]
        n_obs = parsed_data["n_observations"]
        
        recommendations = {
            "data_type": data_type,
            "suggested_tools": [],
            "warnings": []
        }
        
        # æ ¹æ®æ•°æ®ç±»å‹æ¨èå·¥å…·
        if data_type == 'univariate':
            recommendations["suggested_tools"] = [
                "descriptive_statistics",
                "hypothesis_testing",
                "time_series_analysis"
            ]
        elif data_type == 'multivariate':
            recommendations["suggested_tools"] = [
                "descriptive_statistics",
                "correlation_analysis",
                "ols_regression",
                "random_forest_regression_analysis",
                "lasso_regression_analysis"
            ]
        elif data_type == 'time_series':
            recommendations["suggested_tools"] = [
                "time_series_analysis",
                "var_model_analysis",
                "garch_model_analysis"
            ]
        elif data_type == 'panel':
            recommendations["suggested_tools"] = [
                "panel_fixed_effects",
                "panel_random_effects",
                "panel_hausman_test",
                "panel_unit_root_test"
            ]
        
        # æ·»åŠ è­¦å‘Š
        if n_obs < 30:
            recommendations["warnings"].append(
                f"æ ·æœ¬é‡è¾ƒå°ï¼ˆ{n_obs}ä¸ªè§‚æµ‹ï¼‰ï¼Œç»Ÿè®¡æ¨æ–­å¯èƒ½ä¸å¯é "
            )
        
        if n_vars > 10:
            recommendations["warnings"].append(
                f"å˜é‡æ•°é‡è¾ƒå¤šï¼ˆ{n_vars}ä¸ªå˜é‡ï¼‰ï¼Œå¯èƒ½éœ€è¦ç‰¹å¾é€‰æ‹©"
            )
        
        if n_vars > n_obs / 10:
            recommendations["warnings"].append(
                "å˜é‡æ•°é‡æ¥è¿‘æ ·æœ¬é‡çš„1/10ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆé£é™©"
            )
        
        return recommendations


# ============================================================================
# æ–‡ä»¶è¾“å…¥å¤„ç†ç»„ä»¶
# ============================================================================

class FileInputHandler:
    """
    æ–‡ä»¶è¾“å…¥å¤„ç†ç»„ä»¶
    
    ä½¿ç”¨ç»„ä»¶æ¨¡å¼ï¼Œä¸ºä»»ä½•å·¥å…·å‡½æ•°æ·»åŠ æ–‡ä»¶è¾“å…¥æ”¯æŒ
    """
    
    @staticmethod
    def process_input(
        file_content: Optional[str],
        file_format: str,
        tool_type: str,
        data_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶è¾“å…¥å¹¶è½¬æ¢ä¸ºå·¥å…·å‚æ•°
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹
            file_format: æ–‡ä»¶æ ¼å¼
            tool_type: å·¥å…·ç±»å‹
            data_params: å½“å‰æ•°æ®å‚æ•°
        
        Returns:
            æ›´æ–°åçš„å‚æ•°å­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æ–‡ä»¶è¾“å…¥ï¼Œç›´æ¥è¿”å›åŸå‚æ•°
        if file_content is None:
            return data_params
        
        # è§£ææ–‡ä»¶
        parsed = FileParser.parse_file_content(file_content, file_format)
        
        # è½¬æ¢ä¸ºå·¥å…·æ ¼å¼
        converted = FileParser.convert_to_tool_format(parsed, tool_type)
        
        # åˆå¹¶å‚æ•°ï¼ˆæ–‡ä»¶æ•°æ®ä¼˜å…ˆï¼‰
        result = {**data_params, **converted}
        
        return result
    
    @staticmethod
    def with_file_support(tool_type: str):
        """
        è£…é¥°å™¨ï¼šä¸ºå·¥å…·å‡½æ•°æ·»åŠ æ–‡ä»¶è¾“å…¥æ”¯æŒ
        
        Args:
            tool_type: å·¥å…·ç±»å‹ï¼ˆsingle_var, multi_var_dict, regression, panelç­‰ï¼‰
        
        Returns:
            è£…é¥°åçš„å‡½æ•°
        
        ä½¿ç”¨ç¤ºä¾‹ï¼š
            @FileInputHandler.with_file_support('regression')
            async def my_regression_tool(y_data, x_data, file_content=None, file_format='auto'):
                # å‡½æ•°ä¼šè‡ªåŠ¨å¤„ç†file_contentå¹¶å¡«å……y_dataå’Œx_data
                pass
        """
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # æå–æ–‡ä»¶ç›¸å…³å‚æ•°
                file_content = kwargs.get('file_content')
                file_format = kwargs.get('file_format', 'auto')
                
                if file_content is not None:
                    # å¤„ç†æ–‡ä»¶è¾“å…¥
                    processed = FileInputHandler.process_input(
                        file_content=file_content,
                        file_format=file_format,
                        tool_type=tool_type,
                        data_params=kwargs
                    )
                    
                    # æ›´æ–°kwargs
                    kwargs.update(processed)
                
                # è°ƒç”¨åŸå‡½æ•°
                return await func(*args, **kwargs)
            
            return wrapper
        return decorator


class FileInputMixin:
    """
    æ–‡ä»¶è¾“å…¥æ··å…¥ç±»
    
    ä¸ºç±»æä¾›æ–‡ä»¶è¾“å…¥å¤„ç†èƒ½åŠ›
    """
    
    def parse_file_input(
        self,
        file_content: Optional[str],
        file_format: str = "auto"
    ) -> Optional[Dict[str, Any]]:
        """è§£ææ–‡ä»¶è¾“å…¥"""
        if file_content is None:
            return None
        return FileParser.parse_file_content(file_content, file_format)
    
    def convert_for_tool(
        self,
        parsed_data: Dict[str, Any],
        tool_type: str
    ) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå·¥å…·æ ¼å¼"""
        return FileParser.convert_to_tool_format(parsed_data, tool_type)
    
    def get_recommendations(
        self,
        parsed_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è·å–å·¥å…·æ¨è"""
        return FileParser.auto_detect_tool_params(parsed_data)


class UnifiedFileInput:
    """
    ç»Ÿä¸€æ–‡ä»¶è¾“å…¥æ¥å£
    
    æ‰€æœ‰å·¥å…·é€šè¿‡æ­¤ç±»ç»Ÿä¸€å¤„ç†æ–‡ä»¶è¾“å…¥
    """
    
    @staticmethod
    async def handle(
        ctx: Any,
        file_content: Optional[str],
        file_format: str,
        tool_type: str,
        original_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€å¤„ç†æ–‡ä»¶è¾“å…¥
        
        Args:
            ctx: MCPä¸Šä¸‹æ–‡
            file_content: æ–‡ä»¶å†…å®¹
            file_format: æ–‡ä»¶æ ¼å¼
            tool_type: å·¥å…·ç±»å‹
            original_params: åŸå§‹å‚æ•°
        
        Returns:
            å¤„ç†åçš„å‚æ•°
        """
        if file_content is None:
            return original_params
        
        try:
            # è®°å½•æ—¥å¿—
            await ctx.info("æ£€æµ‹åˆ°æ–‡ä»¶è¾“å…¥ï¼Œå¼€å§‹è§£æ...")
            
            # è§£ææ–‡ä»¶
            parsed = FileParser.parse_file_content(file_content, file_format)
            
            # è®°å½•è§£æç»“æœ
            await ctx.info(
                f"æ–‡ä»¶è§£ææˆåŠŸï¼š{parsed['n_variables']}ä¸ªå˜é‡ï¼Œ"
                f"{parsed['n_observations']}ä¸ªè§‚æµ‹ï¼Œ"
                f"æ•°æ®ç±»å‹={parsed['data_type']}"
            )
            
            # è½¬æ¢ä¸ºå·¥å…·æ ¼å¼
            converted = FileParser.convert_to_tool_format(parsed, tool_type)
            
            # åˆå¹¶å‚æ•°
            result = {**original_params}
            result.update(converted)
            
            # è®°å½•è½¬æ¢ç»“æœ
            if tool_type == 'regression':
                await ctx.info(
                    f"æ•°æ®å·²è½¬æ¢ï¼šå› å˜é‡={converted.get('y_variable')}ï¼Œ"
                    f"è‡ªå˜é‡={converted.get('feature_names')}"
                )
            elif tool_type == 'panel':
                await ctx.info(
                    f"é¢æ¿æ•°æ®å·²è¯†åˆ«ï¼š{len(set(converted.get('entity_ids', [])))}ä¸ªå®ä½“ï¼Œ"
                    f"{len(set(converted.get('time_periods', [])))}ä¸ªæ—¶é—´ç‚¹"
                )
            else:
                await ctx.info(f"æ•°æ®å·²è½¬æ¢ä¸º{tool_type}æ ¼å¼")
            
            return result
            
        except Exception as e:
            await ctx.error(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")
            raise ValueError(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


# ============================================================================
# ä¾¿æ·å‡½æ•°å’Œå‚æ•°å®šä¹‰
# ============================================================================

def parse_file_input(
    file_content: Optional[str] = None,
    file_format: str = "auto"
) -> Optional[Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè§£ææ–‡ä»¶è¾“å…¥
    
    Args:
        file_content: æ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼‰
        file_format: æ–‡ä»¶æ ¼å¼
    
    Returns:
        è§£æåçš„æ•°æ®ï¼Œå¦‚æœfile_contentä¸ºNoneåˆ™è¿”å›None
    """
    if file_content is None:
        return None
    
    return FileParser.parse_file_content(file_content, file_format)


async def process_file_for_tool(
    ctx: Any,
    file_content: Optional[str],
    file_format: str,
    tool_type: str,
    **kwargs
) -> Dict[str, Any]:
    """
    ä¸ºå·¥å…·å¤„ç†æ–‡ä»¶è¾“å…¥çš„ä¾¿æ·å‡½æ•°
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
        params = await process_file_for_tool(
            ctx=ctx,
            file_
content=file_content,
            file_format=file_format,
            tool_type='regression',
            y_data=y_data,
            x_data=x_data,
            feature_names=feature_names
        )
        # params ç°åœ¨åŒ…å«å¤„ç†åçš„æ‰€æœ‰å‚æ•°
    """
    return await UnifiedFileInput.handle(
        ctx=ctx,
        file_content=file_content,
        file_format=file_format,
        tool_type=tool_type,
        original_params=kwargs
    )


def create_file_params(
    description: str = "CSVæˆ–JSONæ–‡ä»¶å†…å®¹"
) -> Dict[str, Any]:
    """
    åˆ›å»ºæ ‡å‡†çš„æ–‡ä»¶è¾“å…¥å‚æ•°å®šä¹‰
    
    Args:
        description: å‚æ•°æè¿°
    
    Returns:
        å‚æ•°å®šä¹‰å­—å…¸ï¼Œå¯ç›´æ¥ç”¨äºField()
    """
    return {
        "file_content": {
            "default": None,
            "description": f"""{description}
            
ğŸ“ æ”¯æŒæ ¼å¼ï¼š
- CSV: å¸¦è¡¨å¤´çš„åˆ—æ•°æ®ï¼Œè‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
- JSON: {{"å˜é‡å": [æ•°æ®], ...}} æˆ– [{{"å˜é‡1": å€¼, ...}}, ...]

ğŸ’¡ ä½¿ç”¨æ–¹å¼ï¼š
- æä¾›æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²ï¼ˆå¯ä»¥æ˜¯base64ç¼–ç ï¼‰
- ç³»ç»Ÿä¼šè‡ªåŠ¨è§£æå¹¶è¯†åˆ«å˜é‡
- ä¼˜å…ˆä½¿ç”¨file_contentï¼Œå¦‚æœæä¾›åˆ™å¿½ç•¥å…¶ä»–æ•°æ®å‚æ•°"""
        },
        "file_format": {
            "default": "auto",
            "description": """æ–‡ä»¶æ ¼å¼
            
å¯é€‰å€¼ï¼š
- "auto": è‡ªåŠ¨æ£€æµ‹ï¼ˆé»˜è®¤ï¼‰
- "csv": CSVæ ¼å¼
- "json": JSONæ ¼å¼"""
        }
    }