"""
å·¥å…·å¤„ç†å™¨æ¨¡å—
é›†ä¸­ç®¡ç†æ‰€æœ‰å·¥å…·çš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa import stattools
from scipy import stats
from typing import Dict, List, Any, Optional
from mcp.types import CallToolResult, TextContent

from .statistics import calculate_descriptive_stats, calculate_correlation_matrix, perform_hypothesis_test
from .regression import perform_ols_regression
from .panel_data import fixed_effects_model, random_effects_model, hausman_test, panel_unit_root_test
from .time_series import var_model, vecm_model, garch_model, state_space_model, variance_decomposition
from .machine_learning import (
    random_forest_regression, gradient_boosting_regression,
    lasso_regression, ridge_regression, cross_validation, feature_importance_analysis
)


async def handle_descriptive_statistics(ctx, data: Dict[str, List[float]], **kwargs) -> CallToolResult:
    """å¤„ç†æè¿°æ€§ç»Ÿè®¡"""
    if not data:
        raise ValueError("æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    df = pd.DataFrame(data)
    
    # è®¡ç®—ç»Ÿè®¡é‡
    result_data = {
        "count": len(df),
        "mean": float(df.mean().mean()),
        "std": float(df.std().mean()),
        "min": float(df.min().min()),
        "max": float(df.max().max()),
        "median": float(df.median().mean()),
        "skewness": float(df.skew().mean()),
        "kurtosis": float(df.kurtosis().mean())
    }
    
    correlation_matrix = df.corr().round(4)
    
    return CallToolResult(
        content=[
            TextContent(
                type="text",
                text=f"æè¿°æ€§ç»Ÿè®¡ç»“æœï¼š\n"
                     f"å‡å€¼: {result_data['mean']:.4f}\n"
                     f"æ ‡å‡†å·®: {result_data['std']:.4f}\n"
                     f"æœ€å°å€¼: {result_data['min']:.4f}\n"
                     f"æœ€å¤§å€¼: {result_data['max']:.4f}\n"
                     f"ä¸­ä½æ•°: {result_data['median']:.4f}\n"
                     f"ååº¦: {result_data['skewness']:.4f}\n"
                     f"å³°åº¦: {result_data['kurtosis']:.4f}\n\n"
                     f"ç›¸å…³ç³»æ•°çŸ©é˜µï¼š\n{correlation_matrix.to_string()}"
            )
        ],
        structuredContent=result_data
    )


async def handle_ols_regression(ctx, y_data: List[float], x_data: List[List[float]], 
                                feature_names: Optional[List[str]] = None, **kwargs) -> CallToolResult:
    """å¤„ç†OLSå›å½’"""
    if not y_data or not x_data:
        raise ValueError("å› å˜é‡å’Œè‡ªå˜é‡æ•°æ®ä¸èƒ½ä¸ºç©º")
    
    X = np.array(x_data)
    y = np.array(y_data)
    X_with_const = sm.add_constant(X)
    model = sm.OLS(y, X_with_const).fit()
    
    if feature_names is None:
        feature_names = [f"x{i+1}" for i in range(X.shape[1])]
    
    conf_int = model.conf_int()
    coefficients = {}
    
    for i, coef in enumerate(model.params):
        var_name = "const" if i == 0 else feature_names[i-1]
        coefficients[var_name] = {
            "coef": float(coef),
            "std_err": float(model.bse[i]),
            "t_value": float(model.tvalues[i]),
            "p_value": float(model.pvalues[i]),
            "ci_lower": float(conf_int[i][0]),
            "ci_upper": float(conf_int[i][1])
        }
    
    result_data = {
        "rsquared": float(model.rsquared),
        "rsquared_adj": float(model.rsquared_adj),
        "f_statistic": float(model.fvalue),
        "f_pvalue": float(model.f_pvalue),
        "aic": float(model.aic),
        "bic": float(model.bic),
        "coefficients": coefficients
    }
    
    return CallToolResult(
        content=[
            TextContent(
                type="text",
                text=f"OLSå›å½’åˆ†æç»“æœï¼š\n"
                     f"RÂ² = {result_data['rsquared']:.4f}\n"
                     f"è°ƒæ•´RÂ² = {result_data['rsquared_adj']:.4f}\n"
                     f"Fç»Ÿè®¡é‡ = {result_data['f_statistic']:.4f} (p = {result_data['f_pvalue']:.4f})\n"
                     f"AIC = {result_data['aic']:.2f}, BIC = {result_data['bic']:.2f}\n\n"
                     f"å›å½’ç³»æ•°ï¼š\n{model.summary().tables[1]}"
            )
        ],
        structuredContent=result_data
    )


async def handle_hypothesis_testing(ctx, data1: List[float], data2: Optional[List[float]] = None,
                                    test_type: str = "t_test", **kwargs) -> CallToolResult:
    """å¤„ç†å‡è®¾æ£€éªŒ"""
    if test_type == "t_test":
        if data2 is None:
            result = stats.ttest_1samp(data1, 0)
            ci = stats.t.interval(0.95, len(data1)-1, loc=np.mean(data1), scale=stats.sem(data1))
        else:
            result = stats.ttest_ind(data1, data2)
            ci = None
        
        test_result = {
            "test_type": test_type,
            "statistic": float(result.statistic),
            "p_value": float(result.pvalue),
            "significant": bool(result.pvalue < 0.05),
            "confidence_interval": list(ci) if ci else None
        }
    elif test_type == "adf":
        result = stattools.adfuller(data1)
        test_result = {
            "test_type": "adf",
            "statistic": float(result[0]),
            "p_value": float(result[1]),
            "significant": bool(result[1] < 0.05),
            "confidence_interval": None
        }
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ£€éªŒç±»å‹: {test_type}")
    
    ci_text = ""
    if test_result['confidence_interval']:
        ci_lower = test_result['confidence_interval'][0]
        ci_upper = test_result['confidence_interval'][1]
        ci_text = f"95%ç½®ä¿¡åŒºé—´: [{ci_lower:.4f}, {ci_upper:.4f}]"
    
    return CallToolResult(
        content=[
            TextContent(
                type="text",
                text=f"{test_type.upper()}æ£€éªŒç»“æœï¼š\n"
                     f"æ£€éªŒç»Ÿè®¡é‡ = {test_result['statistic']:.4f}\n"
                     f"på€¼ = {test_result['p_value']:.4f}\n"
                     f"{'æ˜¾è‘—' if test_result['significant'] else 'ä¸æ˜¾è‘—'} (5%æ°´å¹³)\n"
                     f"{ci_text}"
            )
        ],
        structuredContent=test_result
    )


async def handle_time_series_analysis(ctx, data: List[float], **kwargs) -> CallToolResult:
    """å¤„ç†æ—¶é—´åºåˆ—åˆ†æ - å¢å¼ºç‰ˆ"""
    if not data or len(data) < 5:
        raise ValueError("æ—¶é—´åºåˆ—æ•°æ®è‡³å°‘éœ€è¦5ä¸ªè§‚æµ‹ç‚¹")
    
    # åŸºæœ¬ç»Ÿè®¡é‡
    series = pd.Series(data)
    basic_stats = {
        "count": len(series),
        "mean": float(series.mean()),
        "std": float(series.std()),
        "min": float(series.min()),
        "max": float(series.max()),
        "median": float(series.median()),
        "skewness": float(series.skew()),
        "kurtosis": float(series.kurtosis()),
        "variance": float(series.var()),
        "range": float(series.max() - series.min()),
        "cv": float(series.std() / series.mean()) if series.mean() != 0 else 0  # å˜å¼‚ç³»æ•°
    }
    
    # å¹³ç¨³æ€§æ£€éªŒ
    adf_result = stattools.adfuller(data)
    kpss_result = stattools.kpss(data, regression='c', nlags='auto')
    
    # è‡ªç›¸å…³åˆ†æ
    max_nlags = min(20, len(data) - 1, len(data) // 2)
    if max_nlags < 1:
        max_nlags = 1
    
    try:
        acf_values = stattools.acf(data, nlags=max_nlags)
        pacf_values = stattools.pacf(data, nlags=max_nlags)
    except:
        acf_values = np.zeros(max_nlags + 1)
        pacf_values = np.zeros(max_nlags + 1)
        acf_values[0] = pacf_values[0] = 1.0
    
    # è®¡ç®—æ›´å¤šè¯Šæ–­ç»Ÿè®¡é‡
    # è¶‹åŠ¿å¼ºåº¦
    trend_strength = abs(np.corrcoef(range(len(data)), data)[0, 1]) if len(data) > 1 else 0
    
    # å­£èŠ‚æ€§æ£€æµ‹ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿé•¿ï¼‰
    seasonal_pattern = False
    if len(data) >= 12:
        try:
            # ç®€å•çš„å­£èŠ‚æ€§æ£€æµ‹ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨å‘¨æœŸæ€§æ¨¡å¼
            seasonal_acf = stattools.acf(data, nlags=min(12, len(data)//2))
            seasonal_pattern = any(abs(x) > 0.3 for x in seasonal_acf[1:])
        except:
            seasonal_pattern = False
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š æ—¶é—´åºåˆ—åˆ†æç»“æœ

ğŸ” åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼š
- è§‚æµ‹æ•°é‡ = {basic_stats['count']}
- å‡å€¼ = {basic_stats['mean']:.4f}
- æ ‡å‡†å·® = {basic_stats['std']:.4f}
- æ–¹å·® = {basic_stats['variance']:.4f}
- æœ€å°å€¼ = {basic_stats['min']:.4f}
- æœ€å¤§å€¼ = {basic_stats['max']:.4f}
- æå·® = {basic_stats['range']:.4f}
- ä¸­ä½æ•° = {basic_stats['median']:.4f}
- ååº¦ = {basic_stats['skewness']:.4f}
- å³°åº¦ = {basic_stats['kurtosis']:.4f}
- å˜å¼‚ç³»æ•° = {basic_stats['cv']:.4f}

ğŸ“ˆ å¹³ç¨³æ€§æ£€éªŒï¼š
- ADFæ£€éªŒç»Ÿè®¡é‡ = {adf_result[0]:.4f}
- ADFæ£€éªŒpå€¼ = {adf_result[1]:.4f}
- KPSSæ£€éªŒç»Ÿè®¡é‡ = {kpss_result[0]:.4f}
- KPSSæ£€éªŒpå€¼ = {kpss_result[1]:.4f}
- å¹³ç¨³æ€§åˆ¤æ–­ = {'å¹³ç¨³' if adf_result[1] < 0.05 and kpss_result[1] > 0.05 else 'éå¹³ç¨³'}

ğŸ”¬ è‡ªç›¸å…³åˆ†æï¼š
- ACFå‰5é˜¶: {[f'{x:.4f}' for x in acf_values[:5]]}
- PACFå‰5é˜¶: {[f'{x:.4f}' for x in pacf_values[:5]]}
- æœ€å¤§è‡ªç›¸å…³: {max(abs(acf_values[1:])) if len(acf_values) > 1 else 0:.4f}
- æœ€å¤§åè‡ªç›¸å…³: {max(abs(pacf_values[1:])) if len(pacf_values) > 1 else 0:.4f}

ğŸ“Š è¯Šæ–­ç»Ÿè®¡é‡ï¼š
- è¶‹åŠ¿å¼ºåº¦: {trend_strength:.4f}
- å­£èŠ‚æ€§æ¨¡å¼: {'å­˜åœ¨' if seasonal_pattern else 'æœªæ£€æµ‹åˆ°'}
- æ•°æ®æ³¢åŠ¨æ€§: {'é«˜' if basic_stats['cv'] > 0.5 else 'ä¸­ç­‰' if basic_stats['cv'] > 0.2 else 'ä½'}
- åˆ†å¸ƒå½¢æ€: {'å³å' if basic_stats['skewness'] > 0.5 else 'å·¦å' if basic_stats['skewness'] < -0.5 else 'è¿‘ä¼¼å¯¹ç§°'}
- å³°åº¦ç±»å‹: {'å°–å³°' if basic_stats['kurtosis'] > 3 else 'ä½å³°' if basic_stats['kurtosis'] < 3 else 'æ­£æ€'}"""

    # è¯¦ç»†çš„æ¨¡å‹å»ºè®®
    result_text += f"\n\nğŸ’¡ è¯¦ç»†æ¨¡å‹å»ºè®®ï¼š"
    
    if adf_result[1] < 0.05:  # å¹³ç¨³åºåˆ—
        result_text += f"\n- æ•°æ®ä¸ºå¹³ç¨³åºåˆ—ï¼Œå¯ç›´æ¥å»ºæ¨¡"
        
        # æ ¹æ®ACF/PACFæ¨¡å¼ç»™å‡ºè¯¦ç»†å»ºè®®
        acf_decay = abs(acf_values[1]) > 0.5
        pacf_cutoff = abs(pacf_values[1]) > 0.5 and all(abs(x) < 0.3 for x in pacf_values[2:5])
        
        if acf_decay and pacf_cutoff:
            result_text += f"\n- ACFç¼“æ…¢è¡°å‡ï¼ŒPACFåœ¨1é˜¶æˆªå°¾ï¼Œå»ºè®®å°è¯•AR(1)æ¨¡å‹"
            result_text += f"\n- å¯è€ƒè™‘ARMA(1,1)ä½œä¸ºå¤‡é€‰æ¨¡å‹"
        elif not acf_decay and pacf_cutoff:
            result_text += f"\n- ACFå¿«é€Ÿè¡°å‡ï¼ŒPACFæˆªå°¾ï¼Œå»ºè®®å°è¯•MAæ¨¡å‹"
        elif acf_decay and not pacf_cutoff:
            result_text += f"\n- ACFç¼“æ…¢è¡°å‡ï¼ŒPACFæ— æˆªå°¾ï¼Œå»ºè®®å°è¯•ARæ¨¡å‹"
        else:
            result_text += f"\n- ACFå’ŒPACFå‡ç¼“æ…¢è¡°å‡ï¼Œå»ºè®®å°è¯•ARMAæ¨¡å‹"
            
        # æ ¹æ®æ•°æ®ç‰¹å¾ç»™å‡ºé¢å¤–å»ºè®®
        if seasonal_pattern:
            result_text += f"\n- æ£€æµ‹åˆ°å­£èŠ‚æ€§æ¨¡å¼ï¼Œå¯è€ƒè™‘SARIMAæ¨¡å‹"
        if trend_strength > 0.7:
            result_text += f"\n- å¼ºè¶‹åŠ¿æ¨¡å¼ï¼Œå¯è€ƒè™‘å¸¦è¶‹åŠ¿é¡¹çš„æ¨¡å‹"
            
    else:  # éå¹³ç¨³åºåˆ—
        result_text += f"\n- æ•°æ®ä¸ºéå¹³ç¨³åºåˆ—ï¼Œå»ºè®®è¿›è¡Œå·®åˆ†å¤„ç†"
        result_text += f"\n- å¯å°è¯•ARIMA(p,d,q)æ¨¡å‹ï¼Œå…¶ä¸­dä¸ºå·®åˆ†é˜¶æ•°"
        
        # æ ¹æ®è¶‹åŠ¿å¼ºåº¦å»ºè®®å·®åˆ†é˜¶æ•°
        if trend_strength > 0.8:
            result_text += f"\n- å¼ºè¶‹åŠ¿ï¼Œå»ºè®®å°è¯•1-2é˜¶å·®åˆ†"
        elif trend_strength > 0.5:
            result_text += f"\n- ä¸­ç­‰è¶‹åŠ¿ï¼Œå»ºè®®å°è¯•1é˜¶å·®åˆ†"
        else:
            result_text += f"\n- å¼±è¶‹åŠ¿ï¼Œå¯å°è¯•1é˜¶å·®åˆ†"
            
        if seasonal_pattern:
            result_text += f"\n- æ£€æµ‹åˆ°å­£èŠ‚æ€§æ¨¡å¼ï¼Œå¯è€ƒè™‘SARIMAæ¨¡å‹"
    
    # æ ¹æ®æ•°æ®é•¿åº¦ç»™å‡ºå»ºè®®
    if len(data) < 30:
        result_text += f"\n- æ•°æ®é‡è¾ƒå°‘({len(data)}ä¸ªè§‚æµ‹ç‚¹)ï¼Œå»ºè®®è°¨æ…è§£é‡Šç»“æœ"
    elif len(data) < 100:
        result_text += f"\n- æ•°æ®é‡é€‚ä¸­({len(data)}ä¸ªè§‚æµ‹ç‚¹)ï¼Œé€‚åˆå¤§å¤šæ•°æ—¶é—´åºåˆ—æ¨¡å‹"
    else:
        result_text += f"\n- æ•°æ®é‡å……è¶³({len(data)}ä¸ªè§‚æµ‹ç‚¹)ï¼Œå¯è€ƒè™‘å¤æ‚æ¨¡å‹"
    
    result_text += f"\n\nâš ï¸ å»ºæ¨¡æ³¨æ„äº‹é¡¹ï¼š"
    result_text += f"\n- å¹³ç¨³æ€§æ˜¯æ—¶é—´åºåˆ—å»ºæ¨¡çš„é‡è¦å‰æ"
    result_text += f"\n- ACFå’ŒPACFæ¨¡å¼æœ‰åŠ©äºè¯†åˆ«åˆé€‚çš„æ¨¡å‹é˜¶æ•°"
    result_text += f"\n- å»ºè®®ç»“åˆä¿¡æ¯å‡†åˆ™ï¼ˆAIC/BICï¼‰è¿›è¡Œæ¨¡å‹é€‰æ‹©"
    result_text += f"\n- æ¨¡å‹è¯Šæ–­ï¼šæ£€æŸ¥æ®‹å·®çš„è‡ªç›¸å…³æ€§å’Œæ­£æ€æ€§"
    result_text += f"\n- æ¨¡å‹éªŒè¯ï¼šä½¿ç”¨æ ·æœ¬å¤–æ•°æ®è¿›è¡Œé¢„æµ‹éªŒè¯"
    result_text += f"\n- å‚æ•°ç¨³å®šæ€§ï¼šç¡®ä¿æ¨¡å‹å‚æ•°åœ¨æ•´ä¸ªæ ·æœ¬æœŸå†…ç¨³å®š"
    
    result_data = {
        "basic_statistics": basic_stats,
        "adf_statistic": float(adf_result[0]),
        "adf_pvalue": float(adf_result[1]),
        "kpss_statistic": float(kpss_result[0]),
        "kpss_pvalue": float(kpss_result[1]),
        "stationary": bool(adf_result[1] < 0.05 and kpss_result[1] > 0.05),
        "acf": [float(x) for x in acf_values.tolist()],
        "pacf": [float(x) for x in pacf_values.tolist()],
        "diagnostic_stats": {
            "trend_strength": trend_strength,
            "seasonal_pattern": seasonal_pattern,
            "volatility_level": "high" if basic_stats['cv'] > 0.5 else "medium" if basic_stats['cv'] > 0.2 else "low",
            "distribution_shape": "right_skewed" if basic_stats['skewness'] > 0.5 else "left_skewed" if basic_stats['skewness'] < -0.5 else "symmetric",
            "kurtosis_type": "leptokurtic" if basic_stats['kurtosis'] > 3 else "platykurtic" if basic_stats['kurtosis'] < 3 else "mesokurtic"
        },
        "model_suggestions": {
            "is_stationary": adf_result[1] < 0.05,
            "suggested_models": ["ARMA", "ARIMA"] if adf_result[1] < 0.05 else ["ARIMA", "SARIMA"],
            "data_sufficiency": "low" if len(data) < 30 else "medium" if len(data) < 100 else "high",
            "trend_recommendation": "strong_diff" if trend_strength > 0.8 else "moderate_diff" if trend_strength > 0.5 else "weak_diff",
            "seasonal_recommendation": "consider_seasonal" if seasonal_pattern else "no_seasonal"
        }
    }
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result_data
    )


async def handle_correlation_analysis(ctx, data: Dict[str, List[float]], 
                                     method: str = "pearson", **kwargs) -> CallToolResult:
    """å¤„ç†ç›¸å…³æ€§åˆ†æ"""
    if not data or len(data) < 2:
        raise ValueError("è‡³å°‘éœ€è¦2ä¸ªå˜é‡è¿›è¡Œç›¸å…³æ€§åˆ†æ")
    
    df = pd.DataFrame(data)
    correlation_matrix = df.corr(method=method)
    
    return CallToolResult(
        content=[
            TextContent(
                type="text",
                text=f"{method.title()}ç›¸å…³ç³»æ•°çŸ©é˜µï¼š\n{correlation_matrix.round(4).to_string()}"
            )
        ]
    )


# é¢æ¿æ•°æ®å¤„ç†å™¨
async def handle_panel_fixed_effects(ctx, y_data, x_data, entity_ids, time_periods,
                                    feature_names=None, entity_effects=True, time_effects=False, **kwargs):
    """å¤„ç†å›ºå®šæ•ˆåº”æ¨¡å‹ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = fixed_effects_model(y_data, x_data, entity_ids, time_periods, feature_names, entity_effects, time_effects)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š å›ºå®šæ•ˆåº”æ¨¡å‹åˆ†æç»“æœ

ğŸ” æ¨¡å‹æ‹Ÿåˆä¿¡æ¯ï¼š
- RÂ² = {result.rsquared:.4f}
- è°ƒæ•´RÂ² = {result.rsquared_adj:.4f}
- Fç»Ÿè®¡é‡ = {result.f_statistic:.4f} (p = {result.f_pvalue:.4f})
- AIC = {result.aic:.2f}, BIC = {result.bic:.2f}
- è§‚æµ‹æ•°é‡ = {result.n_obs}
- ä¸ªä½“æ•ˆåº” = {'æ˜¯' if result.entity_effects else 'å¦'}
- æ—¶é—´æ•ˆåº” = {'æ˜¯' if result.time_effects else 'å¦'}

ğŸ“ˆ å›å½’ç³»æ•°è¯¦æƒ…ï¼š"""
    
    # æ·»åŠ ç³»æ•°ä¿¡æ¯
    for var_name, coef_info in result.coefficients.items():
        significance = "***" if coef_info["p_value"] < 0.01 else "**" if coef_info["p_value"] < 0.05 else "*" if coef_info["p_value"] < 0.1 else ""
        result_text += f"\n- {var_name}: {coef_info['coef']:.4f}{significance} (se={coef_info['std_err']:.4f}, p={coef_info['p_value']:.4f})"
    
    result_text += "\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šå›ºå®šæ•ˆåº”æ¨¡å‹é€šè¿‡ç»„å†…å˜æ¢æ¶ˆé™¤ä¸ªä½“å›ºå®šå·®å¼‚ï¼Œé€‚ç”¨äºä¸ªä½“é—´å­˜åœ¨ä¸å¯è§‚æµ‹å›ºå®šç‰¹å¾çš„æƒ…å†µã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_panel_random_effects(ctx, y_data, x_data, entity_ids, time_periods,
                                     feature_names=None, entity_effects=True, time_effects=False, **kwargs):
    """å¤„ç†éšæœºæ•ˆåº”æ¨¡å‹ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = random_effects_model(y_data, x_data, entity_ids, time_periods, feature_names, entity_effects, time_effects)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š éšæœºæ•ˆåº”æ¨¡å‹åˆ†æç»“æœ

ğŸ” æ¨¡å‹æ‹Ÿåˆä¿¡æ¯ï¼š
- RÂ² = {result.rsquared:.4f}
- è°ƒæ•´RÂ² = {result.rsquared_adj:.4f}
- Fç»Ÿè®¡é‡ = {result.f_statistic:.4f} (p = {result.f_pvalue:.4f})
- AIC = {result.aic:.2f}, BIC = {result.bic:.2f}
- è§‚æµ‹æ•°é‡ = {result.n_obs}
- ä¸ªä½“æ•ˆåº” = {'æ˜¯' if result.entity_effects else 'å¦'}
- æ—¶é—´æ•ˆåº” = {'æ˜¯' if result.time_effects else 'å¦'}

ğŸ“ˆ å›å½’ç³»æ•°è¯¦æƒ…ï¼š"""
    
    # æ·»åŠ ç³»æ•°ä¿¡æ¯
    for var_name, coef_info in result.coefficients.items():
        significance = "***" if coef_info["p_value"] < 0.01 else "**" if coef_info["p_value"] < 0.05 else "*" if coef_info["p_value"] < 0.1 else ""
        result_text += f"\n- {var_name}: {coef_info['coef']:.4f}{significance} (se={coef_info['std_err']:.4f}, p={coef_info['p_value']:.4f})"
    
    result_text += "\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šéšæœºæ•ˆåº”æ¨¡å‹å‡è®¾ä¸ªä½“å·®å¼‚æ˜¯éšæœºçš„ï¼Œæ¯”å›ºå®šæ•ˆåº”æ¨¡å‹æ›´æœ‰æ•ˆç‡ï¼Œä½†éœ€è¦æ»¡è¶³ä¸ªä½“æ•ˆåº”ä¸è§£é‡Šå˜é‡ä¸ç›¸å…³çš„å‡è®¾ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_panel_hausman_test(ctx, y_data, x_data, entity_ids, time_periods, feature_names=None, **kwargs):
    """å¤„ç†Hausmanæ£€éªŒ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = hausman_test(y_data, x_data, entity_ids, time_periods, feature_names)
    
    result_text = f"""ğŸ“Š Hausmanæ£€éªŒç»“æœ

ğŸ” æ£€éªŒä¿¡æ¯ï¼š
- æ£€éªŒç»Ÿè®¡é‡ = {result.statistic:.4f}
- på€¼ = {result.p_value:.4f}
- æ˜¾è‘—æ€§ = {'æ˜¯' if result.significant else 'å¦'} (5%æ°´å¹³)

ğŸ’¡ æ¨¡å‹é€‰æ‹©å»ºè®®ï¼š
{result.recommendation}

ğŸ“‹ å†³ç­–è§„åˆ™ï¼š
- på€¼ < 0.05: æ‹’ç»åŸå‡è®¾ï¼Œé€‰æ‹©å›ºå®šæ•ˆåº”æ¨¡å‹
- på€¼ >= 0.05: ä¸èƒ½æ‹’ç»åŸå‡è®¾ï¼Œé€‰æ‹©éšæœºæ•ˆåº”æ¨¡å‹

ğŸ”¬ æ£€éªŒåŸç†ï¼šHausmanæ£€éªŒç”¨äºåˆ¤æ–­ä¸ªä½“æ•ˆåº”æ˜¯å¦ä¸è§£é‡Šå˜é‡ç›¸å…³ã€‚åŸå‡è®¾ä¸ºéšæœºæ•ˆåº”æ¨¡å‹æ˜¯ä¸€è‡´çš„ã€‚"""
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_panel_unit_root_test(ctx, **kwargs):
    """
    å¤„ç†é¢æ¿å•ä½æ ¹æ£€éªŒ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼
    
    panel_unit_root_testå‡½æ•°æœŸæœ›ï¼šdata, entity_ids, time_periods
    ä½†panelè£…é¥°å™¨ä¼šä¼ å…¥ï¼šy_data, x_data, entity_ids, time_periods
    """
    # æå–å‚æ•°
    data = kwargs.get('data')
    y_data = kwargs.get('y_data')
    entity_ids = kwargs.get('entity_ids')
    time_periods = kwargs.get('time_periods')
    test_type = kwargs.get('test_type', 'levinlin')
    
    # å¦‚æœæ²¡æœ‰dataä½†æœ‰y_dataï¼Œä½¿ç”¨y_dataï¼ˆæ¥è‡ªpanelè£…é¥°å™¨ï¼‰
    if data is None and y_data is not None:
        data = y_data
    
    if data is None:
        raise ValueError("éœ€è¦æä¾›æ•°æ®ï¼ˆdataæˆ–y_dataï¼‰")
    
    if entity_ids is None or time_periods is None:
        raise ValueError("éœ€è¦æä¾›entity_idså’Œtime_periods")
    
    # åªä¼ é€’panel_unit_root_testéœ€è¦çš„å‚æ•°
    result = panel_unit_root_test(data, entity_ids, time_periods, test_type)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š é¢æ¿å•ä½æ ¹æ£€éªŒç»“æœ

ğŸ” æ£€éªŒä¿¡æ¯ï¼š
- æ£€éªŒæ–¹æ³• = {test_type.upper()}
- ä¸ªä½“æ•°é‡ = {len(set(entity_ids))}
- æ—¶é—´æœŸæ•° = {len(set(time_periods))}
- æ£€éªŒç»Ÿè®¡é‡ = {result.statistic:.4f}
- på€¼ = {result.p_value:.4f}
- å¹³ç¨³æ€§ = {'å¹³ç¨³' if result.stationary else 'éå¹³ç¨³'} (5%æ°´å¹³)

ğŸ“ˆ æ£€éªŒè¯¦æƒ…ï¼š"""
    
    # æ·»åŠ æ£€éªŒè¯¦æƒ…ä¿¡æ¯
    if hasattr(result, 'critical_values'):
        result_text += f"\n- ä¸´ç•Œå€¼: {result.critical_values}"
    if hasattr(result, 'lags_used'):
        result_text += f"\n- ä½¿ç”¨æ»åé˜¶æ•°: {result.lags_used}"
    if hasattr(result, 'test_statistic'):
        result_text += f"\n- æ£€éªŒç»Ÿè®¡é‡: {result.test_statistic:.4f}"
    
    result_text += f"\n\nğŸ’¡ æ£€éªŒè¯´æ˜ï¼šé¢æ¿å•ä½æ ¹æ£€éªŒç”¨äºåˆ¤æ–­é¢æ¿æ•°æ®æ˜¯å¦å¹³ç¨³ï¼Œæ˜¯é¢æ¿æ•°æ®åˆ†æçš„é‡è¦å‰ææ£€éªŒã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šå¦‚æœæ•°æ®éå¹³ç¨³ï¼Œéœ€è¦è¿›è¡Œå·®åˆ†å¤„ç†æˆ–ä½¿ç”¨é¢æ¿åæ•´æ£€éªŒã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


# æ—¶é—´åºåˆ—å¤„ç†å™¨
async def handle_var_model(ctx, data, max_lags=5, ic="aic", **kwargs):
    """å¤„ç†VARæ¨¡å‹åˆ†æ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = var_model(data, max_lags=max_lags, ic=ic)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š VARæ¨¡å‹åˆ†æç»“æœ

ğŸ” æ¨¡å‹åŸºæœ¬ä¿¡æ¯ï¼š
- æœ€ä¼˜æ»åé˜¶æ•° = {result.order}
- å˜é‡æ•°é‡ = {len(result.variables) if hasattr(result, 'variables') else 'æœªçŸ¥'}
- ä¿¡æ¯å‡†åˆ™ = {ic.upper()}
- AIC = {result.aic:.2f}
- BIC = {getattr(result, 'bic', 'N/A')}
- HQIC = {getattr(result, 'hqic', 'N/A')}

ğŸ“ˆ æ¨¡å‹è¯Šæ–­ä¿¡æ¯ï¼š"""
    
    # æ·»åŠ æ¨¡å‹è¯Šæ–­ä¿¡æ¯
    if hasattr(result, 'residuals_normality'):
        result_text += f"\n- æ®‹å·®æ­£æ€æ€§æ£€éªŒ: {result.residuals_normality}"
    if hasattr(result, 'serial_correlation'):
        result_text += f"\n- åºåˆ—ç›¸å…³æ€§æ£€éªŒ: {result.serial_correlation}"
    if hasattr(result, 'stability'):
        result_text += f"\n- æ¨¡å‹ç¨³å®šæ€§: {result.stability}"
    
    # æ·»åŠ å˜é‡ä¿¡æ¯
    if hasattr(result, 'variables'):
        result_text += f"\n\nğŸ”¬ åˆ†æå˜é‡ï¼š"
        for var in result.variables:
            result_text += f"\n- {var}"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šVARæ¨¡å‹ç”¨äºåˆ†æå¤šä¸ªæ—¶é—´åºåˆ—å˜é‡é—´çš„åŠ¨æ€å…³ç³»ï¼Œèƒ½å¤Ÿæ•æ‰å˜é‡é—´çš„ç›¸äº’å½±å“å’Œæ»åæ•ˆåº”ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šVARæ¨¡å‹å‡è®¾æ‰€æœ‰å˜é‡éƒ½æ˜¯å†…ç”Ÿçš„ï¼Œé€‚ç”¨äºåˆ†æå˜é‡é—´çš„åŠ¨æ€äº¤äº’å…³ç³»ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_vecm_model(ctx, data, coint_rank=1, deterministic="co", max_lags=5, **kwargs):
    """å¤„ç†VECMæ¨¡å‹åˆ†æ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = vecm_model(data, coint_rank=coint_rank, deterministic=deterministic, max_lags=max_lags)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š VECMæ¨¡å‹åˆ†æç»“æœ

ğŸ” æ¨¡å‹åŸºæœ¬ä¿¡æ¯ï¼š
- åæ•´ç§© = {result.coint_rank}
- ç¡®å®šæ€§é¡¹ç±»å‹ = {deterministic}
- æœ€å¤§æ»åé˜¶æ•° = {max_lags}
- AIC = {result.aic:.2f}
- BIC = {getattr(result, 'bic', 'N/A')}
- HQIC = {getattr(result, 'hqic', 'N/A')}

ğŸ“ˆ åæ•´å…³ç³»åˆ†æï¼š"""
    
    # æ·»åŠ åæ•´å…³ç³»ä¿¡æ¯
    if hasattr(result, 'coint_relations'):
        result_text += f"\n- åæ•´å…³ç³»æ•°é‡: {len(result.coint_relations)}"
        for i, relation in enumerate(result.coint_relations[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªå…³ç³»
            result_text += f"\n- å…³ç³»{i}: {relation}"
        if len(result.coint_relations) > 3:
            result_text += f"\n- ... è¿˜æœ‰{len(result.coint_relations) - 3}ä¸ªåæ•´å…³ç³»"
    
    # æ·»åŠ è¯¯å·®ä¿®æ­£é¡¹ä¿¡æ¯
    if hasattr(result, 'error_correction'):
        result_text += f"\n\nğŸ”§ è¯¯å·®ä¿®æ­£æœºåˆ¶ï¼š"
        result_text += f"\n- è¯¯å·®ä¿®æ­£é¡¹æ˜¾è‘—æ€§: {result.error_correction}"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šVECMæ¨¡å‹ç”¨äºåˆ†æéå¹³ç¨³æ—¶é—´åºåˆ—çš„é•¿æœŸå‡è¡¡å…³ç³»ï¼ŒåŒ…å«è¯¯å·®ä¿®æ­£æœºåˆ¶æ¥åæ˜ çŸ­æœŸè°ƒæ•´è¿‡ç¨‹ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šVECMæ¨¡å‹è¦æ±‚å˜é‡é—´å­˜åœ¨åæ•´å…³ç³»ï¼Œé€‚ç”¨äºåˆ†æç»æµå˜é‡çš„é•¿æœŸå‡è¡¡å’ŒçŸ­æœŸåŠ¨æ€è°ƒæ•´ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_garch_model(ctx, data, order=(1, 1), dist="normal", **kwargs):
    """å¤„ç†GARCHæ¨¡å‹åˆ†æ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = garch_model(data, order=order, dist=dist)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š GARCHæ¨¡å‹åˆ†æç»“æœ

ğŸ” æ¨¡å‹åŸºæœ¬ä¿¡æ¯ï¼š
- GARCHé˜¶æ•° = ({order[0]}, {order[1]})
- è¯¯å·®åˆ†å¸ƒ = {dist}
- æŒä¹…æ€§ = {result.persistence:.4f}
- AIC = {result.aic:.2f}
- BIC = {getattr(result, 'bic', 'N/A')}

ğŸ“ˆ æ³¢åŠ¨ç‡ç‰¹å¾ï¼š"""
    
    # æ·»åŠ æ³¢åŠ¨ç‡ç‰¹å¾ä¿¡æ¯
    if hasattr(result, 'volatility_persistence'):
        result_text += f"\n- æ³¢åŠ¨ç‡æŒç»­æ€§: {result.volatility_persistence:.4f}"
    if hasattr(result, 'unconditional_variance'):
        result_text += f"\n- æ— æ¡ä»¶æ–¹å·®: {result.unconditional_variance:.4f}"
    if hasattr(result, 'leverage_effect'):
        result_text += f"\n- æ æ†æ•ˆåº”: {result.leverage_effect}"
    
    # æ·»åŠ æ¨¡å‹è¯Šæ–­ä¿¡æ¯
    if hasattr(result, 'residuals_test'):
        result_text += f"\n\nğŸ”§ æ¨¡å‹è¯Šæ–­ï¼š"
        result_text += f"\n- æ®‹å·®æ£€éªŒ: {result.residuals_test}"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šGARCHæ¨¡å‹ç”¨äºåˆ†æé‡‘èæ—¶é—´åºåˆ—çš„æ³¢åŠ¨ç‡èšç±»ç°è±¡ï¼Œèƒ½å¤Ÿæ•æ‰æ¡ä»¶å¼‚æ–¹å·®æ€§ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šGARCHæ¨¡å‹é€‚ç”¨äºé‡‘èæ•°æ®æ³¢åŠ¨ç‡å»ºæ¨¡ï¼Œé˜¶æ•°é€‰æ‹©å½±å“æ¨¡å‹å¯¹æ³¢åŠ¨ç‡æŒç»­æ€§çš„æ•æ‰èƒ½åŠ›ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_state_space_model(ctx, data, state_dim=1, observation_dim=1,
                                  trend=True, seasonal=False, period=12, **kwargs):
    """å¤„ç†çŠ¶æ€ç©ºé—´æ¨¡å‹åˆ†æ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = state_space_model(data, state_dim, observation_dim, trend, seasonal, period)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š çŠ¶æ€ç©ºé—´æ¨¡å‹åˆ†æç»“æœ

ğŸ” æ¨¡å‹ç»“æ„ä¿¡æ¯ï¼š
- çŠ¶æ€ç»´åº¦ = {state_dim}
- è§‚æµ‹ç»´åº¦ = {observation_dim}
- è¶‹åŠ¿é¡¹ = {'åŒ…å«' if trend else 'ä¸åŒ…å«'}
- å­£èŠ‚é¡¹ = {'åŒ…å«' if seasonal else 'ä¸åŒ…å«'}
- å­£èŠ‚å‘¨æœŸ = {period if seasonal else 'N/A'}
- AIC = {result.aic:.2f}
- BIC = {result.bic:.2f}
- å¯¹æ•°ä¼¼ç„¶å€¼ = {result.log_likelihood:.2f}

ğŸ“ˆ çŠ¶æ€åˆ†æï¼š"""

    # æ·»åŠ çŠ¶æ€ä¿¡æ¯
    if result.state_names:
        result_text += f"\n- çŠ¶æ€å˜é‡: {', '.join(result.state_names)}"
    if result.observation_names:
        result_text += f"\n- è§‚æµ‹å˜é‡: {', '.join(result.observation_names)}"
    
    # æ·»åŠ çŠ¶æ€ä¼°è®¡ä¿¡æ¯
    if result.filtered_state:
        result_text += f"\n- æ»¤æ³¢çŠ¶æ€ä¼°è®¡: å·²è®¡ç®—"
    if result.smoothed_state:
        result_text += f"\n- å¹³æ»‘çŠ¶æ€ä¼°è®¡: å·²è®¡ç®—"

    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šçŠ¶æ€ç©ºé—´æ¨¡å‹ç”¨äºåˆ†ææ—¶é—´åºåˆ—çš„æ½œåœ¨çŠ¶æ€å’Œè§‚æµ‹å…³ç³»ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„åŠ¨æ€ç³»ç»Ÿï¼Œç‰¹åˆ«é€‚ç”¨äºå…·æœ‰ä¸å¯è§‚æµ‹çŠ¶æ€çš„æ—¶é—´åºåˆ—å»ºæ¨¡ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šçŠ¶æ€ç©ºé—´æ¨¡å‹å‚æ•°ä¼°è®¡å¯èƒ½å¯¹åˆå§‹å€¼æ•æ„Ÿï¼Œå»ºè®®è¿›è¡Œå¤šæ¬¡åˆå§‹åŒ–å°è¯•ä»¥è·å¾—ç¨³å®šç»“æœã€‚"

    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_variance_decomposition(ctx, data, periods=10, max_lags=5, **kwargs):
    """å¤„ç†æ–¹å·®åˆ†è§£åˆ†æ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = variance_decomposition(data, periods=periods, max_lags=max_lags)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š æ–¹å·®åˆ†è§£åˆ†æç»“æœ

ğŸ” åˆ†æè®¾ç½®ï¼š
- åˆ†è§£æœŸæ•° = {periods}
- æœ€å¤§æ»åé˜¶æ•° = {max_lags}
- å˜é‡æ•°é‡ = {len(data) if data else 'æœªçŸ¥'}

ğŸ“ˆ æ–¹å·®åˆ†è§£ç»“æœï¼š"""

    # æ·»åŠ æ–¹å·®åˆ†è§£ç»“æœ
    if isinstance(result, dict) and "variance_decomposition" in result:
        variance_decomp = result["variance_decomposition"]
        horizon = result.get("horizon", periods)
        
        result_text += f"\n- åˆ†ææœŸæ•°: {horizon}æœŸ"
        
        for var_name, decomposition in variance_decomp.items():
            result_text += f"\n\nğŸ”¬ å˜é‡ '{var_name}' çš„æ–¹å·®æ¥æºï¼š"
            if isinstance(decomposition, dict):
                for source, percentages in decomposition.items():
                    if isinstance(percentages, list) and len(percentages) > 0:
                        # æ˜¾ç¤ºæœ€åä¸€æœŸçš„è´¡çŒ®åº¦
                        final_percentage = percentages[-1] * 100 if isinstance(percentages[-1], (int, float)) else 0
                        result_text += f"\n- {source}: {final_percentage:.1f}%"
                    else:
                        result_text += f"\n- {source}: {percentages:.1f}%"
            else:
                result_text += f"\n- æ€»æ–¹å·®: {decomposition:.1f}%"
    else:
        result_text += f"\n- ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•è§£ææ–¹å·®åˆ†è§£ç»“æœ"

    result_text += f"\n\nğŸ’¡ åˆ†æè¯´æ˜ï¼šæ–¹å·®åˆ†è§£ç”¨äºåˆ†æå¤šå˜é‡ç³»ç»Ÿä¸­å„å˜é‡å¯¹é¢„æµ‹è¯¯å·®æ–¹å·®çš„è´¡çŒ®ç¨‹åº¦ï¼Œåæ˜ å˜é‡é—´çš„åŠ¨æ€å½±å“å…³ç³»ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šæ–¹å·®åˆ†è§£ç»“æœä¾èµ–äºVARæ¨¡å‹çš„æ»åé˜¶æ•°é€‰æ‹©ï¼Œä¸åŒæœŸæ•°çš„åˆ†è§£ç»“æœåæ˜ çŸ­æœŸå’Œé•¿æœŸå½±å“ã€‚"

    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result
    )


# æœºå™¨å­¦ä¹ å¤„ç†å™¨
async def handle_random_forest(ctx, y_data, x_data, feature_names=None, n_estimators=100, max_depth=None, **kwargs):
    """å¤„ç†éšæœºæ£®æ—å›å½’ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = random_forest_regression(y_data, x_data, feature_names, n_estimators, max_depth)
    
    # æ£€æŸ¥RÂ²æ˜¯å¦ä¸ºè´Ÿå€¼
    r2_warning = ""
    if result.r2_score < 0:
        r2_warning = f"\nâš ï¸ è­¦å‘Šï¼šRÂ²ä¸ºè´Ÿå€¼({result.r2_score:.4f})ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½æ¯”ç®€å•å‡å€¼é¢„æµ‹æ›´å·®ã€‚å»ºè®®ï¼š1) æ£€æŸ¥æ•°æ®è´¨é‡ 2) å¢åŠ æ ·æœ¬æ•°é‡ 3) è°ƒæ•´æ¨¡å‹å‚æ•°"
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š éšæœºæ£®æ—å›å½’åˆ†æç»“æœ

ğŸ” æ¨¡å‹æ‹Ÿåˆä¿¡æ¯ï¼š
- RÂ² = {result.r2_score:.4f}
- å‡æ–¹è¯¯å·®(MSE) = {result.mse:.4f}
- å¹³å‡ç»å¯¹è¯¯å·®(MAE) = {result.mae:.4f}
- æ ·æœ¬æ•°é‡ = {result.n_obs}
- æ ‘çš„æ•°é‡ = {result.n_estimators}
- æœ€å¤§æ·±åº¦ = {result.max_depth if result.max_depth else 'æ— é™åˆ¶'}
- è¢‹å¤–å¾—åˆ† = {f"{result.oob_score:.4f}" if result.oob_score else 'æœªè®¡ç®—'}
{r2_warning}

ğŸ“ˆ ç‰¹å¾é‡è¦æ€§ï¼ˆå‰10ä¸ªï¼‰ï¼š"""
    
    # æ·»åŠ ç‰¹å¾é‡è¦æ€§ä¿¡æ¯ï¼ŒæŒ‰é‡è¦æ€§æ’åº
    if result.feature_importance:
        sorted_features = sorted(result.feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            result_text += f"\n- {feature}: {importance:.4f}"
        if len(sorted_features) > 10:
            result_text += f"\n- ... è¿˜æœ‰{len(sorted_features) - 10}ä¸ªç‰¹å¾"
    else:
        result_text += "\n- ç‰¹å¾é‡è¦æ€§æœªè®¡ç®—"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šéšæœºæ£®æ—é€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘å¹¶é›†æˆç»“æœï¼Œèƒ½å¤Ÿå¤„ç†éçº¿æ€§å…³ç³»å’Œç‰¹å¾äº¤äº’ï¼Œå¯¹å¼‚å¸¸å€¼ç¨³å¥ä¸”ä¸æ˜“è¿‡æ‹Ÿåˆã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šéšæœºæ£®æ—æ˜¯é»‘ç›’æ¨¡å‹ï¼Œå¯è§£é‡Šæ€§è¾ƒå·®ï¼Œä½†é¢„æµ‹æ€§èƒ½é€šå¸¸è¾ƒå¥½ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_gradient_boosting(ctx, y_data, x_data, feature_names=None,
                                  n_estimators=100, learning_rate=0.1, max_depth=3, **kwargs):
    """å¤„ç†æ¢¯åº¦æå‡æ ‘å›å½’ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = gradient_boosting_regression(y_data, x_data, feature_names, n_estimators, learning_rate, max_depth)
    
    # æ£€æŸ¥RÂ²æ˜¯å¦ä¸ºè´Ÿå€¼
    r2_warning = ""
    if result.r2_score < 0:
        r2_warning = f"\nâš ï¸ è­¦å‘Šï¼šRÂ²ä¸ºè´Ÿå€¼({result.r2_score:.4f})ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½æ¯”ç®€å•å‡å€¼é¢„æµ‹æ›´å·®ã€‚å»ºè®®ï¼š1) æ£€æŸ¥æ•°æ®è´¨é‡ 2) å¢åŠ æ ·æœ¬æ•°é‡ 3) è°ƒæ•´æ¨¡å‹å‚æ•°"
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š æ¢¯åº¦æå‡æ ‘å›å½’åˆ†æç»“æœ

ğŸ” æ¨¡å‹æ‹Ÿåˆä¿¡æ¯ï¼š
- RÂ² = {result.r2_score:.4f}
- å‡æ–¹è¯¯å·®(MSE) = {result.mse:.4f}
- å¹³å‡ç»å¯¹è¯¯å·®(MAE) = {result.mae:.4f}
- æ ·æœ¬æ•°é‡ = {result.n_obs}
- æ ‘çš„æ•°é‡ = {result.n_estimators}
- å­¦ä¹ ç‡ = {result.learning_rate}
- æœ€å¤§æ·±åº¦ = {result.max_depth}
{r2_warning}

ğŸ“ˆ ç‰¹å¾é‡è¦æ€§ï¼ˆå‰10ä¸ªï¼‰ï¼š"""
    
    # æ·»åŠ ç‰¹å¾é‡è¦æ€§ä¿¡æ¯ï¼ŒæŒ‰é‡è¦æ€§æ’åº
    if result.feature_importance:
        sorted_features = sorted(result.feature_importance.items(), key=lambda x: x[1], reverse=True)
        for i, (feature, importance) in enumerate(sorted_features[:10]):
            result_text += f"\n- {feature}: {importance:.4f}"
        if len(sorted_features) > 10:
            result_text += f"\n- ... è¿˜æœ‰{len(sorted_features) - 10}ä¸ªç‰¹å¾"
    else:
        result_text += "\n- ç‰¹å¾é‡è¦æ€§æœªè®¡ç®—"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šæ¢¯åº¦æå‡æ ‘é€šè¿‡é¡ºåºæ„å»ºå†³ç­–æ ‘ï¼Œæ¯æ£µæ ‘ä¿®æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œé€šå¸¸å…·æœ‰å¾ˆé«˜çš„é¢„æµ‹ç²¾åº¦ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šæ¢¯åº¦æå‡æ ‘å¯¹å‚æ•°æ•æ„Ÿï¼Œéœ€è¦ä»”ç»†è°ƒä¼˜ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿ä½†é¢„æµ‹æ€§èƒ½ä¼˜ç§€ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_lasso_regression(ctx, y_data, x_data, feature_names=None, alpha=1.0, **kwargs):
    """å¤„ç†Lassoå›å½’ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = lasso_regression(y_data, x_data, feature_names, alpha)
    
    # æ£€æŸ¥RÂ²æ˜¯å¦ä¸ºè´Ÿå€¼
    r2_warning = ""
    if result.r2_score < 0:
        r2_warning = f"\nâš ï¸ è­¦å‘Šï¼šRÂ²ä¸ºè´Ÿå€¼({result.r2_score:.4f})ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½æ¯”ç®€å•å‡å€¼é¢„æµ‹æ›´å·®ã€‚å»ºè®®ï¼š1) æ£€æŸ¥æ•°æ®è´¨é‡ 2) å°è¯•æ›´å°çš„alphaå€¼ 3) å¢åŠ æ ·æœ¬æ•°é‡"
    
    # æ£€æŸ¥ç³»æ•°æ˜¯å¦å…¨ä¸º0
    coef_warning = ""
    if all(abs(coef) < 1e-10 for coef in result.coefficients.values()):
        coef_warning = f"\nâš ï¸ è­¦å‘Šï¼šæ‰€æœ‰ç³»æ•°éƒ½è¢«å‹ç¼©ä¸º0ï¼Œæ­£åˆ™åŒ–å‚æ•°alpha={alpha}å¯èƒ½è¿‡å¤§ï¼Œå»ºè®®å‡å°alphaå€¼"
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š Lassoå›å½’åˆ†æç»“æœ

ğŸ” æ¨¡å‹æ‹Ÿåˆä¿¡æ¯ï¼š
- RÂ² = {result.r2_score:.4f}
- å‡æ–¹è¯¯å·®(MSE) = {result.mse:.4f}
- å¹³å‡ç»å¯¹è¯¯å·®(MAE) = {result.mae:.4f}
- æ ·æœ¬æ•°é‡ = {result.n_obs}
- æ­£åˆ™åŒ–å‚æ•°(alpha) = {result.alpha}
{r2_warning}{coef_warning}

ğŸ“ˆ å›å½’ç³»æ•°è¯¦æƒ…ï¼š"""
    
    # æ·»åŠ ç³»æ•°ä¿¡æ¯ï¼ŒæŒ‰ç»å¯¹å€¼æ’åº
    sorted_coefficients = sorted(result.coefficients.items(), key=lambda x: abs(x[1]), reverse=True)
    for var_name, coef in sorted_coefficients:
        if abs(coef) > 1e-10:  # åªæ˜¾ç¤ºéé›¶ç³»æ•°
            result_text += f"\n- {var_name}: {coef:.4f}"
        else:
            result_text += f"\n- {var_name}: 0.0000 (è¢«å‹ç¼©)"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šLassoå›å½’ä½¿ç”¨L1æ­£åˆ™åŒ–è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†ä¸é‡è¦çš„ç‰¹å¾ç³»æ•°å‹ç¼©ä¸º0ï¼Œé€‚ç”¨äºé«˜ç»´æ•°æ®å’Œç‰¹å¾é€‰æ‹©åœºæ™¯ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šç”±äºæ•°æ®æ ‡å‡†åŒ–ï¼Œç³»æ•°å¤§å°éœ€è¦è°¨æ…è§£é‡Šã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_ridge_regression(ctx, y_data, x_data, feature_names=None, alpha=1.0, **kwargs):
    """å¤„ç†Ridgeå›å½’ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = ridge_regression(y_data, x_data, feature_names, alpha)
    
    # æ£€æŸ¥RÂ²æ˜¯å¦ä¸ºè´Ÿå€¼
    r2_warning = ""
    if result.r2_score < 0:
        r2_warning = f"\nâš ï¸ è­¦å‘Šï¼šRÂ²ä¸ºè´Ÿå€¼({result.r2_score:.4f})ï¼Œè¡¨æ˜æ¨¡å‹æ€§èƒ½æ¯”ç®€å•å‡å€¼é¢„æµ‹æ›´å·®ã€‚å»ºè®®ï¼š1) æ£€æŸ¥æ•°æ®è´¨é‡ 2) å°è¯•æ›´å°çš„alphaå€¼ 3) å¢åŠ æ ·æœ¬æ•°é‡"
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š Ridgeå›å½’åˆ†æç»“æœ

ğŸ” æ¨¡å‹æ‹Ÿåˆä¿¡æ¯ï¼š
- RÂ² = {result.r2_score:.4f}
- å‡æ–¹è¯¯å·®(MSE) = {result.mse:.4f}
- å¹³å‡ç»å¯¹è¯¯å·®(MAE) = {result.mae:.4f}
- æ ·æœ¬æ•°é‡ = {result.n_obs}
- æ­£åˆ™åŒ–å‚æ•°(alpha) = {result.alpha}
{r2_warning}

ğŸ“ˆ å›å½’ç³»æ•°è¯¦æƒ…ï¼š"""
    
    # æ·»åŠ ç³»æ•°ä¿¡æ¯ï¼ŒæŒ‰ç»å¯¹å€¼æ’åº
    sorted_coefficients = sorted(result.coefficients.items(), key=lambda x: abs(x[1]), reverse=True)
    for var_name, coef in sorted_coefficients:
        result_text += f"\n- {var_name}: {coef:.4f}"
    
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šRidgeå›å½’ä½¿ç”¨L2æ­£åˆ™åŒ–å¤„ç†å¤šé‡å…±çº¿æ€§é—®é¢˜ï¼Œå¯¹æ‰€æœ‰ç³»æ•°è¿›è¡Œæ”¶ç¼©ä½†ä¸è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œé€‚ç”¨äºéœ€è¦ç¨³å®šä¼°è®¡çš„åœºæ™¯ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šç”±äºæ•°æ®æ ‡å‡†åŒ–ï¼Œç³»æ•°å¤§å°éœ€è¦è°¨æ…è§£é‡Šã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_cross_validation(ctx, y_data, x_data, model_type="random_forest", cv_folds=5, scoring="r2", **kwargs):
    """å¤„ç†äº¤å‰éªŒè¯ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = cross_validation(y_data, x_data, model_type, cv_folds, scoring)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š äº¤å‰éªŒè¯åˆ†æç»“æœ

ğŸ” éªŒè¯ä¿¡æ¯ï¼š
- æ¨¡å‹ç±»å‹ = {result.model_type}
- äº¤å‰éªŒè¯æŠ˜æ•° = {result.n_splits}
- è¯„åˆ†æŒ‡æ ‡ = {scoring}
- å¹³å‡å¾—åˆ† = {result.mean_score:.4f}
- å¾—åˆ†æ ‡å‡†å·® = {result.std_score:.4f}
- å˜å¼‚ç³»æ•° = {(result.std_score / abs(result.mean_score)) * 100 if result.mean_score != 0 else 0:.2f}%

ğŸ“ˆ å„æŠ˜å¾—åˆ†è¯¦æƒ…ï¼š"""
    
    # æ·»åŠ å„æŠ˜å¾—åˆ†
    for i, score in enumerate(result.cv_scores, 1):
        result_text += f"\n- ç¬¬{i}æŠ˜: {score:.4f}"
    
    # è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
    stability_assessment = ""
    cv_threshold = 0.1  # 10%çš„å˜å¼‚ç³»æ•°é˜ˆå€¼
    cv_value = (result.std_score / abs(result.mean_score)) if result.mean_score != 0 else 0
    
    if cv_value < cv_threshold:
        stability_assessment = f"\n\nâœ… æ¨¡å‹ç¨³å®šæ€§ï¼šä¼˜ç§€ï¼ˆå˜å¼‚ç³»æ•°{cv_value*100:.2f}% < {cv_threshold*100:.0f}%ï¼‰"
    elif cv_value < cv_threshold * 2:
        stability_assessment = f"\n\nâš ï¸ æ¨¡å‹ç¨³å®šæ€§ï¼šä¸€èˆ¬ï¼ˆå˜å¼‚ç³»æ•°{cv_value*100:.2f}% åœ¨{cv_threshold*100:.0f}%-{cv_threshold*2*100:.0f}%ä¹‹é—´ï¼‰"
    else:
        stability_assessment = f"\n\nâŒ æ¨¡å‹ç¨³å®šæ€§ï¼šè¾ƒå·®ï¼ˆå˜å¼‚ç³»æ•°{cv_value*100:.2f}% > {cv_threshold*2*100:.0f}%ï¼‰"
    
    result_text += stability_assessment
    result_text += f"\n\nğŸ’¡ æ¨¡å‹è¯´æ˜ï¼šäº¤å‰éªŒè¯é€šè¿‡å°†æ•°æ®åˆ†æˆå¤šä¸ªå­é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šå˜å¼‚ç³»æ•°è¶Šå°è¡¨æ˜æ¨¡å‹è¶Šç¨³å®šï¼Œå»ºè®®é€‰æ‹©å˜å¼‚ç³»æ•°å°äº10%çš„æ¨¡å‹ã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )


async def handle_feature_importance(ctx, y_data, x_data, feature_names=None, method="random_forest", top_k=5, **kwargs):
    """å¤„ç†ç‰¹å¾é‡è¦æ€§åˆ†æ - ç»Ÿä¸€è¾“å‡ºæ ¼å¼"""
    result = feature_importance_analysis(y_data, x_data, feature_names, method, top_k)
    
    # æ„å»ºè¯¦ç»†çš„ç»“æœæ–‡æœ¬
    result_text = f"""ğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æç»“æœ

ğŸ” åˆ†æä¿¡æ¯ï¼š
- åˆ†ææ–¹æ³• = {method}
- æ˜¾ç¤ºTopç‰¹å¾æ•°é‡ = {top_k}
- æ€»ç‰¹å¾æ•°é‡ = {len(result.feature_importance)}

ğŸ“ˆ ç‰¹å¾é‡è¦æ€§æ’åï¼š"""
    
    # æ·»åŠ ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
    for i, (feature, importance) in enumerate(result.sorted_features[:top_k], 1):
        percentage = (importance / sum(result.feature_importance.values())) * 100 if sum(result.feature_importance.values()) > 0 else 0
        result_text += f"\n{i}. {feature}: {importance:.4f} ({percentage:.1f}%)"
    
    # æ·»åŠ é‡è¦æ€§åˆ†å¸ƒä¿¡æ¯
    if len(result.sorted_features) > 0:
        top_k_importance = sum(imp for _, imp in result.sorted_features[:top_k])
        total_importance = sum(result.feature_importance.values())
        top_k_percentage = (top_k_importance / total_importance) * 100 if total_importance > 0 else 0
        
        result_text += f"\n\nğŸ“Š é‡è¦æ€§åˆ†å¸ƒï¼š"
        result_text += f"\n- Top {top_k}ç‰¹å¾ç´¯è®¡é‡è¦æ€§: {top_k_percentage:.1f}%"
        result_text += f"\n- å‰©ä½™ç‰¹å¾é‡è¦æ€§: {100 - top_k_percentage:.1f}%"
    
    result_text += f"\n\nğŸ’¡ åˆ†æè¯´æ˜ï¼šç‰¹å¾é‡è¦æ€§åˆ†æå¸®åŠ©è¯†åˆ«å¯¹é¢„æµ‹ç›®æ ‡æœ€é‡è¦çš„å˜é‡ï¼Œå¯ç”¨äºç‰¹å¾é€‰æ‹©å’Œæ¨¡å‹è§£é‡Šã€‚"
    result_text += f"\n\nâš ï¸ æ³¨æ„äº‹é¡¹ï¼šä¸åŒæ–¹æ³•è®¡ç®—çš„ç‰¹å¾é‡è¦æ€§å¯èƒ½ä¸åŒï¼Œå»ºè®®ç»“åˆä¸šåŠ¡çŸ¥è¯†è¿›è¡Œè§£é‡Šã€‚"
    
    return CallToolResult(
        content=[TextContent(type="text", text=result_text)],
        structuredContent=result.model_dump()
    )