{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa2ee8",
   "metadata": {
    "id": "fbaa2ee8",
    "outputId": "6939b4f1-52f4-497f-d3cb-8c1bbf820070"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n",
    "\n",
    "N, H, W = x_train.shape\n",
    "D = H * W\n",
    "x_train = x_train.reshape(-1, D)\n",
    "x_test = x_test.reshape(-1, D)\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    i = Input(shape=(latent_dim,))\n",
    "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = BatchNormalization(momentum=0.7)(x)\n",
    "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = BatchNormalization(momentum=0.7)(x)\n",
    "    x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = BatchNormalization(momentum=0.7)(x)\n",
    "    x = Dense(D, activation='tanh')(x)\n",
    "    model = Model(i, x)\n",
    "    return model\n",
    "\n",
    "def build_discriminator(img_size):\n",
    "    i = Input(shape=(img_size,))\n",
    "    x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n",
    "    x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(i, x)\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator(D)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "fake_pred = discriminator(img)\n",
    "combined_model_gen = Model(z, fake_pred)\n",
    "combined_model_gen.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "batch_size = 32\n",
    "epochs = 5000\n",
    "sample_period = 200\n",
    "\n",
    "ones = np.ones((batch_size, 1), dtype=np.float32)\n",
    "zeros = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "if not os.path.exists('gan_images'):\n",
    "    os.makedirs('gan_images')\n",
    "\n",
    "def sample_images(epoch):\n",
    "    rows, cols = 5, 5\n",
    "    noise = np.random.randn(rows * cols, latent_dim)\n",
    "    imgs = generator(noise, training=False)\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "    idx = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            axs[i,j].imshow(tf.reshape(imgs[idx], (H, W)).numpy(), cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            idx += 1\n",
    "    fig.savefig(f\"gan_images/{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "@tf.function\n",
    "def train_discriminator(real_imgs, fake_imgs, ones, zeros):\n",
    "    discriminator.trainable = True\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_pred = discriminator(real_imgs, training=True)\n",
    "        d_loss_real = tf.reduce_mean(tf.keras.losses.binary_crossentropy(ones, real_pred))\n",
    "        fake_pred = discriminator(fake_imgs, training=True)\n",
    "        d_loss_fake = tf.reduce_mean(tf.keras.losses.binary_crossentropy(zeros, fake_pred))\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    real_acc = tf.reduce_mean(tf.cast(tf.round(real_pred) == ones, tf.float32))\n",
    "    fake_acc = tf.reduce_mean(tf.cast(tf.round(fake_pred) == zeros, tf.float32))\n",
    "    d_acc = 0.5 * (real_acc + fake_acc)\n",
    "    grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
    "    discriminator.trainable = False\n",
    "    return d_loss, d_acc\n",
    "\n",
    "@tf.function\n",
    "def train_generator(noise, ones):\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_imgs = generator(noise, training=True)\n",
    "        fake_pred = discriminator(fake_imgs, training=False)\n",
    "        g_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(ones, fake_pred))\n",
    "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    combined_model_gen.optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "    return g_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    real_imgs = x_train[idx]\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    fake_imgs = generator(noise, training=False)\n",
    "    d_loss, d_acc = train_discriminator(real_imgs, fake_imgs, ones, zeros)\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    g_loss = train_generator(noise, ones)\n",
    "    noise = np.random.randn(batch_size, latent_dim)\n",
    "    g_loss = train_generator(noise, ones)\n",
    "    d_losses.append(d_loss.numpy())\n",
    "    g_losses.append(g_loss.numpy())\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1}/{epochs}, d_loss: {d_loss:.2f}, d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
    "    if epoch % sample_period == 0:\n",
    "        sample_images(epoch)\n",
    "\n",
    "plt.plot(d_losses, label='Discriminator Loss')\n",
    "plt.plot(g_losses, label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b50c19",
   "metadata": {
    "id": "b0b50c19"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
