#!/usr/bin/env python3
"""
Comparison: Basic HybridRouter vs Ray-Based HybridRouter

Shows the architectural difference between single-coordinator and
Ray-managed parallel coordinator pools.
"""

import asyncio
import logging

from sollol import HybridRouter, RayHybridRouter, OllamaPool

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


def print_header(title: str):
    """Print formatted header."""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


async def demo_basic_hybrid_router():
    """
    Basic HybridRouter - Single Coordinator

    Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama Poolâ”‚  â† Small models (task distribution)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Single Coordinator (18080)    â”‚  â† Large models
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
    â”‚   â”‚ RPC #1  â”‚ RPC #2  â”‚        â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Limitations:
    - Only 1 coordinator serves requests
    - If coordinator busy, requests queue
    - No parallel sharding for high throughput
    """
    print_header("BASIC HYBRID ROUTER (Single Coordinator)")

    print("Configuration:")
    print("  â€¢ Ollama pool for small models (task distribution)")
    print("  â€¢ 1 coordinator with 2 RPC backends (model sharding)")
    print("  â€¢ Sequential request processing\n")

    # Example RPC backends (would be auto-discovered in production)
    rpc_backends = [
        {"host": "192.168.1.100", "port": 50052},
        {"host": "192.168.1.101", "port": 50052},
    ]

    router = HybridRouter(
        ollama_pool=OllamaPool.auto_configure(),
        rpc_backends=rpc_backends,
        coordinator_port=18080,
        enable_distributed=True
    )

    print("Routing behavior:")
    print("  llama3.2:3b  â†’ Ollama pool (small model)")
    print("  llama3.1:70b â†’ Single RPC coordinator (large model)")
    print("  llama3.1:405b â†’ Single RPC coordinator (huge model)")

    print("\nRequest flow:")
    print("  Request 1 â†’ Coordinator (port 18080)")
    print("  Request 2 â†’ WAITS for coordinator to finish")
    print("  Request 3 â†’ WAITS in queue")

    print("\nâœ… Good for: 2-4 RPC backends, low concurrent load")
    print("âŒ Not optimal for: High throughput, many concurrent requests")


async def demo_ray_hybrid_router():
    """
    Ray HybridRouter - Multiple Parallel Coordinators

    Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama Poolâ”‚  â† Small models (task distribution)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              Ray Actor Pool System                  â”‚
    â”‚                                                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚  Pool 0 (18080)  â”‚    â”‚  Pool 1 (18081)  â”‚     â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”     â”‚     â”‚
    â”‚  â”‚  â”‚RPC1â”‚RPC2â”‚     â”‚    â”‚  â”‚RPC3â”‚RPC4â”‚     â”‚     â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜     â”‚    â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜     â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                                                     â”‚
    â”‚  Ray automatically:                                â”‚
    â”‚  â€¢ Picks least busy pool                          â”‚
    â”‚  â€¢ Queues requests if all busy                    â”‚
    â”‚  â€¢ Restarts failed pools                          â”‚
    â”‚  â€¢ Tracks resource usage                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Benefits:
    âœ… Parallel request processing (2x throughput with 2 pools)
    âœ… Automatic load balancing by Ray
    âœ… Fault tolerance with auto-restart
    âœ… Better GPU utilization
    """
    print_header("RAY HYBRID ROUTER (Multiple Parallel Coordinators)")

    print("Configuration:")
    print("  â€¢ Ollama pool for small models (task distribution)")
    print("  â€¢ 2 coordinator pools with 2 RPC backends each (parallel sharding)")
    print("  â€¢ Parallel request processing via Ray\n")

    # Example: 4 RPC backends split into 2 pools
    rpc_backends = [
        {"host": "192.168.1.100", "port": 50052},
        {"host": "192.168.1.101", "port": 50052},
        {"host": "192.168.1.102", "port": 50052},
        {"host": "192.168.1.103", "port": 50052},
    ]

    router = RayHybridRouter(
        ollama_pool=OllamaPool.auto_configure(),
        rpc_backends=rpc_backends,
        coordinator_base_port=18080,
        backends_per_pool=2,  # 2 backends per pool = 2 pools
        enable_distributed=True
    )

    print("Pool distribution:")
    print("  Pool 0 (port 18080): RPC backends 1, 2")
    print("  Pool 1 (port 18081): RPC backends 3, 4")

    print("\nRouting behavior:")
    print("  llama3.2:3b  â†’ Ollama pool (small model)")
    print("  llama3.1:70b â†’ Ray pool (Ray picks least busy)")
    print("  llama3.1:405b â†’ Ray pool (Ray picks least busy)")

    print("\nRequest flow (parallel):")
    print("  Request 1 â†’ Pool 0 (port 18080)")
    print("  Request 2 â†’ Pool 1 (port 18081)  â† PARALLEL!")
    print("  Request 3 â†’ Pool 0 (if free) or Pool 1 (if Pool 0 busy)")

    print("\nâœ… Good for: 4+ RPC backends, high throughput needs")
    print("âœ… Automatic: Load balancing, fault tolerance, resource tracking")

    # Show stats
    stats = router.get_stats()
    print("\nRay pool stats:")
    print(f"  Pools: {stats['ray_pools']['num_pools']}")
    print(f"  Backends per pool: {stats['ray_pools']['backends_per_pool']}")
    print(f"  Total backends: {stats['ray_pools']['total_backends']}")

    await router.shutdown()


async def demo_performance_comparison():
    """
    Performance comparison for concurrent requests.
    """
    print_header("PERFORMANCE COMPARISON (Concurrent Requests)")

    print("Scenario: 4 concurrent requests for llama3.1:70b\n")

    print("Basic HybridRouter (1 coordinator):")
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Req1 â”‚ â†’ Coordinator â†’ Response (2s)")
    print("  â””â”€â”€â”€â”€â”€â”€â”˜")
    print("         â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("         â”‚ Req2 â”‚ â†’ WAIT â†’ Coordinator â†’ Response (4s)")
    print("         â””â”€â”€â”€â”€â”€â”€â”˜")
    print("                â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("                â”‚ Req3 â”‚ â†’ WAIT â†’ Coordinator â†’ Response (6s)")
    print("                â””â”€â”€â”€â”€â”€â”€â”˜")
    print("                       â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("                       â”‚ Req4 â”‚ â†’ WAIT â†’ Coordinator â†’ Response (8s)")
    print("                       â””â”€â”€â”€â”€â”€â”€â”˜")
    print("  Total time: ~8 seconds (sequential)")

    print("\nRay HybridRouter (2 pools):")
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Req1 â”‚ â†’ Pool 0 â†’ Response (2s)")
    print("  â””â”€â”€â”€â”€â”€â”€â”˜")
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("  â”‚ Req2 â”‚ â†’ Pool 1 â†’ Response (2s)  â† PARALLEL!")
    print("  â””â”€â”€â”€â”€â”€â”€â”˜")
    print("         â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("         â”‚ Req3 â”‚ â†’ Pool 0 â†’ Response (4s)")
    print("         â””â”€â”€â”€â”€â”€â”€â”˜")
    print("         â”Œâ”€â”€â”€â”€â”€â”€â”")
    print("         â”‚ Req4 â”‚ â†’ Pool 1 â†’ Response (4s)  â† PARALLEL!")
    print("         â””â”€â”€â”€â”€â”€â”€â”˜")
    print("  Total time: ~4 seconds (parallel)")

    print("\nğŸš€ Ray HybridRouter: 2x throughput with 2 pools!")
    print("ğŸš€ With 4 pools: 4x throughput!")


async def main():
    """Run all demos."""
    print("\n" + "â–ˆ" * 80)
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" + "  SOLLOL: Basic HybridRouter vs Ray HybridRouter".center(78) + "â–ˆ")
    print("â–ˆ" + " " * 78 + "â–ˆ")
    print("â–ˆ" * 80)

    await demo_basic_hybrid_router()
    await demo_ray_hybrid_router()
    await demo_performance_comparison()

    print_header("RECOMMENDATION")

    print("Use Basic HybridRouter when:")
    print("  â€¢ You have 2-4 RPC backends")
    print("  â€¢ Low to moderate concurrent load")
    print("  â€¢ Simplicity is preferred")

    print("\nUse Ray HybridRouter when:")
    print("  â€¢ You have 4+ RPC backends")
    print("  â€¢ High concurrent load (multiple users)")
    print("  â€¢ Need maximum throughput")
    print("  â€¢ Want automatic fault tolerance")

    print("\n" + "=" * 80)
    print("âœ¨ Ray HybridRouter is the future of SOLLOL distributed inference! âœ¨")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
