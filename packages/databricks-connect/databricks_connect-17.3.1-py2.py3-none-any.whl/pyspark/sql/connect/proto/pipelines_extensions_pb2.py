#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: spark/connect/pipelines_extensions.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from pyspark.sql.connect.proto import expressions_pb2 as spark_dot_connect_dot_expressions__pb2
from pyspark.sql.connect.proto import pipelines_pb2 as spark_dot_connect_dot_pipelines__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n(spark/connect/pipelines_extensions.proto\x12\rspark.connect\x1a\x1fspark/connect/expressions.proto\x1a\x1dspark/connect/pipelines.proto"\xe3\t\n\x12\x41utoCdcFlowDetails\x12\x1b\n\x06source\x18\x01 \x01(\tH\x00R\x06source\x88\x01\x01\x12-\n\x04keys\x18\x02 \x03(\x0b\x32\x19.spark.connect.ExpressionR\x04keys\x12?\n\x0bsequence_by\x18\x03 \x01(\x0b\x32\x19.spark.connect.ExpressionH\x01R\nsequenceBy\x88\x01\x01\x12\x34\n\x05where\x18\x04 \x01(\x0b\x32\x19.spark.connect.ExpressionH\x02R\x05where\x88\x01\x01\x12\x33\n\x13ignore_null_updates\x18\x05 \x01(\x08H\x03R\x11ignoreNullUpdates\x88\x01\x01\x12H\n\x10\x61pply_as_deletes\x18\x06 \x01(\x0b\x32\x19.spark.connect.ExpressionH\x04R\x0e\x61pplyAsDeletes\x88\x01\x01\x12L\n\x12\x61pply_as_truncates\x18\x07 \x01(\x0b\x32\x19.spark.connect.ExpressionH\x05R\x10\x61pplyAsTruncates\x88\x01\x01\x12:\n\x0b\x63olumn_list\x18\x08 \x03(\x0b\x32\x19.spark.connect.ExpressionR\ncolumnList\x12G\n\x12\x65xcept_column_list\x18\t \x03(\x0b\x32\x19.spark.connect.ExpressionR\x10\x65xceptColumnList\x12\x43\n\x12stored_as_scd_type\x18\n \x01(\x0e\x32\x16.spark.connect.SCDTypeR\x0fstoredAsScdType\x12T\n\x19track_history_column_list\x18\x0b \x03(\x0b\x32\x19.spark.connect.ExpressionR\x16trackHistoryColumnList\x12\x61\n track_history_except_column_list\x18\x0c \x03(\x0b\x32\x19.spark.connect.ExpressionR\x1ctrackHistoryExceptColumnList\x12\x17\n\x04once\x18\r \x01(\x08H\x06R\x04once\x88\x01\x01\x12_\n\x1fignore_null_updates_column_list\x18\x0e \x03(\x0b\x32\x19.spark.connect.ExpressionR\x1bignoreNullUpdatesColumnList\x12l\n&ignore_null_updates_except_column_list\x18\x0f \x03(\x0b\x32\x19.spark.connect.ExpressionR!ignoreNullUpdatesExceptColumnList\x12J\n\x11\x63olumns_to_update\x18\x10 \x01(\x0b\x32\x19.spark.connect.ExpressionH\x07R\x0f\x63olumnsToUpdate\x88\x01\x01\x42\t\n\x07_sourceB\x0e\n\x0c_sequence_byB\x08\n\x06_whereB\x16\n\x14_ignore_null_updatesB\x13\n\x11_apply_as_deletesB\x15\n\x13_apply_as_truncatesB\x07\n\x05_onceB\x14\n\x12_columns_to_update"\xce\x03\n\x10\x45\x64geTableDetails\x12\x62\n\rtable_details\x18\x01 \x01(\x0b\x32\x38.spark.connect.PipelineCommand.DefineOutput.TableDetailsH\x00R\x0ctableDetails\x88\x01\x01\x12+\n\x0f\x63luster_by_auto\x18\x02 \x01(\x08H\x01R\rclusterByAuto\x88\x01\x01\x12"\n\nrow_filter\x18\x03 \x01(\tH\x02R\trowFilter\x88\x01\x01\x12G\n\x08sql_conf\x18\x04 \x03(\x0b\x32,.spark.connect.EdgeTableDetails.SqlConfEntryR\x07sqlConf\x12\x1d\n\x07private\x18\x05 \x01(\x08H\x03R\x07private\x88\x01\x01\x12\x17\n\x04path\x18\x06 \x01(\tH\x04R\x04path\x88\x01\x01\x1a:\n\x0cSqlConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x42\x10\n\x0e_table_detailsB\x12\n\x10_cluster_by_autoB\r\n\x0b_row_filterB\n\n\x08_privateB\x07\n\x05_path*C\n\x07SCDType\x12\x18\n\x14SCD_TYPE_UNSPECIFIED\x10\x00\x12\x0e\n\nSCD_TYPE_1\x10\x01\x12\x0e\n\nSCD_TYPE_2\x10\x02\x42&\n"com.databricks.spark.connect.protoP\x01\x62\x06proto3'
)

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(
    DESCRIPTOR, "pyspark.sql.connect.proto.pipelines_extensions_pb2", globals()
)
if _descriptor._USE_C_DESCRIPTORS == False:
    DESCRIPTOR._options = None
    DESCRIPTOR._serialized_options = b'\n"com.databricks.spark.connect.protoP\001'
    _EDGETABLEDETAILS_SQLCONFENTRY._options = None
    _EDGETABLEDETAILS_SQLCONFENTRY._serialized_options = b"8\001"
    _SCDTYPE._serialized_start = 1842
    _SCDTYPE._serialized_end = 1909
    _AUTOCDCFLOWDETAILS._serialized_start = 124
    _AUTOCDCFLOWDETAILS._serialized_end = 1375
    _EDGETABLEDETAILS._serialized_start = 1378
    _EDGETABLEDETAILS._serialized_end = 1840
    _EDGETABLEDETAILS_SQLCONFENTRY._serialized_start = 1708
    _EDGETABLEDETAILS_SQLCONFENTRY._serialized_end = 1766
# @@protoc_insertion_point(module_scope)
