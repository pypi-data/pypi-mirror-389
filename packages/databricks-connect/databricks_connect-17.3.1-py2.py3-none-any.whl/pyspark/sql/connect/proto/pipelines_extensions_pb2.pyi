#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file

DATABRICKS CONFIDENTIAL & PROPRIETARY
__________________

Copyright 2025-present Databricks, Inc.
All Rights Reserved.

NOTICE:  All information contained herein is, and remains the property of Databricks, Inc.
and its suppliers, if any.  The intellectual and technical concepts contained herein are
proprietary to Databricks, Inc. and its suppliers and may be covered by U.S. and foreign Patents,
patents in process, and are protected by trade secret and/or copyright law. Dissemination, use,
or reproduction of this information is strictly forbidden unless prior written permission is
obtained from Databricks, Inc.

If you view or obtain a copy of this information and believe Databricks, Inc. may not have
intended it to be made available, please promptly report it to Databricks Legal Department
@ legal@databricks.com.
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import pyspark.sql.connect.proto.expressions_pb2
import pyspark.sql.connect.proto.pipelines_pb2
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _SCDType:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _SCDTypeEnumTypeWrapper(
    google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_SCDType.ValueType], builtins.type
):  # noqa: F821
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    SCD_TYPE_UNSPECIFIED: _SCDType.ValueType  # 0
    SCD_TYPE_1: _SCDType.ValueType  # 1
    SCD_TYPE_2: _SCDType.ValueType  # 2

class SCDType(_SCDType, metaclass=_SCDTypeEnumTypeWrapper): ...

SCD_TYPE_UNSPECIFIED: SCDType.ValueType  # 0
SCD_TYPE_1: SCDType.ValueType  # 1
SCD_TYPE_2: SCDType.ValueType  # 2
global___SCDType = SCDType

class AutoCdcFlowDetails(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    SOURCE_FIELD_NUMBER: builtins.int
    KEYS_FIELD_NUMBER: builtins.int
    SEQUENCE_BY_FIELD_NUMBER: builtins.int
    WHERE_FIELD_NUMBER: builtins.int
    IGNORE_NULL_UPDATES_FIELD_NUMBER: builtins.int
    APPLY_AS_DELETES_FIELD_NUMBER: builtins.int
    APPLY_AS_TRUNCATES_FIELD_NUMBER: builtins.int
    COLUMN_LIST_FIELD_NUMBER: builtins.int
    EXCEPT_COLUMN_LIST_FIELD_NUMBER: builtins.int
    STORED_AS_SCD_TYPE_FIELD_NUMBER: builtins.int
    TRACK_HISTORY_COLUMN_LIST_FIELD_NUMBER: builtins.int
    TRACK_HISTORY_EXCEPT_COLUMN_LIST_FIELD_NUMBER: builtins.int
    ONCE_FIELD_NUMBER: builtins.int
    IGNORE_NULL_UPDATES_COLUMN_LIST_FIELD_NUMBER: builtins.int
    IGNORE_NULL_UPDATES_EXCEPT_COLUMN_LIST_FIELD_NUMBER: builtins.int
    COLUMNS_TO_UPDATE_FIELD_NUMBER: builtins.int
    source: builtins.str
    """The name of the CDC source to stream from."""
    @property
    def keys(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Column(s) that uniquely identify a row in source and target data."""
    @property
    def sequence_by(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Expression to order the source data."""
    @property
    def where(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Optional condition applied to source and target for optimizations like partition pruning."""
    ignore_null_updates: builtins.bool
    """Whether to ignore null values in source data updates."""
    @property
    def apply_as_deletes(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Delete condition for the merged operation."""
    @property
    def apply_as_truncates(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Truncate condition for the merged operation."""
    @property
    def column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns included in the output table."""
    @property
    def except_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns excluded from the output table."""
    stored_as_scd_type: global___SCDType.ValueType
    """SCD Type for target table"""
    @property
    def track_history_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns tracked for change history."""
    @property
    def track_history_except_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Columns not tracked for change history."""
    once: builtins.bool
    """If true, this flow runs only once."""
    @property
    def ignore_null_updates_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Subset of columns to ignore null in updates."""
    @property
    def ignore_null_updates_except_column_list(
        self,
    ) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[
        pyspark.sql.connect.proto.expressions_pb2.Expression
    ]:
        """Subset of columns excluded from ignoring null in updates."""
    @property
    def columns_to_update(self) -> pyspark.sql.connect.proto.expressions_pb2.Expression:
        """Column indicating which user columns to update or ignore."""
    def __init__(
        self,
        *,
        source: builtins.str | None = ...,
        keys: collections.abc.Iterable[pyspark.sql.connect.proto.expressions_pb2.Expression]
        | None = ...,
        sequence_by: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        where: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        ignore_null_updates: builtins.bool | None = ...,
        apply_as_deletes: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        apply_as_truncates: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
        column_list: collections.abc.Iterable[pyspark.sql.connect.proto.expressions_pb2.Expression]
        | None = ...,
        except_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        stored_as_scd_type: global___SCDType.ValueType = ...,
        track_history_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        track_history_except_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        once: builtins.bool | None = ...,
        ignore_null_updates_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        ignore_null_updates_except_column_list: collections.abc.Iterable[
            pyspark.sql.connect.proto.expressions_pb2.Expression
        ]
        | None = ...,
        columns_to_update: pyspark.sql.connect.proto.expressions_pb2.Expression | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_apply_as_deletes",
            b"_apply_as_deletes",
            "_apply_as_truncates",
            b"_apply_as_truncates",
            "_columns_to_update",
            b"_columns_to_update",
            "_ignore_null_updates",
            b"_ignore_null_updates",
            "_once",
            b"_once",
            "_sequence_by",
            b"_sequence_by",
            "_source",
            b"_source",
            "_where",
            b"_where",
            "apply_as_deletes",
            b"apply_as_deletes",
            "apply_as_truncates",
            b"apply_as_truncates",
            "columns_to_update",
            b"columns_to_update",
            "ignore_null_updates",
            b"ignore_null_updates",
            "once",
            b"once",
            "sequence_by",
            b"sequence_by",
            "source",
            b"source",
            "where",
            b"where",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_apply_as_deletes",
            b"_apply_as_deletes",
            "_apply_as_truncates",
            b"_apply_as_truncates",
            "_columns_to_update",
            b"_columns_to_update",
            "_ignore_null_updates",
            b"_ignore_null_updates",
            "_once",
            b"_once",
            "_sequence_by",
            b"_sequence_by",
            "_source",
            b"_source",
            "_where",
            b"_where",
            "apply_as_deletes",
            b"apply_as_deletes",
            "apply_as_truncates",
            b"apply_as_truncates",
            "column_list",
            b"column_list",
            "columns_to_update",
            b"columns_to_update",
            "except_column_list",
            b"except_column_list",
            "ignore_null_updates",
            b"ignore_null_updates",
            "ignore_null_updates_column_list",
            b"ignore_null_updates_column_list",
            "ignore_null_updates_except_column_list",
            b"ignore_null_updates_except_column_list",
            "keys",
            b"keys",
            "once",
            b"once",
            "sequence_by",
            b"sequence_by",
            "source",
            b"source",
            "stored_as_scd_type",
            b"stored_as_scd_type",
            "track_history_column_list",
            b"track_history_column_list",
            "track_history_except_column_list",
            b"track_history_except_column_list",
            "where",
            b"where",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_apply_as_deletes", b"_apply_as_deletes"]
    ) -> typing_extensions.Literal["apply_as_deletes"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_apply_as_truncates", b"_apply_as_truncates"]
    ) -> typing_extensions.Literal["apply_as_truncates"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_columns_to_update", b"_columns_to_update"]
    ) -> typing_extensions.Literal["columns_to_update"] | None: ...
    @typing.overload
    def WhichOneof(
        self,
        oneof_group: typing_extensions.Literal["_ignore_null_updates", b"_ignore_null_updates"],
    ) -> typing_extensions.Literal["ignore_null_updates"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_once", b"_once"]
    ) -> typing_extensions.Literal["once"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_sequence_by", b"_sequence_by"]
    ) -> typing_extensions.Literal["sequence_by"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_source", b"_source"]
    ) -> typing_extensions.Literal["source"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_where", b"_where"]
    ) -> typing_extensions.Literal["where"] | None: ...

global___AutoCdcFlowDetails = AutoCdcFlowDetails

class EdgeTableDetails(google.protobuf.message.Message):
    """LDP tables supports several fields that are not supported by OSS Spark pipelines. These are
    captured here. This message is meant to be used in the extension field of
    PipelineCommand.DefineOutput.details.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class SqlConfEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.str
        value: builtins.str
        def __init__(
            self,
            *,
            key: builtins.str = ...,
            value: builtins.str = ...,
        ) -> None: ...
        def ClearField(
            self, field_name: typing_extensions.Literal["key", b"key", "value", b"value"]
        ) -> None: ...

    TABLE_DETAILS_FIELD_NUMBER: builtins.int
    CLUSTER_BY_AUTO_FIELD_NUMBER: builtins.int
    ROW_FILTER_FIELD_NUMBER: builtins.int
    SQL_CONF_FIELD_NUMBER: builtins.int
    PRIVATE_FIELD_NUMBER: builtins.int
    PATH_FIELD_NUMBER: builtins.int
    @property
    def table_details(
        self,
    ) -> pyspark.sql.connect.proto.pipelines_pb2.PipelineCommand.DefineOutput.TableDetails: ...
    cluster_by_auto: builtins.bool
    """Whether clustering columns should be auto selected."""
    row_filter: builtins.str
    """A row filter SQL clause that filters the rows in the table."""
    @property
    def sql_conf(self) -> google.protobuf.internal.containers.ScalarMap[builtins.str, builtins.str]:
        """SQL configurations to apply to all flows that target the table."""
    private: builtins.bool
    """Specifies that this table should be private. For UC pipelines, private tables
    are created with a disambiguated name and are hidden from users. For HMS pipelines, they are
    excluded from being added to the metastore
    """
    path: builtins.str
    """The path to the table."""
    def __init__(
        self,
        *,
        table_details: pyspark.sql.connect.proto.pipelines_pb2.PipelineCommand.DefineOutput.TableDetails
        | None = ...,
        cluster_by_auto: builtins.bool | None = ...,
        row_filter: builtins.str | None = ...,
        sql_conf: collections.abc.Mapping[builtins.str, builtins.str] | None = ...,
        private: builtins.bool | None = ...,
        path: builtins.str | None = ...,
    ) -> None: ...
    def HasField(
        self,
        field_name: typing_extensions.Literal[
            "_cluster_by_auto",
            b"_cluster_by_auto",
            "_path",
            b"_path",
            "_private",
            b"_private",
            "_row_filter",
            b"_row_filter",
            "_table_details",
            b"_table_details",
            "cluster_by_auto",
            b"cluster_by_auto",
            "path",
            b"path",
            "private",
            b"private",
            "row_filter",
            b"row_filter",
            "table_details",
            b"table_details",
        ],
    ) -> builtins.bool: ...
    def ClearField(
        self,
        field_name: typing_extensions.Literal[
            "_cluster_by_auto",
            b"_cluster_by_auto",
            "_path",
            b"_path",
            "_private",
            b"_private",
            "_row_filter",
            b"_row_filter",
            "_table_details",
            b"_table_details",
            "cluster_by_auto",
            b"cluster_by_auto",
            "path",
            b"path",
            "private",
            b"private",
            "row_filter",
            b"row_filter",
            "sql_conf",
            b"sql_conf",
            "table_details",
            b"table_details",
        ],
    ) -> None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_cluster_by_auto", b"_cluster_by_auto"]
    ) -> typing_extensions.Literal["cluster_by_auto"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_path", b"_path"]
    ) -> typing_extensions.Literal["path"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_private", b"_private"]
    ) -> typing_extensions.Literal["private"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_row_filter", b"_row_filter"]
    ) -> typing_extensions.Literal["row_filter"] | None: ...
    @typing.overload
    def WhichOneof(
        self, oneof_group: typing_extensions.Literal["_table_details", b"_table_details"]
    ) -> typing_extensions.Literal["table_details"] | None: ...

global___EdgeTableDetails = EdgeTableDetails
