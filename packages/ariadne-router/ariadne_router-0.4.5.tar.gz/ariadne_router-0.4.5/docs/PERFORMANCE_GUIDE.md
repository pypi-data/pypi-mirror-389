# ⚡ Ariadne Performance Guide

## Performance Overview

Ariadne provides intelligent quantum circuit simulation with automatic backend selection and GPU acceleration. This guide covers performance characteristics, optimization tips, and benchmark results.

## Key Performance Features

### **Automatic Backend Selection**
- **Clifford Circuits** → STIM (infinite capacity) or Qiskit (6.0 capacity)
- **Non-Clifford Circuits** → CUDA (10.0 capacity) or Tensor Network (9.0 capacity)
- **Large Circuits** → CUDA for maximum performance
- **Small Circuits** → CPU backends for efficiency

### **GPU Acceleration**
- **NVIDIA CUDA**: Up to 5.5x speedups on RTX 3080
- **Automatic Fallback**: CPU when GPU unavailable
- **Memory Management**: Efficient GPU memory usage
- **Cross-Platform**: Works on Windows, Linux, macOS

## Benchmark Results

### **Hardware Configuration**
- **GPU**: NVIDIA GeForce RTX 3080 (10GB, Compute 8.6)
- **CPU**: Modern multi-core processor
- **Memory**: 32GB RAM
- **CUDA**: Version 12.x

### **Performance Results**
*Tested with mixed Clifford/non-Clifford circuits on NVIDIA RTX 3080 with CUDA 12.x*

| Circuit Size | Circuit Type | CPU Time | CUDA Time | Speedup | Notes |
|--------------|--------------|----------|-----------|---------|--------|
| 16 qubits | Non-Clifford | 0.50s | 0.11s | **4.5x** | GPU advantage begins |
| 20 qubits | Non-Clifford | 2.10s | 0.45s | **4.7x** | Significant GPU benefit |
| 24 qubits | Non-Clifford | 8.20s | 1.50s | **5.5x** | Memory efficient GPU |
| 16 qubits | Clifford | 0.30s | 0.25s | 1.2x | CPU often faster for Clifford |
| 20 qubits | Clifford | 1.20s | 0.80s | 1.5x | Moderate GPU benefit |

**Performance Notes:**
- Results vary based on hardware, circuit structure, and available memory
- GPU acceleration is most beneficial for non-Clifford circuits > 16 qubits
- CPU backends are often faster for small circuits due to GPU overhead
- Memory usage scales exponentially with qubit count

### **Backend Selection Intelligence**
- **Non-Clifford circuits**: CUDA selected when beneficial (circuit size dependent)
- **Clifford circuits**: STIM or Qiskit selected based on availability and performance
- **Large circuits**: CUDA preferred when memory allows
- **Adaptive selection**: Backend choice adapts to your hardware and circuit characteristics

## Optimization Tips

### **For Maximum Performance**
1. **Use large circuits** (16+ qubits) to benefit from GPU acceleration
2. **Include non-Clifford gates** (T, S, etc.) to trigger CUDA selection
3. **Ensure CUDA is available** for optimal performance
4. **Use appropriate shot counts** (1000+ for statistical significance)

### **For Memory Efficiency**
1. **Monitor GPU memory usage** for very large circuits
2. **Use CPU fallback** for memory-constrained environments
3. **Optimize circuit depth** to reduce memory requirements
4. **Consider circuit decomposition** for extremely large circuits

### **For Development**
1. **Test with small circuits** first for rapid iteration
2. **Use CPU backends** for debugging and development
3. **Profile performance** with different circuit sizes
4. **Validate results** across different backends

## Performance Characteristics

### **GPU Acceleration Threshold**
- **Small circuits** (< 16 qubits): CPU preferred due to GPU overhead
- **Medium circuits** (16 qubits): Break-even point
- **Large circuits** (> 16 qubits): CUDA provides significant speedups

### **Performance Variability**
- **Hardware dependent**: Results vary significantly between different GPUs and CPUs
- **Circuit dependent**: Performance benefits depend on gate types and structure
- **Memory dependent**: Large circuits may fail or slow down due to memory constraints
- **Environment dependent**: Other GPU applications can impact performance

### **Realistic Expectations**
- **Not always faster**: GPU acceleration has overhead for small circuits
- **Not guaranteed**: Performance improvements depend on your specific setup
- **Memory limited**: Circuit size limited by available RAM (CPU) or VRAM (GPU)
- **Adaptive system**: Ariadne automatically chooses the best available option

### **Memory Usage Patterns**
- **Statevector size**: 2^n complex numbers for n qubits
- **RTX 3080**: Can handle up to ~24 qubits (16GB statevector)
- **CPU fallback**: Automatic when GPU memory insufficient

### **Backend Selection Logic**
```python
# Simplified selection logic
if circuit.is_clifford():
    if stim_available:
        return STIM  # Infinite capacity
    else:
        return QISKIT  # 6.0 capacity
else:
    if cuda_available and circuit.num_qubits >= 16:
        return CUDA  # 10.0 capacity
    else:
        return TENSOR_NETWORK  # 9.0 capacity
```

## Best Practices

### **Circuit Design**
- **Use appropriate gate sets** for your target backend
- **Minimize circuit depth** when possible
- **Consider circuit decomposition** for very large circuits
- **Validate results** across different backends

### **Performance Testing**
- **Test with realistic circuit sizes** for your use case
- **Use appropriate shot counts** for statistical significance
- **Profile memory usage** for large circuits
- **Validate backend selection** for different circuit types

### Continuous Performance Regression Monitoring
Ariadne runs the lightweight `pytest -m performance` suite nightly on Ubuntu (see the `router-performance` job in `.github/workflows/quantum-regression.yml`). The current baseline enforces a 1.5s wall-clock budget for the routed Qiskit fallback path in `tests/performance/test_router_regressions.py`.

When the job fails:

1. Download `performance-results.xml` from the workflow artifacts to inspect failing cases and recorded durations.
2. Reproduce locally with `pytest tests/performance -m performance --durations=5` to observe the regression and validate that it is deterministic.
3. If the regression is legitimate, raise an issue linked to the offending change and update the baseline threshold after capturing new reference timings. If the failure is noise-driven, document the environment factors (CPU load, virtualization) before adjusting the ceiling.

All engineers proposing routing or backend changes should monitor this job to ensure we do not silently erode routing latency guarantees.

### **Production Deployment**
- **Ensure CUDA availability** for maximum performance
- **Monitor GPU memory usage** during operation
- **Implement proper error handling** for backend failures
- **Use appropriate logging** for performance monitoring

## Benchmarking Best Practices

### **Creating Reliable Benchmarks**
1. **Warm up the system** before timing measurements
2. **Use consistent hardware** for fair comparisons
3. **Test multiple circuit types** to understand performance characteristics
4. **Measure with sufficient statistical significance** (1000+ shots)

### **Example Benchmark Script**
```python
import time
from ariadne import simulate
from qiskit import QuantumCircuit

def benchmark_circuit(qubits: int, shots: int = 1000, runs: int = 5):
    """Benchmark a circuit with multiple runs for statistical significance."""
    times = []

    for _ in range(runs):
        # Create circuit
        qc = QuantumCircuit(qubits, qubits)
        for i in range(qubits):
            qc.h(i)
            if i % 3 == 0:
                qc.t(i)  # Non-Clifford gates

        for i in range(qubits - 1):
            qc.cx(i, i + 1)

        # Time the simulation
        start = time.perf_counter()
        result = simulate(qc, shots=shots)
        end = time.perf_counter()

        times.append(end - start)

    # Calculate statistics
    avg_time = sum(times) / len(times)
    min_time = min(times)
    max_time = max(times)

    return {
        'avg_time': avg_time,
        'min_time': min_time,
        'max_time': max_time,
        'backend_used': result.backend_used,
        'confidence': result.routing_decision.confidence_score
    }
```

### **Interpreting Results**
- **Compare consistently**: Use the same circuit types and sizes
- **Consider overhead**: Small circuits may show little GPU benefit
- **Check memory usage**: Large circuits may fail due to memory constraints
- **Verify correctness**: Ensure results are consistent across backends

## Future Optimizations

### **Planned Improvements**
- **Multi-GPU support** for even larger circuits
- **Memory optimization** for very large statevectors
- **Custom CUDA kernels** for specific operations
- **Cloud GPU integration** for scalable deployment

### **Research Directions**
- **Machine learning-based** backend selection
- **Adaptive performance tuning** based on circuit characteristics
- **Distributed simulation** across multiple nodes
- **Quantum advantage boundary** analysis
