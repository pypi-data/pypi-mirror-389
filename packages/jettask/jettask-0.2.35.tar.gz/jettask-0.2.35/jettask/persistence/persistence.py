"""ä»»åŠ¡æŒä¹…åŒ–æ¨¡å—

è´Ÿè´£è§£æRedis Streamæ¶ˆæ¯ï¼Œå¹¶å°†ä»»åŠ¡æ•°æ®æ‰¹é‡æ’å…¥PostgreSQLæ•°æ®åº“ã€‚
"""

import json
import logging
import traceback
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone

from sqlalchemy.orm import sessionmaker
from sqlalchemy.dialects.postgresql import insert

from jettask.db.models.task import Task

logger = logging.getLogger(__name__)


class TaskPersistence:
    """ä»»åŠ¡æŒä¹…åŒ–å¤„ç†å™¨

    èŒè´£ï¼š
    - è§£æStreamæ¶ˆæ¯ä¸ºä»»åŠ¡ä¿¡æ¯
    - æ‰¹é‡æ’å…¥ä»»åŠ¡åˆ°PostgreSQLçš„tasksè¡¨
    - å¤„ç†æ’å…¥å¤±è´¥çš„é™çº§ç­–ç•¥
    """

    def __init__(
        self,
        async_session_local: sessionmaker,
        namespace_id: str,
        namespace_name: str
    ):
        """åˆå§‹åŒ–ä»»åŠ¡æŒä¹…åŒ–å¤„ç†å™¨

        Args:
            async_session_local: SQLAlchemyä¼šè¯å·¥å‚
            namespace_id: å‘½åç©ºé—´ID
            namespace_name: å‘½åç©ºé—´åç§°
        """
        self.AsyncSessionLocal = async_session_local
        self.namespace_id = namespace_id
        self.namespace_name = namespace_name

    # def parse_stream_message(self, task_id: str, data: dict) -> Optional[dict]:
    #     """è§£æStreamæ¶ˆæ¯ä¸ºä»»åŠ¡ä¿¡æ¯ï¼ˆè¿”å›å®Œæ•´çš„å­—æ®µï¼‰

    #     Args:
    #         task_id: ä»»åŠ¡IDï¼ˆRedis Stream IDï¼‰
    #         data: æ¶ˆæ¯æ•°æ®

    #     Returns:
    #         è§£æåçš„ä»»åŠ¡ä¿¡æ¯å­—å…¸ï¼Œå¤±è´¥è¿”å›None
    #     """
    #     try:
    #         from jettask.utils.serializer import loads_str

    #         if b'data' in data:
    #             task_data = loads_str(data[b'data'])
    #         else:
    #             task_data = {}
    #             for k, v in data.items():
    #                 key = k.decode('utf-8') if isinstance(k, bytes) else k
    #                 if isinstance(v, bytes):
    #                     try:
    #                         value = loads_str(v)
    #                     except:
    #                         value = str(v)
    #                 else:
    #                     value = v
    #                 task_data[key] = value

    #         # å¦‚æœé…ç½®äº†å‘½åç©ºé—´ï¼Œæ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å±äºè¯¥å‘½åç©ºé—´
    #         # if self.namespace_id:
    #         #     msg_namespace_id = task_data.get('__namespace_id')
    #         #     # å¦‚æœæ¶ˆæ¯æ²¡æœ‰namespace_idä¸”å½“å‰ä¸æ˜¯é»˜è®¤å‘½åç©ºé—´ï¼Œè·³è¿‡
    #         #     if msg_namespace_id != self.namespace_id:
    #         #         if not (msg_namespace_id is None and self.namespace_id == 'default'):
    #         #             logger.debug(f"Skipping message from different namespace: {msg_namespace_id} != {self.namespace_id}")
    #         #             return None

    #         queue_name = task_data['queue']
    #         task_name = task_data.get('name', task_data.get('task', 'unknown'))

    #         created_at = None
    #         if 'trigger_time' in task_data:
    #             try:
    #                 timestamp = float(task_data['trigger_time'])
    #                 created_at = datetime.fromtimestamp(timestamp, tz=timezone.utc)
    #             except:
    #                 pass

    #         # è¿”å›å®Œæ•´çš„å­—æ®µï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯èƒ½ä¸ºNoneçš„å­—æ®µ
    #         return {
    #             'id': task_id,
    #             'queue_name': queue_name,
    #             'task_name': task_name,
    #             'task_data': json.dumps(task_data),
    #             'priority': int(task_data.get('priority', 0)),
    #             'delay': float(task_data.get('delay', 0)),  # ğŸ”§ æ·»åŠ  delay å­—æ®µ
    #             'retry_count': int(task_data.get('retry', 0)),
    #             'max_retry': int(task_data.get('max_retry', 3)),
    #             'status': 'pending',
    #             'result': None,  # æ–°ä»»åŠ¡æ²¡æœ‰ç»“æœ
    #             'error_message': None,  # æ–°ä»»åŠ¡æ²¡æœ‰é”™è¯¯ä¿¡æ¯
    #             'created_at': created_at,
    #             'started_at': None,  # æ–°ä»»åŠ¡è¿˜æœªå¼€å§‹
    #             'completed_at': None,  # æ–°ä»»åŠ¡è¿˜æœªå®Œæˆ
    #             'scheduled_task_id': task_data.get('scheduled_task_id'),  # è°ƒåº¦ä»»åŠ¡ID
    #             'metadata': json.dumps(task_data.get('metadata', {})),
    #             'worker_id': None,  # æ–°ä»»åŠ¡è¿˜æœªåˆ†é…worker
    #             'execution_time': None,  # æ–°ä»»åŠ¡è¿˜æ²¡æœ‰æ‰§è¡Œæ—¶é—´
    #             'duration': None,  # æ–°ä»»åŠ¡è¿˜æ²¡æœ‰æŒç»­æ—¶é—´
    #             'namespace_id': self.namespace_id  # æ·»åŠ å‘½åç©ºé—´ID
    #         }

    #     except Exception as e:
    #         logger.error(f"Error parsing stream message for task {task_id}: {e}")
    #         logger.error(traceback.format_exc())
    #         return None

    async def insert_tasks(self, tasks: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ’å…¥ä»»åŠ¡åˆ°PostgreSQLï¼ˆä½¿ç”¨ORMï¼‰

        Args:
            tasks: ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨

        Returns:
            å®é™…æ’å…¥çš„è®°å½•æ•°
        """
        if not tasks:
            return 0

        logger.info(f"Attempting to insert {len(tasks)} tasks to tasks table")

        try:
            async with self.AsyncSessionLocal() as session:
                # å‡†å¤‡tasksè¡¨çš„æ•°æ®
                tasks_data = []
                for task in tasks:
                    task_data = json.loads(task['task_data'])

                    # ä»task_dataä¸­è·å–scheduled_task_id
                    scheduled_task_id = task_data.get('scheduled_task_id') or task.get('scheduled_task_id')

                    # æ ¹æ®æ˜¯å¦æœ‰scheduled_task_idæ¥åˆ¤æ–­ä»»åŠ¡æ¥æº
                    if scheduled_task_id:
                        source = 'scheduler'  # å®šæ—¶ä»»åŠ¡
                    else:
                        source = 'redis_stream'  # æ™®é€šä»»åŠ¡

                    tasks_data.append({
                        'stream_id': task['id'],  # Redis Stream IDä½œä¸ºstream_id
                        'queue': task['queue_name'],
                        'namespace': self.namespace_name,
                        'scheduled_task_id': str(scheduled_task_id) if scheduled_task_id else None,
                        'payload': json.loads(task['task_data']),  # è§£æä¸ºdict
                        'priority': task['priority'],
                        'delay': task.get('delay'),  # ğŸ”§ æ·»åŠ  delay
                        'created_at': task['created_at'],
                        'source': source,
                        'task_metadata': json.loads(task.get('metadata', '{}'))  # å¯¹åº”æ¨¡å‹çš„ task_metadata å­—æ®µ
                    })

                # æ‰¹é‡æ’å…¥ - ä½¿ç”¨ ORM çš„ INSERT ON CONFLICT DO NOTHING
                logger.debug(f"Executing batch insert with {len(tasks_data)} tasks")

                try:
                    # ä½¿ç”¨ PostgreSQL çš„ insert().on_conflict_do_nothing()
                    stmt = insert(Task).values(tasks_data).on_conflict_do_nothing(
                        constraint='tasks_pkey'  # ä¸»é”®å†²çªåˆ™è·³è¿‡
                    )

                    await session.execute(stmt)
                    await session.commit()

                    # ORM çš„ on_conflict_do_nothing ä¸è¿”å› rowcountï¼Œæˆ‘ä»¬å‡è®¾å…¨éƒ¨æˆåŠŸ
                    inserted_count = len(tasks_data)
                    logger.debug(f"Tasks table batch insert transaction completed: {inserted_count} tasks")
                    return inserted_count

                except Exception as e:
                    logger.error(f"Error in batch insert, trying fallback: {e}")
                    await session.rollback()

                    # é™çº§ä¸ºé€æ¡æ’å…¥ï¼ˆæ›´ç¨³å¦¥ï¼‰
                    total_inserted = 0

                    for task_dict in tasks_data:
                        try:
                            stmt = insert(Task).values(**task_dict).on_conflict_do_nothing(
                                constraint='tasks_pkey'
                            )
                            await session.execute(stmt)
                            await session.commit()
                            total_inserted += 1
                        except Exception as single_error:
                            logger.error(f"Failed to insert task {task_dict.get('stream_id')}: {single_error}")
                            await session.rollback()

                    if total_inserted > 0:
                        logger.info(f"Fallback insert completed: {total_inserted} tasks inserted")
                    else:
                        logger.info(f"No new tasks inserted in fallback mode")

                    return total_inserted

        except Exception as e:
            logger.error(f"Error inserting tasks to PostgreSQL: {e}")
            logger.error(traceback.format_exc())
            return 0

    async def batch_insert_tasks(self, tasks: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ’å…¥ä»»åŠ¡ï¼ˆå…¼å®¹ buffer.py è°ƒç”¨æ¥å£ï¼‰

        Args:
            tasks: ä»»åŠ¡è®°å½•åˆ—è¡¨

        Returns:
            å®é™…æ’å…¥çš„è®°å½•æ•°
        """
        if not tasks:
            return 0

        logger.info(f"[BATCH INSERT] æ‰¹é‡æ’å…¥ {len(tasks)} æ¡ä»»åŠ¡...")

        try:
            async with self.AsyncSessionLocal() as session:
                # å‡†å¤‡ ORM æ•°æ®
                insert_data = []
                for record in tasks:
                    # record æ˜¯ä» consumer.py ä¼ å…¥çš„æ ¼å¼
                    insert_data.append({
                        'stream_id': record['stream_id'],
                        'queue': record['queue'],
                        'namespace': record['namespace'],
                        'scheduled_task_id': record.get('scheduled_task_id'),
                        'payload': record.get('payload', {}),
                        'priority': record.get('priority', 0),
                        'delay': record.get('delay', 0),
                        'created_at': record.get('created_at'),
                        'source': record.get('source', 'redis_stream'),
                        'task_metadata': record.get('metadata', {})
                    })

                # æ‰¹é‡æ’å…¥ - ä½¿ç”¨ PostgreSQL çš„ INSERT ON CONFLICT DO NOTHING
                # ä½¿ç”¨çº¦æŸåç§°è€Œä¸æ˜¯åˆ—å
                stmt = insert(Task).values(insert_data).on_conflict_do_nothing(
                    constraint='tasks_pkey'
                )

                await session.execute(stmt)
                await session.commit()

                logger.info(f"[BATCH INSERT] âœ“ æˆåŠŸæ’å…¥ {len(insert_data)} æ¡ä»»åŠ¡")
                return len(insert_data)

        except Exception as e:
            logger.error(f"[BATCH INSERT] âœ— æ‰¹é‡æ’å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def batch_update_tasks(self, updates: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ›´æ–°ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€åˆ° task_runs è¡¨

        ä½¿ç”¨ PostgreSQL çš„ INSERT ... ON CONFLICT DO UPDATE å®ç° UPSERT æ“ä½œï¼Œ
        å¦‚æœè®°å½•å­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ã€‚

        Args:
            updates: æ›´æ–°è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å«ï¼š
                - stream_id: Redis Stream IDï¼ˆä¸»é”®ï¼‰
                - status: ä»»åŠ¡çŠ¶æ€
                - result: æ‰§è¡Œç»“æœ
                - error: é”™è¯¯ä¿¡æ¯
                - started_at: å¼€å§‹æ—¶é—´
                - completed_at: å®Œæˆæ—¶é—´
                - retries: é‡è¯•æ¬¡æ•°

        Returns:
            å®é™…æ›´æ–°çš„è®°å½•æ•°
        """
        if not updates:
            return 0

        # logger.info(f"[BATCH UPDATE] æ‰¹é‡æ›´æ–° {len(updates)} æ¡ä»»åŠ¡çŠ¶æ€...")
        # logger.info(f"[BATCH UPDATE] æ›´æ–°è®°å½•ç¤ºä¾‹: {updates[0] if updates else 'N/A'}")

        try:
            from sqlalchemy.dialects.postgresql import insert
            from ..db.models import TaskRun
            from ..utils.serializer import loads_str
            from datetime import datetime, timezone

            # å¯¹ç›¸åŒ stream_id çš„è®°å½•è¿›è¡Œå»é‡ï¼Œä¿ç•™æœ€æ–°çš„
            # ä½¿ç”¨å­—å…¸ï¼Œkey æ˜¯ stream_idï¼Œvalue æ˜¯è®°å½•ï¼ˆåé¢çš„ä¼šè¦†ç›–å‰é¢çš„ï¼‰
            deduplicated = {}
            for record in updates:
                stream_id = record['stream_id']
                deduplicated[stream_id] = record

            # è½¬æ¢å›åˆ—è¡¨
            unique_updates = list(deduplicated.values())

            if len(unique_updates) < len(updates):
                logger.info(
                    f"[BATCH UPDATE] å»é‡: {len(updates)} æ¡ â†’ {len(unique_updates)} æ¡ "
                    f"(åˆå¹¶äº† {len(updates) - len(unique_updates)} æ¡é‡å¤è®°å½•)"
                )

            async with self.AsyncSessionLocal() as session:
                # å‡†å¤‡ UPSERT æ•°æ®
                upsert_data = []
                for record in unique_updates:
                    logger.debug(f"å¤„ç†è®°å½•: {record}")
                    # è§£æ result å­—æ®µï¼ˆå¦‚æœæ˜¯åºåˆ—åŒ–çš„å­—ç¬¦ä¸²ï¼‰
                    result = record.get('result')
                    if result and isinstance(result, bytes):
                        try:
                            result = loads_str(result)
                        except Exception:
                            result = result.decode('utf-8') if isinstance(result, bytes) else result

                    # è§£æ error å­—æ®µ
                    error = record.get('error')
                    if error and isinstance(error, bytes):
                        error = error.decode('utf-8')

                    # è®¡ç®—æ‰§è¡Œæ—¶é•¿
                    duration = None
                    started_at = record.get('started_at')
                    completed_at = record.get('completed_at')
                    if started_at and completed_at:
                        duration = completed_at - started_at

                    # è§£æ status å­—æ®µ
                    status = record.get('status')
                    if status and isinstance(status, bytes):
                        status = status.decode('utf-8')

                    # è§£æ consumer å­—æ®µ
                    consumer = record.get('consumer')
                    if consumer and isinstance(consumer, bytes):
                        consumer = consumer.decode('utf-8')

                    # ğŸ”§ è·å– task_nameï¼ˆå·²ä» consumer æå–ï¼‰
                    task_name = record.get('task_name')

                    upsert_record = {
                        'stream_id': record['stream_id'],
                        'task_name': task_name,  # ğŸ”§ æ·»åŠ  task_name
                        'status': status,
                        'result': result,
                        'error': error,
                        'started_at': started_at,
                        'completed_at': completed_at,
                        'retries': record.get('retries', 0),
                        'duration': duration,
                        'consumer': consumer,
                        'updated_at': datetime.now(timezone.utc),
                    }
                    logger.debug(f"upsert_record: {upsert_record}")
                    upsert_data.append(upsert_record)

                logger.info(f"[BATCH UPDATE] å‡†å¤‡å†™å…¥ {len(upsert_data)} æ¡è®°å½•")

                # æ‰¹é‡ UPSERT - å¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥
                stmt = insert(TaskRun).values(upsert_data)

                # å®šä¹‰å†²çªæ—¶çš„æ›´æ–°ç­–ç•¥
                # ä½¿ç”¨ COALESCE é¿å…ç”¨ NULL è¦†ç›–å·²æœ‰æ•°æ®
                from sqlalchemy import func
                stmt = stmt.on_conflict_do_update(
                    constraint='task_runs_pkey',
                    set_={
                        # status æ€»æ˜¯æ›´æ–°ï¼ˆçŠ¶æ€å˜åŒ–ï¼‰
                        'status': stmt.excluded.status,
                        # å…¶ä»–å­—æ®µï¼šå¦‚æœæ–°å€¼ä¸æ˜¯ NULLï¼Œåˆ™æ›´æ–°ï¼›å¦åˆ™ä¿ç•™æ—§å€¼
                        'result': func.coalesce(stmt.excluded.result, TaskRun.result),
                        'error': func.coalesce(stmt.excluded.error, TaskRun.error),
                        'started_at': func.coalesce(stmt.excluded.started_at, TaskRun.started_at),
                        'completed_at': func.coalesce(stmt.excluded.completed_at, TaskRun.completed_at),
                        'retries': func.coalesce(stmt.excluded.retries, TaskRun.retries),
                        'duration': func.coalesce(stmt.excluded.duration, TaskRun.duration),
                        'consumer': func.coalesce(stmt.excluded.consumer, TaskRun.consumer),
                        # updated_at æ€»æ˜¯æ›´æ–°
                        'updated_at': stmt.excluded.updated_at,
                    }
                )

                await session.execute(stmt)
                await session.commit()

                logger.info(f"[BATCH UPDATE] âœ“ æˆåŠŸæ›´æ–° {len(upsert_data)} æ¡ä»»åŠ¡çŠ¶æ€")
                return len(upsert_data)

        except Exception as e:
            logger.error(f"[BATCH UPDATE] âœ— æ‰¹é‡æ›´æ–°å¤±è´¥: {e}", exc_info=True)
            return 0
