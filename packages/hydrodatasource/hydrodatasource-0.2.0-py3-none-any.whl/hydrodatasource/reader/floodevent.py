"""
Author: Wenyu Ouyang
Date: 2025-01-19 18:05:00
LastEditTime: 2025-08-17 10:09:14
LastEditors: Wenyu Ouyang
Description: æµåŸŸåœºæ¬¡æ•°æ®å¤„ç†ç±» - ç»§æ‰¿è‡ªSelfMadeHydroDataset
FilePath: \hydromodeld:\Code\hydrodatasource\hydrodatasource\reader\floodevent.py
Copyright (c) 2023-2026 Wenyu Ouyang. All rights reserved.
"""

import glob
import re
import pandas as pd
import numpy as np
import os
import xarray as xr
from datetime import datetime, timedelta
from typing import Any, List, Dict, Optional, Tuple, Union
from hydroutils import hydro_event
from hydrodatasource.utils.utils import streamflow_unit_conv
from hydrodatasource.reader.data_source import SelfMadeHydroDataset
from hydrodatasource.configs.config import CACHE_DIR

class FloodEventDatasource(SelfMadeHydroDataset):
    """
    Flood event dataset processing class

    Inherits from SelfMadeHydroDataset, specifically designed for
    processing individual flood event data, including event extraction functions.
    """

    def __init__(
        self,
        data_path: str,
        dataset_name: str = "songliaorrevents",
        time_unit: Optional[List[str]] = None,
        rain_key: str = "rain",
        pet_key: str = "ES",
        net_rain_key: str = "net_rain",
        obs_flow_key: str = "inflow",
        warmup_length: int = 0,
        **kwargs,
    ):
        """
        Initialize the flood event dataset.

        Parameters
        ----------
        data_path : str
            Path to the data.
        dataset_name : str, optional
            Name of the dataset.
        time_unit : list of str, optional
            List of time units, default is ["3h"].
        rain_key : str, optional
            Key name for rain data, default is "rain".
        net_rain_key : str, optional
            Key name for net rain data, default is "net_rain".
        obs_flow_key : str, optional
            Key name for observed flow data, default is "inflow".
        warmup_length : int, optional
            Number of time steps to include before flood event starts as warmup
            period, default is 0.
        **kwargs
            Additional keyword arguments passed to the parent class.
        """
        if time_unit is None:
            time_unit = ["3h"]

        # Derive delta_t_hours from time_unit
        # ä»time_unitæ¨å¯¼å‡ºæ—¶é—´æ­¥é•¿ï¼ˆå°æ—¶ï¼‰
        primary_time_unit = time_unit[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ—¶é—´å•ä½ä½œä¸ºä¸»è¦å•ä½
        delta_t_hours = FloodEventDatasource._parse_time_unit_to_hours(
            primary_time_unit
        )

        # Store constants as instance attributes
        self.rain_key = rain_key
        self.pet_key = pet_key  # å‡è®¾æ•°æ®é›†ä¸­è’¸æ•£å‘çš„åˆ—åä¸º 'ES'
        self.net_rain_key = net_rain_key
        self.obs_flow_key = obs_flow_key
        self.warmup_length = warmup_length
        self.delta_t_hours = delta_t_hours
        self.delta_t_seconds = self.delta_t_hours * 3600.0

        super().__init__(
            data_path=data_path,
            download=False,
            time_unit=time_unit,
            dataset_name=dataset_name,
            **kwargs,
        )

    def get_constants(self):
        """
        Get the constant values used by this datasource.

        Returns
        -------
        dict
            Dictionary containing constant values with keys:
            - 'net_rain_key': Key name for net rain data
            - 'obs_flow_key': Key name for observed flow data
            - 'delta_t_hours': Time step in hours (derived from time_unit)
            - 'delta_t_seconds': Time step in seconds
            - 'warmup_length': Number of warmup time steps
        """
        return {
            "net_rain_key": self.net_rain_key,
            "obs_flow_key": self.obs_flow_key,
            "delta_t_hours": self.delta_t_hours,
            "delta_t_seconds": self.delta_t_seconds,
            "warmup_length": self.warmup_length,
        }

    @staticmethod
    def _parse_time_unit_to_hours(time_unit_str: str) -> float:
        """
        Parse time unit string to hours.

        Parameters
        ----------
        time_unit_str : str
            Time unit string like "1h", "3h", "1D", "2D", "8D", etc.

        Returns
        -------
        float
            Time step in hours.

        Examples
        --------
        >>> FloodEventDatasource._parse_time_unit_to_hours("3h")
        3.0
        >>> FloodEventDatasource._parse_time_unit_to_hours("1D")
        24.0
        >>> FloodEventDatasource._parse_time_unit_to_hours("2D")
        48.0
        """
        import re

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£ææ—¶é—´å•ä½
        pattern = r"^(\d+)([hdHD])$"
        match = re.match(pattern, time_unit_str.strip())

        if not match:
            raise ValueError(
                f"Invalid time_unit format: '{time_unit_str}'. "
                f"Expected format: <number><unit> where unit is 'h' or 'D' "
                f"(e.g., '1h', '3h', '1D', '2D')"
            )

        number = int(match.group(1))
        unit = match.group(2).upper()

        if unit == "H":
            return float(number)
        elif unit == "D":
            return float(number * 24)
        else:
            # è¿™é‡Œå®é™…ä¸Šä¸ä¼šæ‰§è¡Œåˆ°ï¼Œå› ä¸ºæ­£åˆ™è¡¨è¾¾å¼å·²ç»é™åˆ¶äº†å•ä½
            raise ValueError(f"Unsupported time unit: {unit}")

    def extract_flood_events(
        self, df: pd.DataFrame, include_peak_obs: bool = True
    ) -> List[Dict]:
        """
        æå–æ´ªæ°´äº‹ä»¶å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼å­—å…¸

        Parameters
        ----------
        df : pd.DataFrame
            DataFrame containing station data.
        include_peak_obs : bool, default=True
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼

        Returns
        -------
        List[Dict]
            æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰ç®—æ³•å®Œå…¨å…¼å®¹
        """
        # è°ƒç”¨hydroutilsæå–äº‹ä»¶
        flood_events = hydro_event.extract_flood_events(
            df=df,
            warmup_length=self.warmup_length,
            flood_event_col="flood_event",
            time_col="time",
        )

        all_events = []
        for event in flood_events:
            event_data = event["data"]

            # æå–å„åˆ—æ•°æ®
            rain = event_data[self.rain_key].values
            ES = event_data[self.pet_key].values
            inflow = event_data[self.obs_flow_key].values
            flood_event_markers = event_data["flood_event"].values

            # åˆ›å»ºæ ‡å‡†æ ¼å¼å­—å…¸
            event_dict = self._create_event_dict(
                rain=rain,
                inflow=inflow,
                event_name=event["event_name"],
                ES=ES,
                include_peak_obs=include_peak_obs,
                flood_event_markers=flood_event_markers,
            )

            if event_dict is not None:
                all_events.append(event_dict)

        return all_events

    def _create_event_dict(
        self,
        rain: np.ndarray,
        inflow: np.ndarray,
        event_name: str,
        ES: np.ndarray,
        include_peak_obs: bool = True,
        flood_event_markers: Optional[np.ndarray] = None,
    ) -> Optional[Dict]:
        """
        Transform rain, net_rain and inflow arrays into a standard event dictionary format

        Parameters
        ----------
        rain: np.ndarray
            rain array
        inflow: np.ndarray
            inflow array
        event_name: str
            æ´ªå³°æ—¥æœŸï¼ˆ8ä½æ•°å­—æ ¼å¼ï¼‰
        ES: np.ndarray
            è’¸æ•£å‘æ•°ç»„
        include_peak_obs: bool
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        flood_event_markers: np.ndarray, optional
            flood_event markers indicating which time steps belong to actual
            flood event (>0) vs warmup period (0)

        Returns
        -------
            Dict: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸ï¼Œä¸uh_utils.pyå®Œå…¨å…¼å®¹ï¼Œ
            åŒ…å«flood_event_markersä¿¡æ¯
        """
        try:
            # è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            valid_rain_mask = ~np.isnan(rain) & (rain > 0)
            m_eff = np.sum(valid_rain_mask)

            if m_eff == 0:
                return None

            # éªŒè¯å¾„æµæ•°æ®
            if np.nansum(inflow) < 1e-6:
                return None

            # åˆ›å»ºæ ‡å‡†æ ¼å¼å­—å…¸ï¼ˆä¸uh_utils.pyæœŸæœ›çš„keyå®Œå…¨ä¸€è‡´ï¼‰
            event_dict = {
                self.rain_key: rain,
                self.obs_flow_key: inflow,  # è§‚æµ‹å¾„æµ
                "ES": ES,
                "m_eff": m_eff,  # å‡€é›¨æ—¶æ®µæ•°
                "n_specific": len(rain),  # å•ä½çº¿é•¿åº¦
                "filepath": f"event_{event_name}.csv",  # é¿å…KeyError
            }

            # æ·»åŠ æ´ªæ°´äº‹ä»¶æ ‡è®°ï¼ˆå¦‚æœæä¾›ï¼‰
            if flood_event_markers is not None:
                event_dict["flood_event_markers"] = flood_event_markers

            # æ·»åŠ æ´ªå³°è§‚æµ‹å€¼
            if include_peak_obs:
                peak_flow = np.nanmax(inflow)
                if peak_flow < 1e-6:
                    return {}
                event_dict["peak_obs"] = peak_flow

            return event_dict

        except Exception:
            return {}

    def load_1basin_flood_events(
        self,
        station_id: Optional[str] = None,
        flow_unit: str = "mm/3h",
        include_peak_obs: bool = True,
        verbose: bool = True,
    ) -> Optional[List[Dict]]:
        """
        åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®

        Parameters
        ----------
        station_id:
            æŒ‡å®šç«™ç‚¹IDï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰ç«™ç‚¹
        flow_unit
            Unit of streamflow, default is "mm/3h".
        include_peak_obs:
            æ˜¯å¦åŒ…å«æ´ªå³°è§‚æµ‹å€¼
        verbose:
            æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns
        -------
            List[Dict]: æ ‡å‡†æ ¼å¼çš„äº‹ä»¶å­—å…¸åˆ—è¡¨ï¼Œä¸ç°æœ‰ç®—æ³•å®Œå…¨å…¼å®¹
        """
        # è·å–æµåŸŸé¢ç§¯
        basin_area_km2 = None

        if station_id:
            basin_area_km2 = self.read_area([station_id])
        else:
            basin_area_km2 = None

        try:
            if verbose:
                print("ğŸ”„ æ­£åœ¨åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®...")
                if station_id:
                    print(f"   æŒ‡å®šç«™ç‚¹: {station_id}")

            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=["1960-01-01", "2024-12-31"],
                var_lst=["rain", "inflow", "flood_event", "ES"],
                recache=True,  # å¼ºåˆ¶é‡æ–°ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®æœ€æ–°
            )[self.time_unit[0]]

            xr_ds["inflow"] = streamflow_unit_conv(
                xr_ds[["inflow"]],
                target_unit=flow_unit,
                area=basin_area_km2,
            )["inflow"]
            df = xr_ds.to_dataframe()
            if df is None:
                return None

            # ç›´æ¥æå–æ´ªæ°´äº‹ä»¶å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            all_events = self.extract_flood_events(
                df.loc[station_id].reset_index(), include_peak_obs
            )

            if not all_events:
                if verbose:
                    print(f"  âš ï¸  {station_id}: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ´ªæ°´äº‹ä»¶")
                return None

            if verbose:
                print(f"  âœ… {station_id}: æˆåŠŸå¤„ç† {len(all_events)} ä¸ªæ´ªæ°´äº‹ä»¶")
                print(f"âœ… æ€»å…±æˆåŠŸåŠ è½½ {len(all_events)} ä¸ªæ´ªæ°´äº‹ä»¶")

            return all_events

        except Exception as e:
            if verbose:
                print(f"âŒ åŠ è½½æ´ªæ°´äº‹ä»¶æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def parse_augmented_file_metadata(self, augmented_file_path: str) -> Dict:
        """
        è§£æå¢å¼ºæ–‡ä»¶çš„å…ƒä¿¡æ¯

        Parameters
        ----------
        augmented_file_path : str
            å¢å¼ºæ–‡ä»¶çš„è·¯å¾„

        Returns
        -------
        Dict
            åŒ…å«æºåœºæ¬¡ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬èµ·å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ã€æºæ–‡ä»¶åç­‰
        """
        metadata = {}

        with open(augmented_file_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.startswith("#"):
                    if "Source:" in line:
                        source_file = line.split("Source:")[1].strip()
                        metadata["source_file"] = source_file
                        # ä»æºæ–‡ä»¶åæå–èµ·å§‹æ—¶é—´
                        if "event_" in source_file and ".csv" in source_file:
                            time_part = source_file.replace("event_", "").replace(
                                ".csv", ""
                            )
                            if "_" in time_part:
                                start_time_str, end_time_str = time_part.split("_")
                                metadata["original_start_time"] = start_time_str
                                metadata["original_end_time"] = end_time_str
                    elif "Start Time:" in line:
                        metadata["augmented_start_time"] = line.split("Start Time:")[
                            1
                        ].strip()
                    elif "End Time:" in line:
                        metadata["augmented_end_time"] = line.split("End Time:")[
                            1
                        ].strip()
                    elif "Scale Factor:" in line:
                        metadata["scale_factor"] = float(
                            line.split("Scale Factor:")[1].strip()
                        )
                    elif "Sample ID:" in line:
                        metadata["sample_id"] = int(line.split("Sample ID:")[1].strip())
                else:
                    break

        return metadata

    def get_warmup_period_data(
        self,
        original_start_time: str,
        original_end_time: str,
        station_id: str,
        warmup_hours: int = 240,
    ) -> Optional[pd.DataFrame]:
        """
        è·å–åŸå§‹åœºæ¬¡å‰é¢çš„é¢„çƒ­æœŸæ•°æ®

        Parameters
        ----------
        original_start_time : str
            åŸå§‹åœºæ¬¡èµ·å§‹æ—¶é—´ (YYYYMMDDHHæ ¼å¼)
        original_end_time : str
            åŸå§‹åœºæ¬¡ç»“æŸæ—¶é—´ (YYYYMMDDHHæ ¼å¼)
        station_id : str
            ç«™ç‚¹ID
        warmup_hours : int, optional
            é¢„çƒ­æœŸå°æ—¶æ•°ï¼Œé»˜è®¤240å°æ—¶(10å¤©)

        Returns
        -------
        Optional[pd.DataFrame]
            é¢„çƒ­æœŸæ•°æ®ï¼ŒåŒ…å«time, rain, gen_discharge, obs_dischargeåˆ—
        """
        try:
            # ä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œå¤„ç†æ—¶é—´
            year = original_start_time[:4]
            month = original_start_time[4:6]
            day = original_start_time[6:8]
            hour = original_start_time[8:10]
            
            # æ„é€ æ—¶é—´å­—ç¬¦ä¸²
            start_time = f"{year}-{month}-{day} {hour}:00:00"
            
            # ç”±äºæ—¶é—´è¶…å‡ºpandasèŒƒå›´ï¼Œæˆ‘ä»¬æš‚æ—¶ä½¿ç”¨ä¸€ä¸ªåŸºå‡†å¹´ä»½è¿›è¡Œè®¡ç®—
            base_year = "2000"
            base_start = f"{base_year}-{month}-{day} {hour}:00:00"
            base_start_dt = datetime.strptime(base_start, "%Y-%m-%d %H:%M:%S")
            
            # è®¡ç®—é¢„çƒ­æœŸæ—¶é—´
            base_warmup_start = base_start_dt - timedelta(hours=warmup_hours)
            base_warmup_end = base_start_dt - timedelta(hours=self.delta_t_hours)
            
            # æ›¿æ¢å›åŸå§‹å¹´ä»½
            warmup_start = base_warmup_start.strftime(f"{year}-%m-%d %H:%M:%S")
            warmup_end = base_warmup_end.strftime(f"{year}-%m-%d %H:%M:%S")

            # è¯»å–é¢„çƒ­æœŸæ•°æ®
            xr_ds = self.read_ts_xrdataset(
                gage_id_lst=[station_id],
                t_range=[warmup_start, warmup_end],
                var_lst=["rain","inflow","flood_event","ES"],
            )["3h"]

            if xr_ds is None:
                return None

            # è½¬æ¢ä¸ºDataFrame
            df = xr_ds.to_dataframe().reset_index()
            df = df[df["basin"] == station_id].copy()

            # é‡å‘½ååˆ—ä»¥åŒ¹é…å¢å¼ºæ–‡ä»¶æ ¼å¼
            df = df.rename(columns={"inflow": "obs_discharge"})
            df["gen_discharge"] = df["obs_discharge"]

            return df[["time", "rain", "gen_discharge", "obs_discharge"]]
        except Exception as e:
            print(f"è·å–é¢„çƒ­æœŸæ•°æ®å¤±è´¥: {e}")
            return None

    def adjust_warmup_time_to_augmented_year(
        self, warmup_df: pd.DataFrame, augmented_start_time: str
    ) -> pd.DataFrame:
        """
        è°ƒæ•´é¢„çƒ­æœŸæ•°æ®çš„å¹´ä»½åˆ°å¢å¼ºæ•°æ®çš„å¹´ä»½

        Parameters
        ----------
        warmup_df : pd.DataFrame
            é¢„çƒ­æœŸæ•°æ®
        augmented_start_time : str
            å¢å¼ºæ•°æ®çš„èµ·å§‹æ—¶é—´ (YYYYMMDDHHæ ¼å¼)

        Returns
        -------
        pd.DataFrame
            è°ƒæ•´å¹´ä»½åçš„é¢„çƒ­æœŸæ•°æ®
        """
        df = warmup_df.copy()

        # è·å–å¢å¼ºæ•°æ®çš„å¹´ä»½
        aug_year = augmented_start_time

        # è°ƒæ•´æ—¶é—´åˆ—çš„å¹´ä»½ï¼ˆå­—ç¬¦ä¸²æ“ä½œï¼‰
        df["time"] = df["time"].astype(str)
        df["time"] = df["time"].apply(lambda x: aug_year + x[4:])

        return df

    def concatenate_warmup_and_augmented_data(
        self, warmup_df: pd.DataFrame, augmented_file_path: str
    ) -> pd.DataFrame:
        """
        æ‹¼æ¥é¢„çƒ­æœŸæ•°æ®å’Œå¢å¼ºåœºæ¬¡æ•°æ®

        Parameters
        ----------
        warmup_df : pd.DataFrame
            é¢„çƒ­æœŸæ•°æ®
        augmented_file_path : str
            å¢å¼ºæ–‡ä»¶è·¯å¾„

        Returns
        -------
        pd.DataFrame
            æ‹¼æ¥åçš„å®Œæ•´æ•°æ®
        """
        # è¯»å–å¢å¼ºæ•°æ®
        aug_df = pd.read_csv(augmented_file_path, comment="#")
        # å°†æ—¶é—´åˆ—ä¿æŒä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œé¿å…è¶…å‡ºpandasæ—¶é—´æˆ³èŒƒå›´é™åˆ¶
        aug_df["time"] = aug_df["time"].astype(str)

        # è·å–å¢å¼ºæ•°æ®çš„èµ·å§‹æ—¶é—´ï¼Œç”¨äºè°ƒæ•´é¢„çƒ­æœŸæ•°æ®çš„å¹´ä»½
        if not aug_df.empty:
            # ä»å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´ä¸­æå–å¹´ä»½
            aug_start_time = aug_df["time"].min()[:4]
            # è°ƒæ•´é¢„çƒ­æœŸæ•°æ®çš„å¹´ä»½åˆ°å¢å¼ºæ•°æ®çš„å¹´ä»½
            adjusted_warmup_df = self.adjust_warmup_time_to_augmented_year(
                warmup_df, aug_start_time
            )
        else:
            adjusted_warmup_df = warmup_df

        # ç¡®ä¿æ‰€æœ‰æ—¶é—´åˆ—éƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        adjusted_warmup_df["time"] = adjusted_warmup_df["time"].astype(str)
        
        # ä¸ºé¢„çƒ­æœŸæ•°æ®å’Œå¢å¼ºæ•°æ®æ·»åŠ æ ‡è®°åˆ—
        adjusted_warmup_df['flood_event'] = 0  # é¢„çƒ­æœŸæ•°æ®æ ‡è®°ä¸º0
        aug_df['flood_event'] = 1  # æ´ªæ°´æœŸæ•°æ®æ ‡è®°ä¸º1
        
        # æ‹¼æ¥æ•°æ®å¹¶æŒ‰å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´æ’åº
        combined_df = pd.concat([adjusted_warmup_df, aug_df], ignore_index=True)
        # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒè¿›è¡Œæ’åº
        combined_df = combined_df.sort_values("time", key=lambda x: x.astype(str)).reset_index(drop=True)

        return combined_df

    def rename_dataframe_columns(self, df: pd.DataFrame, custom_mapping: dict = None) -> pd.DataFrame:
        """
        é‡å‘½åæ•°æ®æ¡†çš„åˆ—åï¼ŒåŒ…æ‹¬é»˜è®¤çš„æ˜ å°„å’Œè‡ªå®šä¹‰æ˜ å°„

        Parameters
        ----------
        df : pd.DataFrame
            éœ€è¦é‡å‘½ååˆ—çš„æ•°æ®æ¡†
        custom_mapping : dict, optional
            è‡ªå®šä¹‰çš„åˆ—åæ˜ å°„å­—å…¸ï¼Œä¾‹å¦‚ {'old_name': 'new_name'}

        Returns
        -------
        pd.DataFrame
            åˆ—åé‡å‘½ååçš„æ•°æ®æ¡†
        """
        # é»˜è®¤çš„åˆ—åæ˜ å°„
        default_mapping = {
            'gen_discharge': 'inflow',
            # åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–é»˜è®¤çš„åˆ—åæ˜ å°„
        }

        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰æ˜ å°„ï¼Œåˆ™æ›´æ–°é»˜è®¤æ˜ å°„
        if custom_mapping:
            default_mapping.update(custom_mapping)

        # è·å–æ•°æ®æ¡†ä¸­å®é™…å­˜åœ¨çš„åˆ—
        existing_columns = set(df.columns)
        
        # åªé‡å‘½åå®é™…å­˜åœ¨çš„åˆ—
        mapping_to_apply = {
            old: new for old, new in default_mapping.items() 
            if old in existing_columns
        }

        # å¦‚æœæœ‰éœ€è¦é‡å‘½åçš„åˆ—ï¼Œåˆ™è¿›è¡Œé‡å‘½å
        if mapping_to_apply:
            df = df.rename(columns=mapping_to_apply)
            renamed_cols = ', '.join([f"{old}->{new}" for old, new in mapping_to_apply.items()])
            
        return df

    def create_xarray_dataset_from_augdf(
        self, df: pd.DataFrame, station_id: str, time_unit: str = "3h"
    ) -> xr.Dataset:
        """
        å°†æ—¶é—´åºåˆ—DataFrameè½¬æ¢ä¸ºxarray Datasetæ ¼å¼

        Parameters
        ----------
        df : pd.DataFrame
            æ—¶é—´åºåˆ—æ•°æ®
        station_id : str
            ç«™ç‚¹ID
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"

        Returns
        -------
        xr.Dataset
            xarrayæ ¼å¼çš„æ•°æ®é›†
        """
        # The gen_discharge is the generated discharge by the data augmentation method
        # åˆ›å»ºæ•°æ®é›†å­—å…¸
        data_vars = {}
        
        # æ·»åŠ é™é›¨æ•°æ®
        if "rain" in df.columns:
            data_vars["rain"] = (
                ["time", "basin"],
                df[["rain"]].values.reshape(-1, 1),
            )
            
        # æ·»åŠ ç”Ÿæˆçš„æµé‡æ•°æ®
        if "inflow"  in df.columns:
            data_vars["inflow"] = (
                ["time", "basin"],
                df[["inflow"]].values.reshape(-1, 1),
            )
            
        if "gen_discharge"  in df.columns:
            data_vars["inflow"] = (
                ["time", "basin"],
                df[["gen_discharge"]].values.reshape(-1, 1),
            )
            
        # æ·»åŠ è§‚æµ‹æµé‡æ•°æ®
        # if "obs_discharge" in df.columns:
        #     data_vars["obs_discharge"] = (
        #         ["time", "basin"],
        #         df[["obs_discharge"]].values.reshape(-1, 1),
        #     )
        
        # æ´ªæ°´æœŸæ ‡è®°
        if "flood_event"  in df.columns:
            data_vars["flood_event"] = (
                ["time", "basin"],
                df[["flood_event"]].values.reshape(-1, 1),
            )
            
        # æ·»åŠ è’¸æ•£å‘æ•°æ®
        if "ES" in df.columns:
            data_vars["ES"] = (
                ["time", "basin"],
                df[["ES"]].values.reshape(-1, 1),
            )
            
        # åˆ›å»ºæ•°æ®é›†
        ds = xr.Dataset(
            data_vars,
            coords={"time": df["time"].values, "basin": [station_id]},
        )

        # æ·»åŠ æ•°æ®é›†å±æ€§
        ds.attrs["description"] = "Augmented hydrological time series data"
        ds.attrs["station_id"] = station_id
        ds.attrs["time_unit"] = time_unit
        ds.attrs["creation_time"] = datetime.now().isoformat()

        # ä¸ºæ¯ä¸ªå˜é‡æ·»åŠ  units å±æ€§
        for var_name in ds.data_vars:
            if var_name == "rain":
                ds[var_name].attrs["units"] = f"mm/{time_unit}"  # é™é›¨å•ä½
            elif var_name in ["inflow","gen_discharge", "obs_discharge"]:
                ds[var_name].attrs["units"] = "m^3/s"  # æµé‡å•ä½ï¼ˆåŒ…æ‹¬ç”Ÿæˆçš„å’Œè§‚æµ‹çš„ï¼‰
            elif var_name == "flood_event":
                ds[var_name].attrs["units"] = "dimensionless"  # æ— é‡çº²
            elif var_name == "ES":
                ds[var_name].attrs["units"] = f"mm/{time_unit}"  # è’¸æ•£å‘å•ä½
            else:
                ds[var_name].attrs["units"] = "unknown"  # é»˜è®¤å€¼
            
            # æ·»åŠ å˜é‡æè¿°
            if var_name == "rain":
                ds[var_name].attrs["description"] = "é™é›¨é‡"
            elif var_name == "inflow":
                ds[var_name].attrs["description"] = "ç”Ÿæˆçš„æµé‡"
            elif var_name == "gen_discharge":
                ds[var_name].attrs["description"] = "ç”Ÿæˆçš„æµé‡"
            elif var_name == "obs_discharge":
                ds[var_name].attrs["description"] = "è§‚æµ‹æµé‡"
            elif var_name == "flood_event":
                ds[var_name].attrs["description"] = "æ´ªæ°´äº‹ä»¶æ ‡è®°"
            elif var_name == "ES":
                ds[var_name].attrs["description"] = "è’¸æ•£å‘"

        return ds

    def save_augmented_timeseries_to_cache(
        self, ds: xr.Dataset, station_ids: Union[str, List[str]], time_unit: str = "3h"
    ) -> str:
        """
        å°†å¢å¼ºæ—¶é—´åºåˆ—æ•°æ®ä¿å­˜åˆ°cacheç›®å½•

        Parameters
        ----------
        ds : xr.Dataset
            è¦ä¿å­˜çš„æ•°æ®é›†
        station_ids : Union[str, List[str]]
            ç«™ç‚¹IDæˆ–ç«™ç‚¹IDåˆ—è¡¨
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"

        Returns
        -------
        str
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # å…¼å®¹å•ä¸ªç«™ç‚¹çš„æƒ…å†µ
        if isinstance(station_ids, str):
            station_ids = [station_ids]

        # æ„é€ æ–‡ä»¶åï¼Œå‚è€ƒåŸæœ‰çš„å‘½åè§„åˆ™ï¼ŒåŠ ä¸Šdataaugmentå‰ç¼€
        prefix = f"{self.dataset_name}_dataaugment_"
        first_station = station_ids[0]
        last_station = station_ids[-1]
        cache_file_name = (
            f"{prefix}timeseries_{time_unit}_batch_{first_station}_{last_station}.nc"
        )
        cache_file_path = os.path.join(CACHE_DIR, cache_file_name)

        # ä¿å­˜æ•°æ®
        ds.to_netcdf(cache_file_path)

        return cache_file_path

    def read_ts_xrdataset_augmented(
        self,
        gage_id_lst: Optional[List[str]] = None,
        t_range: Optional[List[str]] = None,
        var_lst: Optional[List[str]] = None,
        time_unit: str = "3h",
        **kwargs,
    ) -> Dict:
        """
        è¯»å–å¢å¼ºçš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œä¼˜å…ˆä»dataaugmentç¼“å­˜æ–‡ä»¶è¯»å–

        Parameters
        ----------
        gage_id_lst : Optional[List[str]], optional
            ç«™ç‚¹IDåˆ—è¡¨
        t_range : Optional[List[str]], optional
            æ—¶é—´èŒƒå›´
        var_lst : Optional[List[str]], optional
            å˜é‡åˆ—è¡¨
        time_unit : str, optional
            æ—¶é—´å•ä½ï¼Œé»˜è®¤"3h"
        **kwargs
            å…¶ä»–å‚æ•°

        Returns
        -------
        Dict
            åŒ…å«å¢å¼ºæ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼ä¸read_ts_xrdatasetä¸€è‡´
        """
        if gage_id_lst is None or len(gage_id_lst) == 0:
            return self.read_ts_xrdataset(gage_id_lst, t_range, var_lst, **kwargs)

        # æ„é€ å¢å¼ºæ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒå¤šæµåŸŸ
        prefix = f"{self.dataset_name}_dataaugment_"
        first_station = gage_id_lst[0]
        last_station = gage_id_lst[-1]
        cache_file_name = (
            f"{prefix}timeseries_{time_unit}_batch_{first_station}_{last_station}.nc"
        )
        cache_file_path = os.path.join(CACHE_DIR, cache_file_name)

        # æ£€æŸ¥å¢å¼ºæ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_file_path):
            try:
                # è¯»å–å¢å¼ºæ•°æ®
                ds = xr.open_dataset(cache_file_path)

                # è¿‡æ»¤ç«™ç‚¹ID
                available_stations = [
                    station
                    for station in gage_id_lst
                    if station in ds.coords.get("gage_id", [])
                ]
                if available_stations:
                    ds = ds.sel(gage_id=available_stations)

                # åº”ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
                if t_range is not None and len(t_range) >= 2:
                    # è·å–æ•°æ®é›†çš„æ—¶é—´èŒƒå›´
                    ds_start_time = str(ds.time.values[0])
                    ds_end_time = str(ds.time.values[-1])
                    
                    # ç¡®å®šå®é™…çš„åˆ‡ç‰‡èŒƒå›´
                    actual_start = max(t_range[0], ds_start_time)
                    actual_end = min(t_range[1], ds_end_time)
                    
                    # ä½¿ç”¨è°ƒæ•´åçš„æ—¶é—´èŒƒå›´è¿›è¡Œåˆ‡ç‰‡
                    ds = ds.sel(time=slice(actual_start, actual_end))
                    
                    # å¦‚æœåˆ‡ç‰‡åçš„æ•°æ®é›†ä¸ºç©ºï¼Œè¿”å›è­¦å‘Š
                    if len(ds.time) == 0:
                        print(f"è­¦å‘Šï¼šæŒ‡å®šçš„æ—¶é—´èŒƒå›´ {t_range[0]} åˆ° {t_range[1]} ä¸æ•°æ®é›†æ—¶é—´èŒƒå›´ {ds_start_time} åˆ° {ds_end_time} æ²¡æœ‰é‡å ")

                # åº”ç”¨å˜é‡è¿‡æ»¤
                if var_lst is not None:
                    available_vars = [var for var in var_lst if var in ds.data_vars]
                    if available_vars:
                        ds = ds[available_vars]

                # ä¸ºå˜é‡æ·»åŠ å¿…éœ€çš„ units å±æ€§
                for var_name in ds.data_vars:
                    if "units" not in ds[var_name].attrs:
                        # æ ¹æ®å˜é‡ç±»å‹æ·»åŠ åˆé€‚çš„å•ä½
                        if any(
                            keyword in var_name.lower()
                            for keyword in ["flow", "inflow", "streamflow","gen_discharge","obs_discharge"]
                        ):  # å¦‚æœ var_name åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯ï¼Œæ‰§è¡Œè¿™é‡Œçš„ä»£ç 
                            ds[var_name].attrs["units"] = "m^3/s"  # æµé‡å•ä½
                        elif (
                            "rain" in var_name.lower()
                            or "precipitation" in var_name.lower()
                        ):
                            ds[var_name].attrs["units"] = f"mm/{time_unit}"  # é™é›¨å•ä½
                        elif "flood_event" in var_name.lower():
                            ds[var_name].attrs["units"] = "dimensionless"  # æ— é‡çº²
                        else:
                            ds[var_name].attrs["units"] = "unknown"  # é»˜è®¤å€¼

                print(f"æˆåŠŸä»å¢å¼ºæ•°æ®ç¼“å­˜è¯»å–: {cache_file_path}")
                return {time_unit: ds}

            except Exception as e:
                print(f"è¯»å–å¢å¼ºæ•°æ®ç¼“å­˜å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®: {e}")

        # å¦‚æœå¢å¼ºæ•°æ®ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ•°æ®
        result = self.read_ts_xrdataset(gage_id_lst, t_range, var_lst, **kwargs)

        # ä¸ºæ‰€æœ‰è¿”å›çš„æ•°æ®é›†æ·»åŠ  units å±æ€§
        for time_unit_key, ds in result.items():
            for var_name in ds.data_vars:
                if "units" not in ds[var_name].attrs:
                    # æ ¹æ®å˜é‡ç±»å‹æ·»åŠ åˆé€‚çš„å•ä½
                    if any(
                        keyword in var_name.lower()
                        for keyword in ["flow", "inflow", "streamflow"]
                    ):
                        ds[var_name].attrs["units"] = "m^3/s"  # æµé‡å•ä½
                    elif (
                        "rain" in var_name.lower()
                        or "precipitation" in var_name.lower()
                    ):
                        ds[var_name].attrs["units"] = f"mm/{time_unit}"  # é™é›¨å•ä½
                    elif "flood_event" in var_name.lower():
                        ds[var_name].attrs["units"] = "dimensionless"  # æ— é‡çº²
                    else:
                        ds[var_name].attrs["units"] = "unknown"  # é»˜è®¤å€¼

        return result

    def discover_augmented_files(
        self,
        augmented_files_dir: str,
        source_event: Optional[str] = None,
        modified_by: Optional[List[str]] = None,
        time_range: Optional[Tuple[str, str]] = None,
        latest_only: bool = False,
    ) -> List[Dict]:
        """
        å‘ç°å¢å¼ºæ•°æ®æ–‡ä»¶çš„æ™ºèƒ½æ¥å£

        Args:
            augmented_files_dir: å¢å¼ºæ•°æ®æ–‡ä»¶ç›®å½•
            source_event: æºäº‹ä»¶åè¿‡æ»¤ (å¦‚ "event_1994081520_1994081805")
            modified_by: ä¿®æ”¹è€…åˆ—è¡¨è¿‡æ»¤
            time_range: ä¿®æ”¹æ—¶é—´èŒƒå›´è¿‡æ»¤ ("2025-01-01", "2025-12-31")
            latest_only: æ˜¯å¦åªè¿”å›æ¯ä¸ªæºäº‹ä»¶çš„æœ€æ–°ç‰ˆæœ¬

        Returns:
            List[Dict]: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ã€å…ƒæ•°æ®ç­‰
        """
        print(f"ğŸ” æœç´¢å¢å¼ºæ•°æ®æ–‡ä»¶: {augmented_files_dir}")

        # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        csv_pattern = os.path.join(augmented_files_dir, "*.csv")
        all_files = glob.glob(csv_pattern)

        discovered_files = []

        for file_path in all_files:
            try:
                file_info = self._parse_augmented_file_info(file_path)

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if source_event and file_info["source_event"] != source_event:
                    continue

                if modified_by and file_info["modified_by"] not in modified_by:
                    continue

                if time_range and file_info["modified_time"]:
                    file_time = datetime.strptime(
                        file_info["modified_time"], "%Y-%m-%d %H:%M:%S"
                    )
                    start_time = datetime.strptime(time_range[0], "%Y-%m-%d")
                    end_time = datetime.strptime(time_range[1], "%Y-%m-%d")
                    if not (start_time <= file_time <= end_time):
                        continue

                discovered_files.append(file_info)
            except Exception as e:
                print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: {str(e)}")
                continue

        # æŒ‰æºäº‹ä»¶åˆ†ç»„å¹¶é€‰æ‹©æœ€æ–°ç‰ˆæœ¬
        if latest_only and discovered_files:
            latest_files = {}
            for file_info in discovered_files:
                source = file_info["source_event"]
                if source not in latest_files:
                    latest_files[source] = file_info
                else:
                    # æ¯”è¾ƒä¿®æ”¹æ—¶é—´ï¼Œé€‰æ‹©æœ€æ–°çš„
                    current_time = file_info.get("modified_time") or ""
                    existing_time = latest_files[source].get("modified_time") or ""
                    if current_time > existing_time:
                        latest_files[source] = file_info

            discovered_files = list(latest_files.values())

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå¤„ç†Noneå€¼
        discovered_files.sort(key=lambda x: x.get("modified_time") or "", reverse=True)
        print(f"   âœ… å‘ç° {len(discovered_files)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
        return discovered_files

    def _parse_augmented_file_info(self, file_path: str) -> Dict:
        """è§£æå¢å¼ºæ•°æ®æ–‡ä»¶çš„ä¿¡æ¯"""
        filename = os.path.basename(file_path)

        # è§£ææ–‡ä»¶åæ ¼å¼: xxx_modifiedbyXXX_atYYYYMMDDHHMMSS.csv
        name_pattern = r"(.+)_modifiedby(.+)_at(\d{14})\.csv"
        match = re.match(name_pattern, filename)
        file_info: Dict[str, Any] = {
            "file_path": file_path,
            "filename": filename,
            "source_event": None,
            "modified_by": None,
            "modified_time": None,
            "metadata": {},
        }

        if match:
            base_name = match.group(1)
            modifier = match.group(2)
            timestamp = match.group(3)

            file_info["source_event"] = base_name
            file_info["modified_by"] = modifier

            # è§£ææ—¶é—´æˆ³
            try:
                dt = datetime.strptime(timestamp, "%Y%m%d%H%M%S")
                file_info["modified_time"] = dt.strftime("%Y-%m-%d %H:%M:%S")
            except ValueError:
                pass
        # è¯»å–æ–‡ä»¶å¤´éƒ¨çš„å…ƒæ•°æ®
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            metadata = {}
            for line in lines:
                if line.startswith("#"):
                    line = line.strip("#").strip()
                    if ":" in line:
                        key, value = line.split(":", 1)
                        key_clean = key.strip().lower().replace(" ", "_")
                        metadata[key_clean] = value.strip()
                else:
                    break

            file_info["metadata"] = metadata

            # ä»å…ƒæ•°æ®ä¸­è·å–æ›´å¤šä¿¡æ¯
            if "source" in metadata:
                file_info["source_event"] = metadata["source"].replace(".csv", "")

        except Exception as e:
            print(f"   âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶å¤´éƒ¨: {file_path}, {str(e)}")
        return file_info

    def process_augmented_files_by_discovery(
        self,
        station_ids: Union[str, List[str]],
        augmented_files_dir: str,
        source_event: Optional[str] = None,
        modified_by: Optional[List[str]] = None,
        time_range: Optional[Tuple[str, str]] = None,
        latest_only: bool = True,
        warmup_hours: int = 240,
        time_unit: str = "3h",
    ) -> Optional[str]:
        """
        Process augmented data files based on file discovery.

        Parameters
        ----------
        station_ids : Union[str, List[str]]
            Station ID or list of station IDs.
        augmented_files_dir : str
            Directory containing augmented data files.
        source_event : Optional[str], optional
            Filter by source event name.
        modified_by : Optional[List[str]], optional
            Filter by list of modifiers.
        time_range : Optional[Tuple[str, str]], optional
            Filter by modification time range.
        latest_only : bool, optional
            Whether to process only the latest version for each source event.
        warmup_hours : int, optional
            Number of warmup hours.
        time_unit : str, optional
            Time unit.

        Returns
        -------
        Optional[str]
            Path to the cache file, or None if processing fails.
        """
        # å…¼å®¹å•ä¸ªç«™ç‚¹çš„æƒ…å†µ
        if isinstance(station_ids, str):
            station_ids = [station_ids]

        # å‘ç°å¯ç”¨æ–‡ä»¶
        discovered_files = self.discover_augmented_files(
            augmented_files_dir=augmented_files_dir,
            source_event=source_event,
            modified_by=modified_by,
            time_range=time_range,
            latest_only=latest_only,
        )

        if not discovered_files:
            print("âŒ æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„å¢å¼ºæ•°æ®æ–‡ä»¶")
            return None
        print(f"ğŸ”„ å‡†å¤‡å¤„ç† {len(discovered_files)} ä¸ªå¢å¼ºæ•°æ®æ–‡ä»¶:")
        # for file_info in discovered_files:
        #     modified_by = file_info.get("modified_by", "unknown")
        #     print(f"   - {file_info['filename']} (ä¿®æ”¹è€…: {modified_by})")
        # åˆ›å»ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œç”¨äºç°æœ‰çš„å¤„ç†æ–¹æ³•
        file_paths = [file_info["file_path"] for file_info in discovered_files]
        # è°ƒç”¨ç°æœ‰çš„å¤„ç†æ–¹æ³•ï¼Œä½†ä½¿ç”¨æ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯ç¼–å·
        return self._process_augmented_files_by_paths(
            station_ids=station_ids,
            file_paths=file_paths,
            warmup_hours=warmup_hours,
            time_unit=time_unit,
        )

    def _process_augmented_files_by_paths(
        self,
        station_ids: Union[str, List[str]],
        file_paths: List[str],
        warmup_hours: int = 240,
        time_unit: str = "3h",
    ) -> Optional[str]:
        """
        Process augmented data files based on file paths.

        Parameters
        ----------
        station_ids : Union[str, List[str]]
            Station ID or list of station IDs.
        file_paths : List[str]
            List of file paths to process.
        warmup_hours : int, optional
            Number of warmup hours.
        time_unit : str, optional
            Time unit.

        Returns
        -------
        Optional[str]
            Path to the cache file, or None if processing fails.
        """
        # å…¼å®¹å•ä¸ªç«™ç‚¹çš„æƒ…å†µ
        if isinstance(station_ids, str):
            station_ids = [station_ids]

        all_datasets = []
        processed_count = 0

        # å¯¹æ¯ä¸ªç«™ç‚¹å¤„ç†å¢å¼ºæ•°æ®
        for station_id in station_ids:
            print(f"ğŸ”„ å¤„ç†ç«™ç‚¹: {station_id}")
            station_timeseries = []

            for file_path in file_paths:
                try:
                    print(f"   ğŸ”„ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

                    # è§£æå¢å¼ºæ–‡ä»¶çš„å…ƒä¿¡æ¯
                    metadata = self.parse_augmented_file_metadata(file_path)
                    if not metadata:
                        print(f"      âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: æ— æ³•è§£æå…ƒæ•°æ®")
                        continue

                    # è·å–é¢„çƒ­æœŸæ•°æ®
                    warmup_df = self.get_warmup_period_data(
                        original_start_time=metadata.get("original_start_time"),
                        original_end_time=metadata.get("original_end_time"),
                        station_id=station_id,
                        warmup_hours=warmup_hours,
                    )

                    if warmup_df is None:
                        print(f"      âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: æ— æ³•è·å–é¢„çƒ­æœŸæ•°æ®")
                        continue

                    # è°ƒæ•´é¢„çƒ­æœŸæ—¶é—´å¹¶æ‹¼æ¥å¢å¼ºæ•°æ®
                    timeseries_df = self.concatenate_warmup_and_augmented_data(
                        warmup_df, file_path
                    )

                    if timeseries_df is not None and len(timeseries_df) > 0:
                        station_timeseries.append(timeseries_df)
                        # print(f"      âœ… æˆåŠŸå¤„ç†: {len(timeseries_df)} æ¡è®°å½•")
                    else:
                        print(f"      âš ï¸ è·³è¿‡æ–‡ä»¶ {file_path}: å¤„ç†åæ•°æ®ä¸ºç©º")
                except Exception as e:
                    print(f"      âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                    continue

            # å¦‚æœè¯¥ç«™ç‚¹æœ‰æ•°æ®ï¼Œåˆ™åˆå¹¶å¹¶è½¬æ¢ä¸ºxarray Dataset
            if station_timeseries:
                print(f"   ğŸ”„ åˆå¹¶ç«™ç‚¹ {station_id} çš„æ—¶é—´åºåˆ—æ•°æ®...")
                combined_df = pd.concat(station_timeseries, ignore_index=True)
                combined_df = combined_df.sort_values("time").reset_index(drop=True)
                # é‡å‘½ååˆ— gen_discharge -> inflow
                combined_df = self.rename_dataframe_columns(combined_df)

                # è½¬æ¢ä¸ºxarray Dataset
                station_ds = self.create_xarray_dataset_from_augdf(
                    combined_df, station_id, time_unit
                )
                all_datasets.append(station_ds)
                processed_count += 1
                print(f"   âœ… ç«™ç‚¹ {station_id} å¤„ç†å®Œæˆ: {len(combined_df)} æ¡è®°å½•")
            else:
                print(f"   âš ï¸ ç«™ç‚¹ {station_id} æ²¡æœ‰å¯ç”¨æ•°æ®")

        if not all_datasets:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•ç«™ç‚¹çš„æ•°æ®")
            return None

        print(f"âœ… æˆåŠŸå¤„ç† {processed_count} ä¸ªç«™ç‚¹")

        # åˆå¹¶æ‰€æœ‰ç«™ç‚¹çš„æ•°æ®é›†
        if len(all_datasets) == 1:
            final_ds = all_datasets[0]
        else:
            print("ğŸ”„ åˆå¹¶å¤šä¸ªç«™ç‚¹çš„æ•°æ®é›†...")
            final_ds = xr.concat(all_datasets, dim="gage_id")

        # ä¿å­˜åˆ°ç¼“å­˜
        print("ğŸ”„ ä¿å­˜å¢å¼ºæ•°æ®åˆ°ç¼“å­˜...")
        cache_file_path = self.save_augmented_timeseries_to_cache(
            final_ds, station_ids, time_unit
        )
        return cache_file_path

    def get_user_contributions_summary(self, augmented_files_dir: str) -> pd.DataFrame:
        """è·å–ç”¨æˆ·è´¡çŒ®ç»Ÿè®¡"""
        discovered_files = self.discover_augmented_files(augmented_files_dir)
        if not discovered_files:
            return pd.DataFrame()
        summary_data = []
        user_stats = {}
        for file_info in discovered_files:
            user = file_info.get("modified_by", "unknown")
            source = file_info.get("source_event", "unknown")

            if user not in user_stats:
                user_stats[user] = {
                    "user": user,
                    "total_files": 0,
                    "unique_events": set(),
                    "latest_modification": None,
                }

            user_stats[user]["total_files"] += 1
            user_stats[user]["unique_events"].add(source)

            mod_time = file_info.get("modified_time")
            if mod_time:
                latest = user_stats[user]["latest_modification"]
                if latest is None or mod_time > latest:
                    user_stats[user]["latest_modification"] = mod_time

        for user, stats in user_stats.items():
            summary_data.append(
                {
                    "user": user,
                    "total_files": stats["total_files"],
                    "unique_events": len(stats["unique_events"]),
                    "latest_modification": stats["latest_modification"],
                }
            )
        summary_df = pd.DataFrame(summary_data)
        summary_df = summary_df.sort_values("total_files", ascending=False)
        return summary_df

    def check_event_data_nan(
        self,
        all_event_data: List[Dict],
        exclude_warmup: bool = True,
    ):
        """
        Check for NaN values in rainfall and runoff data for all flood events.

        This method leverages the class's attributes (net_rain_key, obs_flow_key,
        warmup_length) and can optionally exclude warmup period from NaN checking.

        Parameters
        ----------
        all_event_data : list of dict
            List of event dictionaries, each containing net rainfall, runoff, filepath, etc.
        exclude_warmup : bool, optional
            Whether to exclude warmup period data from NaN checking, by default True.
            When True, only checks data points where flood_event_markers > 0 or
            excludes the first warmup_length data points if markers are not available.

        Raises
        ------
        ValueError
            If any NaN values are found in the non-warmup data, raises an exception
            and prints detailed information.

        Notes
        -----
        This method uses the class attributes:
        - self.net_rain_key: Key name for net rainfall data
        - self.obs_flow_key: Key name for observed flow data
        - self.warmup_length: Number of warmup time steps

        When exclude_warmup=True:
        1. If 'flood_event_markers' exists in event data, only checks where markers > 0
        2. Otherwise, skips the first self.warmup_length data points
        """
        for event in all_event_data:
            event_name = event.get("filepath", "unknown")
            p_eff = event.get(self.net_rain_key)
            q_obs = event.get(self.obs_flow_key)

            if p_eff is None or q_obs is None:
                continue

            # Convert to numpy arrays if needed
            p_eff = np.array(p_eff)
            q_obs = np.array(q_obs)

            # Determine which indices to check based on exclude_warmup setting
            if exclude_warmup:
                flood_event_markers = event.get("flood_event_markers")

                if flood_event_markers is not None:
                    # Use flood_event_markers to identify non-warmup data
                    flood_event_markers = np.array(flood_event_markers)
                    check_indices = flood_event_markers > 0

                    # Apply mask to data arrays
                    p_eff_to_check = p_eff[check_indices]
                    q_obs_to_check = q_obs[check_indices]

                    # Get the actual indices for error reporting
                    original_indices = np.arange(len(p_eff))[check_indices]
                else:
                    # Fallback: exclude first warmup_length points
                    start_idx = self.warmup_length
                    p_eff_to_check = p_eff[start_idx:]
                    q_obs_to_check = q_obs[start_idx:]

                    # Get the actual indices for error reporting
                    original_indices = np.arange(start_idx, len(p_eff))
            else:
                # Check all data points
                p_eff_to_check = p_eff
                q_obs_to_check = q_obs
                original_indices = np.arange(len(p_eff))

            # Check for NaN in net rain
            if np.any(np.isnan(p_eff_to_check)):
                nan_mask = np.isnan(p_eff_to_check)
                nan_indices = original_indices[nan_mask]
                print(
                    f"âŒ åœºæ¬¡ {event_name} çš„ {self.net_rain_key} å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_indices}"
                )
                raise ValueError(
                    f"Event {event_name} has NaN in {self.net_rain_key} at index {nan_indices}"
                )

            # Check for NaN in observed flow
            if np.any(np.isnan(q_obs_to_check)):
                nan_mask = np.isnan(q_obs_to_check)
                nan_indices = original_indices[nan_mask]
                print(
                    f"âŒ åœºæ¬¡ {event_name} çš„ {self.obs_flow_key} å­˜åœ¨ç©ºå€¼ï¼Œç´¢å¼•: {nan_indices}"
                )
                raise ValueError(
                    f"Event {event_name} has NaN in {self.obs_flow_key} at index {nan_indices}"
                )


def _calculate_event_characteristics(
    event: Dict,
    delta_t_hours: float = 3.0,
    net_rain_key: str = "P_eff",
    obs_flow_key: str = "Q_obs_eff",
) -> Dict:
    """
    è®¡ç®—æ´ªæ°´äº‹ä»¶çš„è¯¦ç»†ç‰¹å¾æŒ‡æ ‡ï¼Œç”¨äºç”»å›¾å’Œåˆ†æ

    Parameters
    ----------
        event: dict
            äº‹ä»¶å­—å…¸ï¼ŒåŒ…å«å‡€é›¨å’Œå¾„æµæ•°ç»„
        delta_t_hours: float
            æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶
        net_rain_key: str
            å‡€é›¨æ•°æ®çš„é”®åï¼Œé»˜è®¤"P_eff"
        obs_flow_key: str
            è§‚æµ‹æµé‡æ•°æ®çš„é”®åï¼Œé»˜è®¤"Q_obs_eff"

    Returns
    -------
        Dict: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡

    Calculated metrics:
        - peak_obs: æ´ªå³°æµé‡ (mÂ³/s)
        - runoff_volume_m3: æ´ªé‡ (mÂ³)
        - runoff_duration_hours: æ´ªæ°´å†æ—¶ (å°æ—¶)
        - total_net_rain: æ€»å‡€é›¨é‡ (mm)
        - lag_time_hours: æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
    """
    try:
        # æå–æ•°æ®
        net_rain = event.get(net_rain_key, [])
        direct_runoff = event.get(obs_flow_key, [])

        net_rain = np.array(net_rain)
        direct_runoff = np.array(direct_runoff)

        # è½¬æ¢ä¸ºç§’
        delta_t_seconds = delta_t_hours * 3600.0

        # 1. è®¡ç®—æ´ªå³°æµé‡
        peak_obs = np.max(direct_runoff)
        if peak_obs < 1e-6:
            return None

        # 2. è®¡ç®—æ´ªé‡ (mÂ³)
        runoff_volume_m3 = np.sum(direct_runoff) * delta_t_seconds

        # 3. è®¡ç®—æ´ªæ°´å†æ—¶ (å°æ—¶)
        runoff_indices = np.where(direct_runoff > 1e-6)[0]
        if len(runoff_indices) < 2:
            return None
        runoff_duration_hours = (
            runoff_indices[-1] - runoff_indices[0] + 1
        ) * delta_t_hours

        # 4. è®¡ç®—æ€»å‡€é›¨é‡ (mm)
        total_net_rain = np.sum(net_rain)

        # 5. è®¡ç®—æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
        t_peak_flow_idx = np.argmax(direct_runoff)
        t_peak_rain_idx = np.argmax(net_rain)
        lag_time_hours = (t_peak_flow_idx - t_peak_rain_idx) * delta_t_hours

        # 6. è®¡ç®—æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
        m_eff = len(net_rain)

        # 7. è®¡ç®—å¾„æµæ—¶æ®µæ•°
        n_obs = len(direct_runoff)

        # 8. è®¡ç®—å•ä½çº¿é•¿åº¦
        n_specific = n_obs - m_eff + 1

        # è¿”å›è®¡ç®—ç»“æœ
        characteristics = {
            "peak_obs": peak_obs,  # æ´ªå³°æµé‡ (mÂ³/s)
            "runoff_volume_m3": runoff_volume_m3,  # æ´ªé‡ (mÂ³)
            "runoff_duration_hours": runoff_duration_hours,  # æ´ªæ°´å†æ—¶ (å°æ—¶)
            "total_net_rain": total_net_rain,  # æ€»å‡€é›¨é‡ (mm)
            "lag_time_hours": lag_time_hours,  # æ´ªå³°é›¨å³°å»¶è¿Ÿ (å°æ—¶)
            "m_eff": m_eff,  # æœ‰æ•ˆé™é›¨æ—¶æ®µæ•°
            "n_obs": n_obs,  # å¾„æµæ—¶æ®µæ•°
            "n_specific": n_specific,  # å•ä½çº¿é•¿åº¦
            "delta_t_hours": delta_t_hours,  # æ—¶æ®µé•¿åº¦
        }

        return characteristics

    except Exception as e:
        print(f"è®¡ç®—äº‹ä»¶ç‰¹å¾æ—¶å‡ºé”™: {e}")
        return None


def calculate_events_characteristics(
    events: List[Dict],
    delta_t_hours: float = 3.0,
    net_rain_key: str = "P_eff",
    obs_flow_key: str = "Q_obs_eff",
) -> List[Dict]:
    """
    æ‰¹é‡è®¡ç®—å¤šä¸ªæ´ªæ°´äº‹ä»¶çš„ç‰¹å¾æŒ‡æ ‡

    Args:
        events: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å«å‡€é›¨å’Œå¾„æµæ•°ç»„
        delta_t_hours: æ—¶æ®µé•¿åº¦ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤3å°æ—¶
        net_rain_key: å‡€é›¨æ•°æ®çš„é”®åï¼Œé»˜è®¤"P_eff"
        obs_flow_key: è§‚æµ‹æµé‡æ•°æ®çš„é”®åï¼Œé»˜è®¤"Q_obs_eff"

    Returns:
        List[Dict]: åŒ…å«è®¡ç®—å‡ºçš„æ°´æ–‡ç‰¹å¾æŒ‡æ ‡çš„äº‹ä»¶åˆ—è¡¨
    """
    enhanced_events = []

    for i, event in enumerate(events):
        # è®¡ç®—ç‰¹å¾æŒ‡æ ‡
        characteristics = _calculate_event_characteristics(
            event, delta_t_hours, net_rain_key, obs_flow_key
        )

        if characteristics:
            # å°†ç‰¹å¾æŒ‡æ ‡æ·»åŠ åˆ°åŸäº‹ä»¶å­—å…¸ä¸­
            enhanced_event = event.copy()
            enhanced_event.update(characteristics)
            enhanced_events.append(enhanced_event)
        else:
            print(f"âš ï¸ äº‹ä»¶ {i+1} ç‰¹å¾è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡")

    return enhanced_events
