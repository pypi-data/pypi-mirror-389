import asyncio
import json
import logging
from collections import OrderedDict
from typing import Callable, Iterator, Union

import grpc
from google.protobuf.descriptor_pb2 import DescriptorProto
from google.protobuf.message import Message

from ..shared import (NOT_RETRIABLE_GRPC_CODES, NonRetriableException,
                      RecordType, StreamConfigurationOptions, StreamState,
                      TableProperties, ZerobusException, _StreamFailureInfo,
                      _StreamFailureType, log_and_get_exception,
                      zerobus_service_pb2, zerobus_service_pb2_grpc)
from ..shared.headers_provider import HeadersProvider, OAuthHeadersProvider

logger = logging.getLogger("zerobus_sdk")

# Default timeout in milliseconds for stream operations, used when recovery is not enabled.
CREATE_STREAM_TIMEOUT_MS = 15000


class ZerobusStream:
    """
    Manages a single, stateful gRPC stream for ingesting records asynchronously.

    This class uses asyncio to handle the complexities of a bi-directional streaming
    RPC, including sending records, receiving acknowledgments, managing in-flight
    record limits, and automatic recovery from transient network errors.

    Args:
        stub (zerobus_service_pb2_grpc.ZerobusStub): The async gRPC stub for the service.
        headers_provider (HeadersProvider): Provider for headers.
        table_properties (TableProperties): The properties of the target table.
        options (StreamConfigurationOptions): Configuration options for the stream.
    """

    def __init__(
        self,
        stub: zerobus_service_pb2_grpc.ZerobusStub,
        headers_provider: HeadersProvider,
        table_properties: TableProperties,
        options: StreamConfigurationOptions,
    ):
        self.__stub = stub
        self._headers_provider = headers_provider
        self._table_properties = table_properties
        self._options = options
        self.__record_queue = asyncio.Queue(maxsize=0)
        self.__inflight_records_tokens = asyncio.Queue(maxsize=self._options.max_inflight_records)
        self.__inflight_records_done = asyncio.Condition()
        self.__pending_futures = OrderedDict()
        self.__unacked_records = list()
        self.__last_received_offset = None
        self.stream_id = None
        self.__state = StreamState.UNINITIALIZED
        self.__state_changed = asyncio.Condition()
        self.__receiver_task = None
        self.__sender_running = False
        self.__sender_status = asyncio.Condition()
        self.__error_handling_in_progress = False
        self.__server_unresponsiveness_detection_task = None
        self.__stream_failure_info = _StreamFailureInfo()

    async def __set_state(self, new_state: StreamState):
        async with self.__state_changed:
            self.__state = new_state
            self.__state_changed.notify_all()

    async def __with_retries(self, func: Callable, max_attempts: int):
        timeout_seconds = (
            (self._options.recovery_timeout_ms / 1000) if self._options.recovery else (CREATE_STREAM_TIMEOUT_MS / 1000)
        )
        backoff_seconds = (self._options.recovery_backoff_ms / 1000) if self._options.recovery else 0

        for attempt in range(max_attempts):
            logger.info(f"Attempting retry {attempt} out of {max_attempts}")
            try:
                try:
                    await asyncio.wait_for(func(), timeout=timeout_seconds)
                    break
                except asyncio.TimeoutError:
                    # We add from None to suppress the stack trace generated by asyncio
                    # (a lot of timeout and cancelled errors)
                    # This makes the exception trace much cleaner when shown to the user
                    raise Exception("Stream operation timed out...") from None
            except NonRetriableException as e:
                raise e
            except asyncio.CancelledError:
                raise
            except Exception as e:
                if attempt == max_attempts - 1:
                    raise e  # Re-raise last exception if all attempts fail
                await asyncio.sleep(backoff_seconds)

    # Define callable for stream creation
    async def __create_stream(self):
        headers = self._headers_provider.get_headers()

        # Stream already opened
        if self.__state == StreamState.OPENED:
            return

        # Stream closed/failed
        if self.__state == StreamState.CLOSED or self.__state == StreamState.FAILED:
            raise ZerobusException("Stream already closed and cannot be reused.")

        # UNINITIALIZED or RECOVERING
        self.__last_received_offset = None
        self.stream_id = None
        self._stream = None
        self.__receiver_task = None

        try:
            self._stream = self.__stub.EphemeralStream(self.__sender(), metadata=headers)

            success = False
            async for response in self._stream:
                if response.HasField("create_stream_response"):
                    self.stream_id = response.create_stream_response.stream_id
                    success = True

                break

            if not success:
                logger.error("No response received from the server on stream creation")
                raise ZerobusException("No response received from the server on stream creation")

            self.__receiver_task = asyncio.create_task(self.__receiver())
            self.__server_unresponsiveness_detection_task = asyncio.create_task(
                self.__server_unresponsiveness_detection()
            )

            logger.info(f"Stream created. Stream ID: {self.stream_id}")

        except grpc.RpcError as e:
            if e.code() in NOT_RETRIABLE_GRPC_CODES:
                # Non-retriable gRPC errors
                logger.error(f"Non-retriable gRPC error during stream creation: {str(e)}")
                raise NonRetriableException(f"Failed to create a stream: {str(e)}") from e
            else:
                # Retriable gRPC errors
                logger.error(f"Retriable gRPC error during stream creation: {str(e)}")
                raise ZerobusException(f"Failed to create a stream: {str(e)}") from e
        except Exception as e:
            logger.error(f"Create stream error: {str(e)}")
            raise ZerobusException(f"Failed to create a stream: {str(e)}") from e

    async def _initialize(self):
        # Asynchronous initialization method for ZerobusStream. Must be called before using the stream.
        try:
            logger.info("Starting initializing stream")
            max_attempts = self._options.recovery_retries if self._options.recovery else 1
            await self.__with_retries(self.__create_stream, max_attempts)
            await self.__set_state(StreamState.OPENED)
        except ZerobusException:
            await self.__set_state(StreamState.FAILED)
            raise
        except Exception as e:
            await self.__set_state(StreamState.FAILED)
            raise ZerobusException(f"Failed to create a stream: {str(e)}") from e

    async def __close(self, hard_failure, err_msg=""):
        try:
            if self.__server_unresponsiveness_detection_task is not None:
                self.__server_unresponsiveness_detection_task.cancel()
                await self.__server_unresponsiveness_detection_task
        except:  # noqa: E722
            pass
        finally:
            self.__server_unresponsiveness_detection_task = None

        try:
            if self._stream is not None:
                self._stream.cancel()
        except:  # noqa: E722
            pass

        async with self.__sender_status:
            if self.__sender_running:
                await self.__sender_status.wait_for(lambda: not self.__sender_running)

        try:
            if self.__receiver_task is not None:
                self.__receiver_task.cancel()
                await self.__receiver_task
        except:  # noqa: E722
            pass
        finally:
            self.__receiver_task = None

        # Save all unacked records that are sent but not acknowledged
        if hard_failure:
            for future, record, _ in self.__pending_futures.values():
                if future is not None and future.done():
                    continue

                self.__unacked_records.append((record, future))
                if future is not None:
                    future.set_exception(ZerobusException(err_msg))
                    future.exception()  # this is to avoid too many logs that future is not handled

            self.__pending_futures.clear()

            # Save all unacked records that are not even sent
            async with self.__inflight_records_done:
                while not self.__record_queue.empty():
                    future, record, _ = self.__record_queue.get_nowait()
                    self.__unacked_records.append((record, future))
                    if future is not None:
                        future.set_exception(ZerobusException(err_msg))
                        future.exception()  # this is to avoid too many logs that future is not handled

                while not self.__inflight_records_tokens.empty():
                    self.__inflight_records_tokens.get_nowait()
                self.__inflight_records_done.notify_all()

    async def __recover_stream(self) -> bool:
        if not self._options.recovery:
            # recovery is disabled
            return False

        left_retries = max(0, self._options.recovery_retries - self.__stream_failure_info.failure_counts + 1)
        if left_retries == 0:
            # we've tried to recover the stream too many times, so we give up
            return False

        try:
            logger.info("Recovering stream...")

            await self.__close(hard_failure=False)
            await self.__with_retries(self.__create_stream, left_retries)
            await self.__set_state(StreamState.OPENED)

            logger.info("Stream recovered successfully")
        except Exception as e:
            logger.error(f"Failed to recover stream: {str(e)}")
            return False

        return True

    async def __handle_stream_failed(
        self, failed_stream_id, failure_type: _StreamFailureType, exception: Exception = None
    ):
        if self.__error_handling_in_progress:
            # Error handling is already in progress, so we don't need to handle it again
            return

        if failed_stream_id != self.stream_id:
            # Stream was reset in the meantime, so we will repeat ingesting the record
            return

        try:
            self.__error_handling_in_progress = True

            # RECOVERING -> Something else is already taking care of the failure
            # FAILED -> Stream's failure is already processed'
            # UNINITIALIZED -> Stream is not opened - means that the stream was not created yet
            if (
                self.__state == StreamState.RECOVERING
                or self.__state == StreamState.FAILED
                or self.__state == StreamState.UNINITIALIZED
            ):
                return

            if self.__state == StreamState.CLOSED and (
                exception is None or isinstance(exception, asyncio.CancelledError)
            ):
                # Stream failed after closed, but without exception - that's expected (stream closed gracefully)
                return

            err_msg = str(exception) if exception is not None else "Stream closed unexpectedly!"
            self.__stream_failure_info.log_failure(failure_type)

            should_recover = (
                (self.__state == StreamState.OPENED or self.__state == StreamState.FLUSHING)
                and not isinstance(exception, NonRetriableException)
                and self._options.recovery
            )

            if should_recover:
                # Set the state to recovering
                # This is to prevent the stream from being closed multiple times
                self.__state = StreamState.RECOVERING
                recovered = await self.__recover_stream()
                if recovered:
                    # Stream recovered successfully
                    return
                # Recovery failed
                logger.error(f"Stream failed permanently after failed recovery attempt: {err_msg}")
            else:
                # Non-recoverable error
                logger.error(f"Stream closed due to a non-recoverable error: {err_msg}")

            # Close the stream for new events
            await self.__set_state(StreamState.FAILED)
            await self.__close(hard_failure=True, err_msg=err_msg)
        finally:
            self.__error_handling_in_progress = False

    async def __wait_for_stream_to_finish_initialization(self):
        async with self.__state_changed:
            if self.__state == StreamState.UNINITIALIZED or self.__state == StreamState.RECOVERING:
                await self.__state_changed.wait_for(
                    lambda: self.__state != StreamState.UNINITIALIZED and self.__state != StreamState.RECOVERING
                )

    async def __next_record(self):
        try:
            return await asyncio.wait_for(self.__record_queue.get(), timeout=1.0)
        except asyncio.TimeoutError:
            return (None, None, None)

    async def __sender(self):
        self.__sender_running = True
        exception = None
        offset_id = -1
        stream_id = None
        record_ack_received_future, record = (None, None)

        try:
            # 1. CREATE STREAM
            logger.info("Sending CreateIngestStreamRequest to gRPC stream")
            create_stream_request = zerobus_service_pb2.CreateIngestStreamRequest(
                table_name=self._table_properties.table_name.encode("utf-8"),
                record_type=self._options.record_type.value,
            )

            # Only include descriptor for PROTO streams
            if self._options.record_type == RecordType.PROTO:
                create_stream_request.descriptor_proto = self._get_descriptor_bytes(
                    self._table_properties.descriptor_proto
                )

            yield zerobus_service_pb2.EphemeralStreamRequest(create_stream=create_stream_request)
            logger.info("Waiting for CreateIngestStreamResponse")
            await self.__wait_for_stream_to_finish_initialization()
            stream_id = self.stream_id

            if self.__state == StreamState.FAILED or self.__state == StreamState.UNINITIALIZED:
                return

            # 2. RETRY UNACKED RECORDS
            try:
                # First resend all the records that are sent but not acknowledged
                # this is triggered after recovery, for pending acks records
                # (for fresh new streams, this is a no-op)

                unacked_offset_ids = list(self.__pending_futures.keys())
                for old_offset_id in unacked_offset_ids:
                    (record_ack_received_future, sent_record, serialized_record) = self.__pending_futures.get(
                        old_offset_id
                    )

                    offset_id += 1

                    if offset_id != old_offset_id:
                        self.__pending_futures[offset_id] = (
                            record_ack_received_future,
                            sent_record,
                            serialized_record,
                        )
                        self.__pending_futures.pop(old_offset_id)

                    ingest_request = zerobus_service_pb2.IngestRecordRequest(offset_id=offset_id)
                    if self._options.record_type == RecordType.PROTO:
                        ingest_request.proto_encoded_record = serialized_record
                    elif self._options.record_type == RecordType.JSON:
                        ingest_request.json_record = serialized_record.decode("utf-8")

                    yield zerobus_service_pb2.EphemeralStreamRequest(ingest_record=ingest_request)

            except Exception as e:
                raise ZerobusException("Failed to resend unacked records after stream recovery: " + str(e))

            offset_id += 1

            # 3. SENDING NEW RECORDS
            while (
                self.__state == StreamState.OPENED
                or self.__state == StreamState.FLUSHING
                or not self.__record_queue.empty()
            ):
                if offset_id > 0:
                    self.__stream_failure_info.reset_failure(_StreamFailureType.SENDER_ERROR)

                # Failed -> return from the generator, stream is closed
                # Recovering -> new sender generator will be created
                if self.__state == StreamState.FAILED or self.__state == StreamState.RECOVERING:
                    return

                record_ack_received_future, record, serialized_record = await self.__next_record()

                if record is None:
                    continue

                self.__pending_futures[offset_id] = (
                    record_ack_received_future,
                    record,
                    serialized_record,
                )
                # Failed -> return from the generator, stream is closed
                # Recovering -> new sender generator will be created
                if self.__state == StreamState.FAILED or self.__state == StreamState.RECOVERING:
                    # We shouldn't loose the record
                    return

                ingest_request = zerobus_service_pb2.IngestRecordRequest(offset_id=offset_id)
                if self._options.record_type == RecordType.PROTO:
                    ingest_request.proto_encoded_record = serialized_record
                elif self._options.record_type == RecordType.JSON:
                    ingest_request.json_record = serialized_record.decode("utf-8")

                yield zerobus_service_pb2.EphemeralStreamRequest(ingest_record=ingest_request)

                offset_id += 1

            # If stream state is closed, wait for all records to be sent before closing the underlying grpc (graceful close)
            # Not waiting here would close the underlying grpc stream, so we need to wait until all pending futures were received from the server
            # If pending futures cannot be received, server unresponsiveness task will clean it up.
            while self.__state == StreamState.CLOSED and len(self.__pending_futures) > 0:
                await self.__next_record()

        except asyncio.CancelledError as e:
            exception = e
        except grpc.RpcError as e:
            # Check if this is a CANCELLED error due to intentional stream closure
            if self.__state == StreamState.CLOSED and e.code() == grpc.StatusCode.CANCELLED:
                # Stream was cancelled during close() - don't log as error
                exception = ZerobusException(f"Error happened in sending records: {e}")
            else:
                exception = log_and_get_exception(e)
        except Exception as e:
            logger.error(f"Error happened in sending records: {str(e)}")
            exception = ZerobusException(f"Error happened in sending records: {str(e)}")
        except GeneratorExit:
            exception = ZerobusException("Stream cancelled")
        finally:
            async with self.__sender_status:
                self.__sender_running = False
                self.__sender_status.notify_all()

            asyncio.create_task(self.__handle_stream_failed(stream_id, _StreamFailureType.SENDER_ERROR, exception))

    async def __receiver(self):
        exception = None

        try:
            await self.__wait_for_stream_to_finish_initialization()

            if self.__state != StreamState.OPENED and self.__state != StreamState.FLUSHING:
                return

            async for response in self._stream:
                if response.HasField("ingest_record_response"):
                    response = response.ingest_record_response
                elif response.HasField("close_stream_signal"):
                    close_stream_duration = response.close_stream_signal.duration.seconds
                    logger.info(f"Stream will be gracefully closed by server in {close_stream_duration} seconds.")
                    if self._options.recovery:
                        # If recovery is enabled, immediately start recovery process
                        return
                    else:
                        # If not, keep stream opened until server sends an error
                        continue
                else:
                    raise ZerobusException("Unexpected response type received. Expected IngestRecordResponse.")

                first_offset_in_ack = 0 if self.__last_received_offset is None else self.__last_received_offset + 1
                for offset_to_ack in range(first_offset_in_ack, response.durability_ack_up_to_offset + 1):
                    (future, _, _) = self.__pending_futures.get(offset_to_ack, (None, None, None))
                    if future is not None:
                        future.set_result(offset_to_ack)
                    self.__pending_futures.pop(offset_to_ack, (None, None, None))

                    async with self.__inflight_records_done:
                        if not self.__inflight_records_tokens.empty():
                            await self.__inflight_records_tokens.get()
                            self.__inflight_records_done.notify_all()

                self.__last_received_offset = response.durability_ack_up_to_offset
                if self._options.ack_callback:
                    self._options.ack_callback(response)

                if self.__last_received_offset > 0:
                    self.__stream_failure_info.reset_failure(_StreamFailureType.RECEIVER_ERROR)
                    self.__stream_failure_info.reset_failure(_StreamFailureType.SERVER_UNRESPONSIVE)

                # For gracefully closed streams, we need to wait all acks to be received
                if (
                    self.__state != StreamState.OPENED
                    and self.__state != StreamState.FLUSHING
                    and len(self.__pending_futures) == 0
                    and self.__record_queue.empty()
                ):
                    break
        except asyncio.CancelledError as e:
            exception = e
        except grpc.RpcError as e:
            # Check if this is a CANCELLED error due to intentional stream closure
            if self.__state == StreamState.CLOSED and e.code() == grpc.StatusCode.CANCELLED:
                # Stream was cancelled during close() - don't log as error
                exception = ZerobusException(f"Error happened in receiving records: {e}")
            else:
                exception = log_and_get_exception(e)
        except Exception as e:
            logger.error(f"Error happened in receiving records: {str(e)}")
            exception = ZerobusException(f"Error happened in receiving records: {str(e)}")
        finally:
            self.__receiver_task = None
            asyncio.create_task(
                self.__handle_stream_failed(self.stream_id, _StreamFailureType.RECEIVER_ERROR, exception)
            )

    async def __server_unresponsiveness_detection(self):
        exception = None

        try:
            await self.__wait_for_stream_to_finish_initialization()

            while self.__state == StreamState.OPENED or self.__state == StreamState.FLUSHING:
                last_received_offset = self.__last_received_offset

                async with self.__inflight_records_done:
                    # wait for inflight records to not be empty
                    if self.__inflight_records_tokens.empty():
                        await self.__inflight_records_done.wait_for(lambda: not self.__inflight_records_tokens.empty())

                await asyncio.sleep(self._options.server_lack_of_ack_timeout_ms / 1000)

                if last_received_offset == self.__last_received_offset:
                    raise ZerobusException("Server is unresponsive. Stream failed.")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(e)
            exception = e

        if exception is not None:
            self.__server_unresponsiveness_detection_task = None
            asyncio.create_task(
                self.__handle_stream_failed(self.stream_id, _StreamFailureType.SERVER_UNRESPONSIVE, exception)
            )

    def get_state(self) -> StreamState:
        """
        Returns the current operational state of the stream.

        Returns:
            StreamState: The current state as a StreamState enum member.
        """
        return self.__state

    def get_unacked_records(self) -> Iterator:
        """
        Retrieves records that were not acknowledged before the stream failed or was closed.

        Returns:
            Iterator: An iterator yielding the unacknowledged records.
        """
        return (record[0] for record in self.__unacked_records)

    async def ingest_record(self, record: Union[Message, dict]) -> asyncio.Future:
        """
        Asynchronously submits a single record for ingestion into the stream.

        This coroutine will await if the maximum number of in-flight records has been
        reached, pausing until there is capacity.

        Args:
            record: Either a Protobuf Message object or a dict (for JSON records).
                   Type must match the stream's configured record_type.

        Returns:
            asyncio.Future: A future that will be completed with the server's acknowledgment.
                The caller can await this future to confirm receipt.

        Raises:
            ValueError: If record type doesn't match stream configuration.
            ZerobusException: If the stream is not in a valid state for ingestion.
        """
        # Validate record type and serialize appropriately
        if self._options.record_type == RecordType.PROTO:
            if not isinstance(record, Message):
                raise ValueError(
                    f"Stream is configured for PROTO records, but received {type(record).__name__}. "
                    "Pass a Protobuf Message object."
                )
            serialized_record = record.SerializeToString()

        elif self._options.record_type == RecordType.JSON:
            if not isinstance(record, dict):
                raise ValueError(
                    f"Stream is configured for JSON records, but received {type(record).__name__}. "
                    "Pass a dict object."
                )
            # Serialize dict to JSON string and encode to bytes
            serialized_record = json.dumps(record).encode("utf-8")

        else:
            raise ValueError(f"Unsupported record type: {self._options.record_type}")

        # Wait for the flush to finish
        async with self.__state_changed:
            if self.__state == StreamState.FLUSHING:
                await self.__state_changed.wait_for(lambda: self.__state != StreamState.FLUSHING)

            # Check if the stream is closed or failed and fail ingesting if so
            if (
                self.__state == StreamState.CLOSED
                or self.__state == StreamState.FAILED
                or self.__state == StreamState.UNINITIALIZED
            ):
                raise ZerobusException("Cannot ingest records after stream is closed or before it's opened.")

        record_ack_received_future = asyncio.get_running_loop().create_future()

        async with self.__inflight_records_done:
            if self.__inflight_records_tokens.full():
                await self.__inflight_records_done.wait_for(lambda: not self.__inflight_records_tokens.full())

            if self.__state != StreamState.OPENED and self.__state != StreamState.RECOVERING:
                raise ZerobusException("Cannot ingest records after stream is closed or before it's opened.")
            await self.__inflight_records_tokens.put(None)
            await self.__record_queue.put((record_ack_received_future, record, serialized_record))

        await asyncio.sleep(0)
        return record_ack_received_future

    async def __wait_all_records_to_be_flushed(self):
        try:
            async with self.__state_changed:
                if self.__state == StreamState.OPENED:
                    self.__state = StreamState.FLUSHING

            async with self.__inflight_records_done:
                await asyncio.wait_for(
                    self.__inflight_records_done.wait_for(lambda: self.__inflight_records_tokens.empty()),
                    timeout=self._options.flush_timeout_ms / 1000,
                )

        except asyncio.TimeoutError:
            logger.error("Flush timed out ...")
            # We add from None to suppress the stack trace generated by asyncio (a lot of timeout and cancelled errors)
            # This makes the exception trace much cleaner when shown to the user
            raise ZerobusException("Flush timed out ...") from None
        finally:
            async with self.__state_changed:
                if self.__state == StreamState.FLUSHING:
                    self.__state = StreamState.OPENED
                    self.__state_changed.notify_all()

    async def flush(self):
        """
        Asynchronously waits until all currently submitted records have been acknowledged.

        This is a coroutine and should be awaited.

        Raises:
            IngestApiException: If the stream is not initialized or fails during the flush.
        """

        if self.__state == StreamState.UNINITIALIZED:
            raise ZerobusException("Stream is not initialized. Cannot flush.")

        async with self.__state_changed:
            if self.__state == StreamState.RECOVERING:
                await self.__state_changed.wait_for(lambda: self.__state != StreamState.RECOVERING)

        await self.__wait_all_records_to_be_flushed()

        if self.__state == StreamState.FAILED:
            raise ZerobusException("Stream failed with unacknowledged records. Cannot flush.")

        logger.info("Flush completed successfully")

    async def close(self):
        """
        Gracefully closes the stream after flushing all pending records.

        This is a coroutine that waits for all submitted records to be
        acknowledged by the server before shutting down the connection and background tasks.

        Raises:
            IngestApiException: If the stream is not in a valid state to be closed.
        """
        if self.__state == StreamState.UNINITIALIZED:
            raise ZerobusException("Stream is not initialized. Cannot close.")

        # Waiting for recovering/flushing to finish (either failed or recovered)
        async with self.__state_changed:
            if self.__state == StreamState.RECOVERING or self.__state == StreamState.FLUSHING:
                await self.__state_changed.wait_for(
                    lambda: (self.__state != StreamState.FLUSHING and self.__state != StreamState.RECOVERING)
                )

            if self.__state == StreamState.CLOSED:
                return

            if self.__state == StreamState.FAILED:
                raise ZerobusException("Stream failed. Cannot close.")

            logger.info("Closing the stream gracefully...")
            self.__state = StreamState.CLOSED
            self.__state_changed.notify_all()

        try:
            await self.flush()
        finally:
            await self.__close(hard_failure=True)

        logger.info("Stream closed successfully")

    @staticmethod
    def _get_descriptor_bytes(descriptor):
        """
        Private method for getting descriptor bytes.
        """
        descriptor_proto = DescriptorProto()
        descriptor.CopyToProto(descriptor_proto)

        serialized_descriptor = descriptor_proto.SerializeToString()
        return serialized_descriptor


class ZerobusSdk:
    """
    This class serves as the main entry point for interacting with the Zerobus service
    asynchronously, handling gRPC channel setup and stream creation.

    Args:
        host (str): The hostname or IP address of the Zerobus service.
        unity_catalog_url (str): The URL of the Unity Catalog endpoint.
    """

    def __init__(self, host: str, unity_catalog_url: str):
        """
        Initialize ZerobusSdk with standard OAuth authentication.

        Args:
            host: The hostname or IP address of the Zerobus service.
            unity_catalog_url: The URL of the Unity Catalog endpoint.
        """
        self.__host = host
        self.__unity_catalog_url = unity_catalog_url
        self.__workspace_id = host.split(".")[0]

    async def create_stream(
        self,
        client_id: str,
        client_secret: str,
        table_properties: TableProperties,
        options: StreamConfigurationOptions = StreamConfigurationOptions(),
    ) -> ZerobusStream:
        """
        Asynchronously creates, initializes, and returns a new ingestion stream.

        This is a coroutine and must be awaited.

        Args:
            client_id (str): The client ID.
            client_secret (str): The client secret.
            table_properties (TableProperties): The properties of the target table,
                including its name and schema.
            options (StreamConfigurationOptions): Optional configuration for the stream's
                behavior, such as recovery settings.

        Returns:
            ZerobusStream: An initialized and active stream instance.
        """
        headers_provider = OAuthHeadersProvider(
            self.__workspace_id, self.__unity_catalog_url, table_properties.table_name, client_id, client_secret
        )
        return await self.create_stream_with_headers_provider(headers_provider, table_properties, options)

    async def create_stream_with_headers_provider(
        self,
        headers_provider: HeadersProvider,
        table_properties: TableProperties,
        options: StreamConfigurationOptions = StreamConfigurationOptions(),
    ) -> ZerobusStream:
        """
        Asynchronously creates, initializes, and returns a new ingestion stream using a custom headers provider.

        This is a coroutine and must be awaited.

        Args:
            headers_provider: The headers provider.
            table_properties: The properties of the target table.
            options: The options for the stream.

        Returns:
            ZerobusStream: An initialized and active stream instance.

        Raises:
            ValueError: If record_type is PROTO but descriptor_proto is not provided.
        """
        # Validate record_type and descriptor compatibility
        if options.record_type == RecordType.PROTO:
            if table_properties.descriptor_proto is None:
                raise ValueError("descriptor_proto is required in TableProperties when record_type is PROTO")
        elif options.record_type == RecordType.JSON:
            if table_properties.descriptor_proto is not None:
                logger.warning("descriptor_proto provided for JSON stream will be ignored")

        channel = grpc.aio.secure_channel(
            self.__host,
            grpc.ssl_channel_credentials(),
            options=[("grpc.max_send_message_length", -1), ("grpc.max_receive_message_length", -1)],
        )
        stub = zerobus_service_pb2_grpc.ZerobusStub(channel)
        stream = ZerobusStream(
            stub,
            headers_provider,
            table_properties,
            options,
        )
        await stream._initialize()
        return stream

    async def recreate_stream(self, old_stream: ZerobusStream) -> ZerobusStream:
        """
        Asynchronously creates a new stream to replace a failed or closed stream.

        This coroutine automatically re-ingests any records that were not acknowledged
        by the previous stream.

        Args:
            old_stream (ZerobusStream): The failed or closed stream instance.

        Returns:
            ZerobusStream: A new, active stream with unacknowledged records re-queued.

        Raises:
            ZerobusException: If the provided old stream is still active.
        """
        old_stream_state = old_stream.get_state()
        if old_stream_state != StreamState.CLOSED and old_stream_state != StreamState.FAILED:
            raise ZerobusException("Stream is not closed. Cannot create new stream.")

        new_stream = await self.create_stream_with_headers_provider(
            old_stream._headers_provider, old_stream._table_properties, old_stream._options
        )

        # Loop over unacked records and ingest them into the new stream
        unacked_records = old_stream.get_unacked_records()
        for record in unacked_records:
            await new_stream.ingest_record(record)

        return new_stream
