[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "llama_models"
version = "0.3.0"
authors = [
    {name = "Meta Llama", email = "llama-oss@meta.com"},
]
description = "Llama models"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "PyYAML",
    "jinja2>=3.1.6",
    "tiktoken",
    "pydantic>=2",
    "Pillow",
    "rich",
    "httpx",
    "termcolor",
    "huggingface-hub",
]
classifiers = []

[project.urls]
Homepage = "https://github.com/meta-llama/llama-models"

[project.scripts]
llama-model = "llama_models.cli.llama:main"
multimodal_example_chat_completion = "llama_models.scripts.multimodal_example_chat_completion:main"
multimodal_example_text_completion = "llama_models.scripts.multimodal_example_text_completion:main"
example_chat_completion = "llama_models.scripts.example_chat_completion:main"
example_text_completion = "llama_models.scripts.example_text_completion:main"
llama4_completion = "llama_models.llama4.scripts.completion:main"
llama4_chat_completion = "llama_models.llama4.scripts.chat_completion:main"

[project.optional-dependencies]
dev = [
    "pytest",
    "black",
    "isort",
    "mypy",
    "ruff",
]
torch = [
    "torch",
    "torchvision",
    "fairscale",
    "fire",
    "blobfile",
    "fbgemm-gpu-genai==1.1.2",
]

[tool.setuptools]
package-dir = {"llama_models" = "llama_models"}
packages = {find = {}}
