Metadata-Version: 2.4
Name: llm-annotator
Version: 0.4.0
Summary: An easy-to-extend LLM annotator for robust, resumable data annotation.
Author-email: Bram Vanroy <2779410+BramVanroy@users.noreply.github.com>
License-Expression: Apache-2.0
License-File: LICENSE
Requires-Python: >=3.12
Requires-Dist: datasets<5,>=4.1.1
Requires-Dist: hf-transfer<1,>=0.1.9
Requires-Dist: hf-xet<2,>=1.1.10
Requires-Dist: mistral-common>=1.8.5
Requires-Dist: vllm<0.12,>=0.11
Description-Content-Type: text/markdown

# A simple, extensible LLM Annotator

This repository provides a small, resumable framework for annotating datasets with
LLMs (via `vllm`). Below is a minimal usage example showing how to instantiate the
`Annotator` class and run a short annotation job.

## Installation

Recommended:

```sh
uv add llm-annotator
```

or

```sh
pip install llm-annotator
```

Installing flash-infer for your version (eg CUDA12.8)

```sh
uv pip install flashinfer-python flashinfer-cubin
# JIT cache package (replace cu129 with your CUDA version: cu128, cu129, or cu130)
uv pip install flashinfer-jit-cache --index-url https://flashinfer.ai/whl/cu128
```

## Usage

See [examples/](examples/) for usage examples.


## Testing

```sh
pytest -q
```