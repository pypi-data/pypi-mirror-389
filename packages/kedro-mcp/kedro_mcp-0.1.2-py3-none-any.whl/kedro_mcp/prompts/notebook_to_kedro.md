# ü§ñ AI ASSISTANT INSTRUCTIONS - SOW-DRIVEN CONVERSION

> **CRITICAL**: These instructions create a Statement of Work (SOW) approach for reliable notebook conversion

## üéØ NOTEBOOK TO KEDRO CONVERSION - SOW ENFORCEMENT

### üö® CORE PRINCIPLE: CREATE SOW ‚Üí GET APPROVAL ‚Üí DELIVER EXACTLY

The conversion plan functions as a **Statement of Work** with specific deliverables and acceptance criteria.

---

## 6-STEP SOW WORKFLOW

#### ‚úÖ STEP 1: ANALYZE & CREATE STATEMENT OF WORK
**ANNOUNCE:** "Step 1: Creating detailed conversion plan..."

### üìã PLAN OUTPUTS (TERMINAL + FILE)
When analyzing notebook, produce **two** artifacts:

1) **Executive Summary (Terminal)** - print readable overview:
```
Conversion Plan Summary:
Pipelines (X): [list names with key transformations]
Datasets (Y): [input/intermediate/output with types]
Parameters: [configuration groups identified]
Dependencies: [only actual imports found]
```

2) **Detailed Statement of Work (File)** - comprehensive deliverable specification:
```
<project-root>/<project-name>-YYYY-MM-DD_HHMM_conversion-plan.md
```

### üîç SOW ANALYSIS REQUIREMENTS

**DEPENDENCY ANALYSIS - SCAN ACTUAL IMPORTS:**
```python
# VALIDATION CHECKLIST - Scan every cell for:
import pandas as pd          ‚úì ‚Üí kedro-datasets[pandas] 
import matplotlib.pyplot     ‚úì ‚Üí kedro-datasets[matplotlib]
from sklearn import model   ‚úì ‚Üí scikit-learn
import plotly.express        ‚úì ‚Üí kedro-datasets[plotly]
import seaborn              ‚úì ‚Üí seaborn
# 1. Mark each ‚úì/‚úó - NO package added without explicit import found
# 2. When multiple dataset backends are detected, combine extras, e.g.:
#       kedro-datasets[pandas,matplotlib]
# 3. Do not infer or install implicit dependencies.
```

**PARAMETER DECISION FRAMEWORK:**
For each hardcoded value in the notebook, evaluate:

**‚úÖ SHOULD BE CONFIGURABLE** (users will likely experiment with):
- **Experimentation values**: Learning rates, model hyperparameters, feature thresholds
- **Business logic**: Confidence thresholds, approval limits, scoring cutoffs
- **Data processing**: Batch sizes, outlier detection limits, sampling ratios
- **Feature engineering**: Correlation thresholds, scaling parameters, binning criteria
- **Model training**: Train/test splits, cross-validation folds, regularization parameters

**‚ùå KEEP HARDCODED** (structural constants unlikely to change):
- **Data schema**: Column names, expected data types, table structures
- **Mathematical constants**: PI, e, conversion factors, statistical distributions
- **Business rules fixed by regulation**: Tax rates, compliance thresholds (if legally mandated)
- **String literals**: Error messages, log statements, display text
- **System constraints**: File extensions, API endpoints (if fixed), standard formats
- **Date/time formats**: Standard format strings like '%H:%M:%S'

**DECISION CRITERIA QUESTIONS:**
For each hardcoded value, ask:
1. "Would a data scientist want to experiment with this value?"
2. "Does this value change based on business conditions or model performance?"
3. "Would different datasets or use cases require different values?"
4. "Is this value likely to be tuned during model development?"

If YES to any question ‚Üí Make it configurable
If NO to all questions ‚Üí Keep hardcoded

**PARAMETER USAGE METHODS:**
1. **Individual parameters**: `"params:parameter_name"` for single values
2. **Parameter groups**: `"params:group_name"` for related sets (PREFERRED)
3. **All parameters**: `"parameters"` only when function needs many unrelated params

### üìÑ STATEMENT OF WORK STRUCTURE

**SOW must include numbered deliverables (NO implementation guidance):**
This example illustrates structure and expected clarity only ‚Äî actual deliverables, datasets, and dependencies must be inferred from the scanned notebook.

```markdown
# KEDRO CONVERSION - STATEMENT OF WORK
## PROJECT DELIVERABLES

### DELIVERABLE 1: PIPELINE IMPLEMENTATION
1.1 data_processing pipeline
    - Task 1.1.1: clean_data(raw_table, params:data_params) ‚Üí outputs: cleaned_table  
    - Task 1.1.2: validate_data(cleaned_table) ‚Üí outputs: valid_table
    
1.2 feature_engineering pipeline  
    - Task 1.2.1: build_features(valid_table, params:feature_flags) ‚Üí outputs: features
    - Task 1.2.2: join_features(features, external_lookup) ‚Üí outputs: feature_set

1.3 model_training pipeline
    - Task 1.3.1: split_data(feature_set, params:split_params) ‚Üí outputs: train_X, train_y, test_X, test_y
    - Task 1.3.2: train_model(train_X, train_y, params:model_params) ‚Üí outputs: model
    - Task 1.3.3: evaluate_model(model, test_X, test_y) ‚Üí outputs: metrics

### DELIVERABLE 2: DATA CATALOG CONFIGURATION
2.1 raw_table ‚Üí pandas.CSVDataset (filepath: data/01_raw/raw_sales_data.csv)
2.2 cleaned_table ‚Üí pandas.ParquetDataset (filepath: data/02_intermediate/cleaned_sales.parquet)
2.3 valid_table ‚Üí pandas.ParquetDataset (filepath: data/03_primary/valid_sales.parquet)
2.4 features ‚Üí pandas.ParquetDataset (filepath: data/04_feature/features.parquet)
2.5 feature_set ‚Üí pandas.ParquetDataset (filepath: data/04_feature/feature_set.parquet)
2.6 train_X ‚Üí pandas.ParquetDataset (filepath: data/05_model_input/train_X.parquet)
2.7 train_y ‚Üí pandas.ParquetDataset (filepath: data/05_model_input/train_y.parquet)
2.8 test_X ‚Üí pandas.ParquetDataset (filepath: data/05_model_input/test_X.parquet)
2.9 test_y ‚Üí pandas.ParquetDataset (filepath: data/05_model_input/test_y.parquet)
2.10 model ‚Üí pickle.PickleDataset (filepath: data/06_models/model.pkl)
2.11 metrics ‚Üí json.JSONDataset (filepath: data/08_reporting/metrics.json)

### DELIVERABLE 3: PARAMETER CONFIGURATION
3.1 data_params group: outlier_threshold, min_samples, batch_size
3.2 feature_flags group: use_scaling, use_ohe, correlation_threshold
3.3 split_params group: test_size, random_state
3.4 model_params group: learning_rate, n_estimators, max_depth, random_state

### DELIVERABLE 4: VISUALIZATION OUTPUTS
4.1 sales_trend_plot ‚Üí matplotlib.MatplotlibDataset ‚Üí data/08_reporting/sales_trend.png
4.2 feature_importance ‚Üí plotly.JSONDataset ‚Üí data/08_reporting/feature_importance.html
4.3 model_performance ‚Üí matplotlib.MatplotlibDataset ‚Üí data/08_reporting/model_performance.png

### DELIVERABLE 5: DEPENDENCY SPECIFICATIONS
5.1 kedro-datasets[pandas,matplotlib,plotly,seaborn]  
‚ÄÉ‚ÄÉ(RATIONALE: imports detected for pandas, matplotlib, plotly, seaborn)  
5.2 scikit-learn (RATIONALE: from sklearn import found in cell Z)

TOTAL DELIVERABLES: X pipelines, Y tasks, Z datasets, W parameters, V visualizations, U dependencies
```

### üîß TECHNICAL IMPLEMENTATION STANDARDS

**NODE IMPLEMENTATION REQUIREMENTS:**
- Pure functions only (no side effects)
- Multiple inputs allowed, **exactly ONE output** per node
- Clear type hints required
- Use specific parameter references (params:group_name preferred)

**PARAMETER USAGE STANDARDS:**
```python
# ‚úÖ PREFERRED - Parameter groups
inputs=["data", "params:model_params"]
inputs=["features", "params:data_params"] 

# ‚úÖ ACCEPTABLE - Individual parameters
inputs=["data", "params:learning_rate"]

# ‚ùå USE SPARINGLY - Whole parameters (only if many unrelated params needed)
inputs=["data", "parameters"]
```

**PIPELINE CREATION STANDARDS:**
- Use `kedro pipeline create <name>` CLI only (never manual folder creation)
- Run all `kedro` CLI commands from within an existing Kedro project directory, except for the `kedro new` command
- Follow standard Kedro project structure:  
  - Define all node functions in `nodes.py`.  
  - Create pipelines with these nodes in `pipeline.py`.
- Register all pipelines in pipeline_registry.py

**DATA ORGANIZATION STANDARDS:**
```
data/01_raw/          # Original input files
data/02_intermediate/ # Cleaned, processed data
data/03_primary/      # Business logic datasets  
data/04_feature/      # Feature engineering outputs
data/05_model_input/  # Model training ready
data/06_models/       # Trained models
data/07_model_output/ # Predictions, results
data/08_reporting/    # Visualizations, reports
```

**DATASET TYPE STANDARDS:**
- Use Kedro 1.0+ dataset names only
- Correct casing: pandas.CSVDataset (not CSVDataSet)
- Match dataset type to use case (see reference table)

#### ‚úÖ STEP 2: SOW APPROVAL & SIGN-OFF
**ANNOUNCE:** "Step 2: Plan approval required..."

**USER-FRIENDLY APPROVAL:**
```
Detailed conversion plan saved to ./YYYY-MM-DD_HHMM_conversion-plan.md
Type "details" to review the full plan here.

PLAN OVERVIEW:
- Deliverable 1: X pipelines with Y total tasks  
- Deliverable 2: Z datasets with proper types
- Deliverable 3: W parameter groups 
- Deliverable 4: V visualization outputs
- Deliverable 5: U dependencies (with justification)

Proceed to implement this plan? (yes/no)
```

**APPROVAL RULES:**
- If **yes** ‚Üí SOW approved, proceed to implementation
- If **no** ‚Üí Revise plan based on feedback
- If **details** ‚Üí Show full SOW, ask again
- **NO IMPLEMENTATION until explicit approval**

#### ‚úÖ STEP 3: PROJECT SETUP
**ANNOUNCE:** "Step 3: Setting up project structure..."

```bash
kedro new --name <project_name> --tools data,lint,test --example n --telemetry no
```
**ALWAYS include 'data' tool for folder structure**

#### ‚úÖ STEP 4: SOW IMPLEMENTATION WITH PROGRESS TRACKING
**ANNOUNCE:** "Step 4: Implementing approved plan with progress updates..."

### üìä DELIVERABLE TRACKING

**Progress dashboard (update after each milestone):**
```
üìã PROJECT PROGRESS:

DELIVERABLE 1 - PIPELINE IMPLEMENTATION:
‚ñ° Pipeline 1.1 (data_processing): CREATED ‚úì/‚úó  
  ‚ñ° Task 1.1.1 (load_raw_data): COMPLETED ‚úì/‚úó
  ‚ñ° Task 1.1.2 (clean_data): COMPLETED ‚úì/‚úó  
  ‚ñ° Task 1.1.3 (validate_data): COMPLETED ‚úì/‚úó
‚ñ° Pipeline 1.2 (feature_engineering): CREATED ‚úì/‚úó
  ‚ñ° Task 1.2.1 (build_features): COMPLETED ‚úì/‚úó
  ‚ñ° Task 1.2.2 (join_features): COMPLETED ‚úì/‚úó

DELIVERABLE 2 - DATA CATALOG:
‚ñ° Item 2.1 (raw_table): CONFIGURED ‚úì/‚úó
‚ñ° Item 2.2 (cleaned_table): CONFIGURED ‚úì/‚úó  
‚ñ° Item 2.3 (valid_table): CONFIGURED ‚úì/‚úó

DELIVERABLE 3 - PARAMETER CONFIGURATION:
‚ñ° Item 3.1 (data_params): CREATED ‚úì/‚úó
‚ñ° Item 3.2 (feature_flags): CREATED ‚úì/‚úó

DELIVERABLE 4 - VISUALIZATIONS:
‚ñ° Item 4.1 (sales_trend_plot): CONFIGURED ‚úì/‚úó
‚ñ° Item 4.2 (feature_importance): CONFIGURED ‚úì/‚úó

COMPLETION STATUS: [X/TOTAL] deliverables finished
```

### üìä IMPLEMENTATION PHASES

**Phase A: Pipeline Structure Setup**
```bash
# Create each pipeline using CLI only
kedro pipeline create data_processing    # Deliverable 1.1
kedro pipeline create feature_engineering # Deliverable 1.2
kedro pipeline create model_training     # Deliverable 1.3

# VALIDATION: Verify all planned pipelines exist
ls src/<project>/pipelines/
```

**Phase B: Task Implementation** 
- Complete each numbered task from Deliverable 1
- Follow technical implementation standards
- Cross-reference: Task outputs match Deliverable 2 dataset names
- Update progress after each pipeline completion

**Phase C: Data Catalog Setup**
- Configure each dataset from Deliverable 2
- Use exact Kedro 1.0+ dataset types specified
- Verify filepaths match specifications
- Ensure correct naming conventions

**Phase D: Parameter Configuration**
- Create parameter groups from Deliverable 3 using decision framework
- Structure parameters.yml with proper grouping
- Validate all specified parameters present
- Verify parameter references in tasks

**MILESTONE CHECKPOINTS:**
```
‚ö†Ô∏è  MILESTONE REVIEW:
Deliverable [X] Status: [Y/Z] items completed
Any incomplete items? ‚Üí Complete before proceeding
All items finished? ‚Üí Move to next deliverable
```

#### ‚úÖ STEP 5: PRE-DELIVERY QUALITY ASSURANCE
**ANNOUNCE:** "Step 5: Quality assurance review before testing..."

**COMPREHENSIVE QA REVIEW:**
```
üîç QUALITY ASSURANCE CHECKLIST:

DELIVERABLE 1 REVIEW - PIPELINE IMPLEMENTATION:
‚úì Pipeline count: [actual] vs [planned] ‚Üí MATCH/MISMATCH
‚úì Task count: [actual] vs [planned] ‚Üí MATCH/MISMATCH  
‚úì Task names: All match specifications ‚Üí YES/NO
‚úì Parameter usage: Uses technical standards ‚Üí YES/NO
‚úì Single outputs: All tasks have exactly one output ‚Üí YES/NO

DELIVERABLE 2 REVIEW - DATA CATALOG:
‚úì Dataset count: [actual] vs [planned] ‚Üí MATCH/MISMATCH
‚úì Dataset types: All use Kedro 1.0+ names ‚Üí YES/NO
‚úì Type casing: Correct format (pandas.CSVDataset) ‚Üí YES/NO
‚úì File paths: All match specifications ‚Üí YES/NO
‚úì Folder structure: Follows data organization standards ‚Üí YES/NO

DELIVERABLE 3 REVIEW - PARAMETER CONFIGURATION:
‚úì Parameter groups: [actual] vs [planned] ‚Üí MATCH/MISMATCH  
‚úì Parameter keys: All specified keys present ‚Üí YES/NO
‚úì Decision quality: Parameters follow decision framework ‚Üí YES/NO
‚úì Usage: Tasks reference parameters correctly ‚Üí YES/NO

DELIVERABLE 4 REVIEW - VISUALIZATIONS:
‚úì Visualization count: [actual] vs [planned] ‚Üí MATCH/MISMATCH
‚úì Output types: Correct dataset types ‚Üí YES/NO
‚úì File paths: All match specifications ‚Üí YES/NO

DELIVERABLE 5 REVIEW - DEPENDENCIES:
‚úì Dependency count: [actual] vs [planned] ‚Üí MATCH/MISMATCH
‚úì Package list: Exactly matches plan ‚Üí YES/NO
‚úì No extras: Zero unauthorized additions ‚Üí YES/NO

QUALITY SCORE: [X/TOTAL] = [percentage]%

üö® REQUIREMENT: 100% quality score before delivery
Any issues found? ‚Üí Fix and re-review
Perfect score? ‚Üí Ready for final testing
```

#### ‚úÖ STEP 6: FINAL DELIVERY & ACCEPTANCE
**ANNOUNCE:** "Step 6: Final delivery testing..."

**DELIVERY TESTING:**
1. **Pre-test confirmation**: "Implementation complete and QA passed. Test the full pipeline? (yes/no)"
2. **Wait for approval** before testing
3. **Execute delivery test**: `kedro run`
4. **Validate deliverables**

**ACCEPTANCE CRITERIA:**
```
üéØ DELIVERY ACCEPTANCE TEST:

FUNCTIONALITY VERIFICATION:
‚ñ° All planned pipelines executed successfully: ‚úì/‚úó
‚ñ° All specified datasets created in correct locations: ‚úì/‚úó  
‚ñ° All visualization outputs generated: ‚úì/‚úó
‚ñ° No execution errors: ‚úì/‚úó
‚ñ° Data flows correctly through all pipelines: ‚úì/‚úó

DELIVERABLE VERIFICATION:
[For each Deliverable 2 item]:
‚ñ° Item 2.1 (raw_table): FILE EXISTS at specified location ‚úì/‚úó
‚ñ° Item 2.2 (cleaned_table): FILE EXISTS at specified location ‚úì/‚úó
‚ñ° Item 2.10 (model): FILE EXISTS at specified location ‚úì/‚úó

[For each Deliverable 4 item]:  
‚ñ° Item 4.1 (sales_trend_plot): VISUALIZATION EXISTS ‚úì/‚úó
‚ñ° Item 4.2 (feature_importance): VISUALIZATION EXISTS ‚úì/‚úó

PROJECT ACCEPTANCE: [X/TOTAL] criteria met
```

**SUCCESS CRITERIA:**
- [ ] 100% SOW deliverables completed
- [ ] `kedro run` executes successfully  
- [ ] All specified outputs generated and verified
- [ ] Zero deviations from approved plan

---

## üö´ PROJECT VIOLATIONS - ZERO TOLERANCE

### SOW Compliance Issues
- ‚ùå **Any deviation** from approved SOW without change order
- ‚ùå **Skip any numbered deliverable** 
- ‚ùå **Multiple outputs from single task** (violates technical standards)
- ‚ùå **Wrong dataset types** (must use specified Kedro 1.0+ types)

### Implementation Standard Violations
- ‚ùå **Manual pipeline creation** (must use `kedro pipeline create`)
- ‚ùå **Incorrect parameter usage** (must follow technical standards)
- ‚ùå **Functions with side effects** (violates pure function standard)
- ‚ùå **Wrong dataset naming** (CSVDataSet vs CSVDataset)

### Parameter Decision Violations
- ‚ùå **Make structural constants configurable** (column names, schema definitions)
- ‚ùå **Leave obvious experiment values hardcoded** (learning rates, thresholds)
- ‚ùå **Ignore decision framework** when classifying parameters

### Quality Violations
- ‚ùå **Proceed with <100% QA score**
- ‚ùå **Skip milestone checkpoints**
- ‚ùå **Ignore specification mismatches**

---

## üìö IMPLEMENTATION REFERENCE

### Kedro 1.0+ Dataset Specifications (Deliverable 2)
| Data Type | Kedro 1.0+ Dataset | Use Case | SOW Reference |
|-----------|-------------------|----------|---------------|
| CSV files | `pandas.CSVDataset` | Raw input data | Item 2.1 |
| Parquet files | `pandas.ParquetDataset` | Processed data | Items 2.2-2.9 |
| Static plots | `matplotlib.MatplotlibDataset` | PNG visualizations | Item 4.1 |
| Interactive plots | `plotly.JSONDataset` | HTML charts | Item 4.2 |
| JSON data | `json.JSONDataset` | Metadata, results | Item 2.11 |
| Pickle files | `pickle.PickleDataset` | Models | Item 2.10 |

### Parameter Decision Examples
```
‚úÖ MAKE CONFIGURABLE:
- learning_rate = 0.01 (experimentation value)
- confidence_threshold = 0.8 (business logic tuning)
- batch_size = 32 (performance optimization)
- test_size = 0.2 (data splitting experimentation)

‚ùå KEEP HARDCODED:
- column_names = ["age", "income"] (data schema)
- PI = 3.14159 (mathematical constant)
- "Model training complete" (display message)
- file_extension = ".csv" (system constraint)
```

---

*SOW-driven implementation - Reliable delivery guaranteed*
