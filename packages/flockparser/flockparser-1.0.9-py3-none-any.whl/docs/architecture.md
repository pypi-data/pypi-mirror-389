# FlockParser Architecture

## Table of Contents

- [Why Distributed AI Inference?](#why-distributed-ai-inference)
- [System Overview](#system-overview)
- [Core Components](#core-components)
- [Intelligent Routing](#intelligent-routing)
- [Privacy Architecture](#privacy-architecture)
- [Technical Deep Dive](#technical-deep-dive)
- [Design Decisions](#design-decisions)

---

## Why Distributed AI Inference?

### The Problem

Most RAG (Retrieval-Augmented Generation) systems assume:
- **Homogeneous hardware** - All nodes have identical GPUs/CPUs
- **Single-node processing** - One powerful machine does everything
- **Cloud-first** - External APIs (OpenAI, Anthropic) handle inference
- **Network perfection** - No failures, no latency spikes

**Real-world constraints:**
- Mixed hardware (RTX 4090 + GTX 1050Ti + CPU-only laptops)
- Limited budgets (can't afford 8x A100s)
- Privacy requirements (healthcare, legal, financial docs)
- Network unpredictability (nodes go down, latency varies)

### The Solution

FlockParser treats **inference as a distributed systems problem**:

1. **Auto-discovery** - Find available Ollama nodes on the network
2. **Health scoring** - Rank nodes by GPU presence, VRAM, response time
3. **Adaptive routing** - Sequential when one node dominates, parallel when balanced
4. **Graceful degradation** - Failover to slower nodes if fast ones are busy

**Result:** 61.7x speedup on heterogeneous hardware (372s ‚Üí 6s) in real demo.

---

## System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Interfaces (4 Options)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   CLI    ‚îÇ  ‚îÇ  Web UI  ‚îÇ  ‚îÇ REST API ‚îÇ  ‚îÇ MCP Server   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ (local)  ‚îÇ  ‚îÇ (local)  ‚îÇ  ‚îÇ(network) ‚îÇ  ‚îÇ (Claude)     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ         FlockParser Core Engine             ‚îÇ
        ‚îÇ                                             ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ      Document Processing Pipeline     ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ                                       ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  PDF ‚Üí Text Extraction ‚Üí OCR Fallback‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Üí Multi-format (TXT/MD/DOCX/JSON)   ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îÇ                 ‚ñº                           ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ   ChromaDB Vector Database (Local)   ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ                                       ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Persistent embeddings storage     ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Semantic similarity search        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Metadata + source tracking        ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îÇ                 ‚ñº                           ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
        ‚îÇ  ‚îÇ   Intelligent Load Balancer          ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ                                       ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Node discovery (HTTP scanning)    ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Health scoring (GPU/VRAM/speed)   ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Routing strategy (adaptive)       ‚îÇ  ‚îÇ
        ‚îÇ  ‚îÇ  ‚Ä¢ Failover & retry logic            ‚îÇ  ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ      Distributed Ollama Cluster             ‚îÇ
        ‚îÇ                                             ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
        ‚îÇ  ‚îÇ  Node 1    ‚îÇ  ‚îÇ  Node 2    ‚îÇ  ‚îÇNode 3 ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ  .90:11434 ‚îÇ  ‚îÇ  .91:11434 ‚îÇ  ‚îÇ.92... ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ       ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ GPU: A4000 ‚îÇ  ‚îÇ GPU: 1050Ti‚îÇ  ‚îÇCPU-   ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ VRAM: 16GB ‚îÇ  ‚îÇ VRAM: 4GB  ‚îÇ  ‚îÇonly   ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ Health:367 ‚îÇ  ‚îÇ Health:210 ‚îÇ  ‚îÇ 50    ‚îÇ‚îÇ
        ‚îÇ  ‚îÇ Status: ‚úì  ‚îÇ  ‚îÇ Status: ‚úì  ‚îÇ  ‚îÇ ‚úì     ‚îÇ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
        ‚îÇ                                             ‚îÇ
        ‚îÇ  Inference requests distributed based on:  ‚îÇ
        ‚îÇ  ‚Ä¢ Workload size (embeddings vs generation)‚îÇ
        ‚îÇ  ‚Ä¢ Node availability (not processing)      ‚îÇ
        ‚îÇ  ‚Ä¢ Speed ratios (7.2x threshold)           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Core Components

### 1. Document Processing Pipeline

**Purpose:** Convert PDFs ‚Üí searchable text with multiple output formats

**Flow:**
1. **PDF Ingestion** - Accept PDF files via any interface
2. **Text Extraction** - Try pdfplumber, PyPDF2, pypdf in sequence
3. **OCR Fallback** - If extraction fails/returns nothing, use Tesseract
4. **Format Conversion** - Generate TXT, Markdown, DOCX, JSON outputs
5. **Embedding Generation** - Send to Ollama cluster for vector embeddings
6. **Storage** - Persist in ChromaDB with metadata (filename, page numbers, timestamps)

**Key Technologies:**
- `pdfplumber` - Primary PDF extraction (handles text, tables)
- `pytesseract` - OCR for scanned documents
- `pdf2image` - Convert PDF pages to images for OCR
- `python-docx` - DOCX generation
- `markdown` - Markdown formatting

---

### 2. ChromaDB Vector Database

**Purpose:** Semantic search over document embeddings

**Why ChromaDB?**
- ‚úÖ **Local-first** - No external API calls (privacy)
- ‚úÖ **Persistent** - Survives restarts, no re-processing
- ‚úÖ **Performant** - SQLite backend for single-user, scales to PostgreSQL
- ‚úÖ **Metadata-aware** - Store source file, page numbers, timestamps
- ‚ùå **Concurrent writes limited** - SQLite backend (use PostgreSQL in production)

**Collections:**
- `flockparse_cli` - CLI/Web UI documents
- `document_store` - REST API documents
- Separate databases prevent locking issues

**Embedding Model:**
- `mxbai-embed-large` (default) - 1024-dimensional embeddings
- `nomic-embed-text` (alternative) - Lower memory, faster

---

### 3. Intelligent Load Balancer

**Purpose:** Route inference requests across heterogeneous Ollama nodes

#### Node Discovery

```python
def discover_nodes():
    """
    Scan local network for Ollama instances
    - Try common ports (11434)
    - Check /api/tags endpoint for availability
    - Return list of healthy nodes
    """
    nodes = []
    for ip in local_network_range():
        try:
            response = requests.get(f"http://{ip}:11434/api/tags", timeout=2)
            if response.status_code == 200:
                nodes.append({"url": f"http://{ip}:11434"})
        except:
            continue
    return nodes
```

#### Health Scoring

Each node gets a **health score** based on:

| Factor | Score Impact | Rationale |
|--------|--------------|-----------|
| **GPU present** | +200 | Orders of magnitude faster |
| **GPU available (no active load)** | +50 | Can utilize GPU immediately |
| **VRAM > 8GB** | +100 | Can run larger models |
| **Response time < 100ms** | +50 | Network latency matters |
| **Recent failures** | -100 per failure | Avoid unstable nodes |
| **CPU-only** | Base 50 | Still useful for fallback |

**Example:**
- Node 1: GPU (A4000 16GB, active) = 200 + 100 + 50 = **350**
- Node 2: GPU (1050Ti 4GB, idle) = 200 + 50 + 50 = **300**
- Node 3: CPU-only = **50**

#### Routing Strategy (Adaptive)

**Problem:** Should we route sequentially (all to fastest node) or in parallel?

**Solution:** Calculate speed ratio and decide adaptively.

```python
def choose_routing_strategy(nodes):
    """
    Sequential: All requests to fastest node
    Parallel: Distribute across all nodes

    Decision: If fastest node is >7.2x faster than average,
              use sequential. Otherwise, parallel.
    """
    fastest = nodes[0].speed
    average = sum(n.speed for n in nodes) / len(nodes)
    ratio = fastest / average

    if ratio > 7.2:  # Dominant node exists
        return "sequential", [nodes[0]]
    else:  # Balanced cluster
        return "parallel", nodes
```

**Real-world example:**
- **Single A4000 (GPU)** vs **8x CPU nodes** ‚Üí Sequential (A4000 is 10x+ faster)
- **2x A4000** + **3x RTX 3090** ‚Üí Parallel (all GPUs, similar speed)
- **A4000** + **1050Ti** + **CPU** ‚Üí Sequential (A4000 dominates, others bottleneck)

---

### 4. Multi-Protocol Interfaces

**Why 4 interfaces?** Different use cases require different privacy/usability tradeoffs.

#### CLI (`flockparsecli.py`)
- **Privacy:** üü¢ 100% local
- **Use case:** Power users, scripting, air-gapped systems
- **Features:** Interactive chat, batch processing, node management

#### Web UI (`flock_webui.py`)
- **Privacy:** üü¢ 100% local (Streamlit runs on localhost)
- **Use case:** General users, visual feedback, monitoring dashboards
- **Features:** Drag-and-drop PDFs, real-time VRAM charts, node health display

#### REST API (`flock_ai_api.py`)
- **Privacy:** üü° Local network (no internet required)
- **Use case:** Multi-user environments, application integration
- **Features:** API key auth, FastAPI (auto-generated docs), stateless endpoints

#### MCP Server (`flock_mcp_server.py`)
- **Privacy:** üî¥ Cloud (integrates with Claude Desktop ‚Üí Anthropic API)
- **Use case:** AI assistant workflows, developer tooling
- **Features:** Claude can process PDFs, query documents, chat with context

---

## Intelligent Routing

### Scenario 1: Homogeneous Cluster (4x RTX 4090)

**Decision:** Parallel routing

**Rationale:**
- All nodes have similar performance (~5 tokens/sec)
- Speed ratio: 5 / 5 = 1.0 (< 7.2 threshold)
- Bottleneck: LLM inference time, not orchestration overhead
- **4x parallelism = 4x throughput**

**Implementation:**
```python
# Split document chunks across all nodes
chunks = split_document(doc, num_chunks=len(nodes))
results = parallel_map(process_chunk, chunks, nodes)
combined = merge_results(results)
```

---

### Scenario 2: Heterogeneous Cluster (A4000 + 3x CPU)

**Decision:** Sequential routing (to A4000 only)

**Rationale:**
- A4000: ~50 tokens/sec (GPU)
- CPU nodes: ~3 tokens/sec (CPU)
- Speed ratio: 50 / 13 = 3.8x ‚Üí Wait, threshold is 7.2x
- **Actually parallel in this case!**

**But if it were 1050Ti instead:**
- A4000: 50 tokens/sec
- 1050Ti + CPUs: ~5 tokens/sec average
- Speed ratio: 50 / 5 = **10x** ‚Üí Sequential routing
- CPUs would bottleneck (wait for slowest to finish)

---

### Scenario 3: Network Partition (Node failure)

**Problem:** Node 1 (fastest) becomes unreachable mid-request

**Solution:**
1. **Detect failure** - Timeout after 30 seconds
2. **Update health score** - Node 1 health -= 100
3. **Failover** - Retry request on Node 2
4. **Re-rank nodes** - Node 2 becomes primary
5. **Background recovery** - Periodically ping Node 1, restore when healthy

```python
try:
    result = send_to_node(node1, request, timeout=30)
except TimeoutError:
    node1.health -= 100  # Penalize
    node1.last_failure = time.now()
    result = send_to_node(node2, request)  # Failover
```

---

## Privacy Architecture

### Design Principle: Privacy as a Spectrum

Not all use cases have the same privacy requirements:

| Use Case | Privacy Needs | Best Interface |
|----------|---------------|----------------|
| **Healthcare records** | HIPAA compliance, air-gapped | CLI (100% local) |
| **Legal discovery** | Attorney-client privilege | CLI or Web UI (local) |
| **Enterprise research** | Internal network only | REST API (local network) |
| **Personal productivity** | Convenience > privacy | MCP (Claude Desktop) |

### Implementation Details

#### 100% Local (CLI + Web UI)
- No outbound network calls
- ChromaDB on local filesystem
- Ollama nodes on LAN (optional internet for model downloads)
- Can run fully air-gapped after setup

#### Local Network (REST API)
- Listens on `0.0.0.0:8000` (configurable)
- API key authentication (bearer tokens)
- No external dependencies
- Firewall can restrict to trusted IPs

#### Cloud Integration (MCP Server)
- **What's sent to Anthropic:**
  - User queries ("Summarize this document")
  - Document snippets retrieved from ChromaDB (top 5 results)
  - Chat history (conversation context)

- **What stays local:**
  - Full document text (only embeddings sent to Claude)
  - ChromaDB database contents
  - Ollama node IPs and configurations

- **User control:**
  - Explicitly choose MCP interface (opt-in)
  - Can disable by removing from Claude Desktop config
  - Warning displayed in README and documentation

---

## Technical Deep Dive

### PDF Processing Pipeline

**Challenge:** PDFs are notoriously difficult to parse.

**Our approach (3-tier fallback):**

1. **pdfplumber** (First attempt)
   - Best for text-based PDFs with tables
   - Handles: Font extraction, table detection, layout preservation
   - Fails on: Scanned images, encrypted PDFs, corrupted files

2. **PyPDF2 / pypdf** (Second attempt)
   - Fallback if pdfplumber fails
   - Simpler extraction (no tables)
   - Handles: Basic text PDFs, some encrypted files

3. **OCR (Tesseract)** (Last resort)
   - Converts PDF pages ‚Üí images ‚Üí text
   - Handles: Scanned documents, images of text
   - Slow but universal (works on anything visible)

```python
def extract_text(pdf_path):
    # Tier 1: pdfplumber
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = "\n".join(page.extract_text() for page in pdf.pages)
            if len(text) > 100:  # Reasonable content
                return text
    except:
        pass

    # Tier 2: pypdf
    try:
        reader = pypdf.PdfReader(pdf_path)
        text = "\n".join(page.extract_text() for page in reader.pages)
        if len(text) > 100:
            return text
    except:
        pass

    # Tier 3: OCR
    images = pdf2image.convert_from_path(pdf_path, dpi=300)
    text = "\n".join(pytesseract.image_to_string(img) for img in images)
    return text
```

---

### Embedding Generation

**Why embeddings?** Convert text ‚Üí vectors for semantic search.

**Example:**
- Query: "What are the financial results?"
- Matches: "Q3 revenue was $50M" (similar meaning, different words)
- Doesn't match: "The meeting is at 3pm" (different semantic meaning)

**Process:**
1. **Chunk documents** - Split into 512-token chunks (overlap: 50 tokens)
2. **Generate embeddings** - Send to Ollama with `mxbai-embed-large`
3. **Store in ChromaDB** - With metadata (filename, page, chunk_id)
4. **Query time** - Embed query ‚Üí cosine similarity search ‚Üí top K results

**Performance:**
- Embedding: ~100ms per chunk (GPU) vs ~2s (CPU)
- Storage: ~4KB per chunk (1024-dim float32 vectors)
- Search: <10ms for 10K chunks (ChromaDB with HNSW index)

---

### MCP Integration (Model Context Protocol)

**What is MCP?** Anthropic's 2024 standard for AI tool integration.

**How it works:**
1. Claude Desktop reads `claude_desktop_config.json`
2. Spawns `flock_mcp_server.py` as subprocess (stdio transport)
3. Discovers tools via JSON-RPC protocol
4. User asks Claude: "Process this PDF"
5. Claude calls `process_pdf` tool via MCP
6. FlockParser processes ‚Üí returns result ‚Üí Claude formats response

**Tools exposed:**
- `process_pdf(file_path)` - Extract + embed document
- `query_documents(query)` - Semantic search
- `chat_with_context(message, session_id)` - RAG with history

**Challenges solved:**
- **Timeouts** - PDFs can take 5+ minutes (set 5min timeout)
- **Concurrency** - ThreadPoolExecutor with 50 workers
- **Database locking** - Separate ChromaDB per interface
- **Absolute paths** - MCP runs from different working directory

---

## Design Decisions

### 1. Why Ollama instead of cloud APIs?

**Pros:**
- ‚úÖ Privacy (no data leaves your network)
- ‚úÖ Cost (no per-token charges)
- ‚úÖ Customization (run any model)
- ‚úÖ Offline capability (air-gapped deployments)

**Cons:**
- ‚ùå Requires local compute (GPU/CPU)
- ‚ùå Slower than GPT-4 (but 60x faster with distribution!)
- ‚ùå Model management (manual downloads)

**Decision:** Privacy + cost benefits outweigh complexity for target users.

---

### 2. Why ChromaDB instead of Pinecone/Weaviate?

**Pros:**
- ‚úÖ Local-first (no cloud accounts)
- ‚úÖ Simple setup (pip install, done)
- ‚úÖ Good performance (HNSW indexing)
- ‚úÖ Metadata filtering (source tracking)

**Cons:**
- ‚ùå SQLite backend (concurrent write limits)
- ‚ùå No built-in sharding (single-node)

**Decision:** Simplicity + privacy > scale for v1.0. Can upgrade to pgvector later.

---

### 3. Why 7.2x threshold for routing?

**Based on queuing theory:**
- Parallel processing has orchestration overhead (~10%)
- Sequential processing avoids coordination
- Crossover point: When fastest node is ~7x faster than average, sequential wins

**Empirical validation:**
- Tested with 3-node clusters (various GPU/CPU mixes)
- 7.2x threshold consistently gave best results
- Could be tuned per-cluster (future improvement)

---

### 4. Why separate databases per interface?

**Problem:** ChromaDB (SQLite backend) can't handle concurrent writes.

**Solutions considered:**
1. ‚ùå **Locking mechanism** - Too slow, complex
2. ‚ùå **PostgreSQL backend** - Requires external DB setup
3. ‚úÖ **Separate databases** - Simple, works for v1.0

**Trade-off:** Documents processed in CLI aren't visible in API (separate namespaces).

**Future:** Offer PostgreSQL backend as optional dependency.

---

### 5. Why ThreadPoolExecutor with 50 workers?

**Problem:** Default Python ThreadPoolExecutor uses min(32, CPU_count + 4) workers.

**Issue:** PDF processing can take 300+ seconds. With 32 workers, worker 33+ block indefinitely.

**Solution:** Increase to 50 workers (reasonable for I/O-bound tasks).

**Trade-off:** More memory usage (~50MB per worker), but prevents blocking.

---

## Performance Characteristics

### Bottlenecks (in order)

1. **LLM inference time** - 80% of total time (GPU helps most here)
2. **Network latency** - 10% (LAN vs internet makes difference)
3. **PDF extraction** - 5% (OCR is slow, ~10s per page)
4. **Embedding generation** - 3% (fast on GPU)
5. **ChromaDB search** - 2% (HNSW index is efficient)

### Scaling Limits

| Component | Single-node limit | Distributed limit | Bottleneck |
|-----------|------------------|-------------------|------------|
| **PDF processing** | 1 page/sec (OCR) | 10 pages/sec | OCR speed |
| **Embeddings** | 100 chunks/sec (GPU) | 500 chunks/sec | GPU memory |
| **LLM inference** | 50 tokens/sec | 500 tokens/sec | Model size |
| **Search** | 1000 queries/sec | 1000 queries/sec | Not distributed |

**Next bottleneck:** Embedding generation becomes limiting factor at ~5 nodes.

---

## Future Architecture

### Roadmap

1. **PostgreSQL backend** (Q1 2025) - Better concurrent access
2. **WebSocket MCP transport** (Q2 2025) - Streaming responses
3. **Prometheus metrics** (Q2 2025) - Production monitoring
4. **Multi-cluster federation** (Q3 2025) - Scale beyond LAN
5. **GPU utilization verification** (Q4 2025) - Check actual VRAM usage

### Research Questions

- Can we predict node failure before it happens? (ML-based health scoring)
- Is there a better routing algorithm than speed ratio? (RL-based)
- How to handle partial failures in parallel mode? (speculative execution)

---

## Related Documentation

- [README.md](../README.md) - Quickstart and usage
- [VRAM_MONITORING.md](../VRAM_MONITORING.md) - GPU monitoring
- [CHROMADB_PRODUCTION.md](../CHROMADB_PRODUCTION.md) - Database scaling
- [ERROR_HANDLING.md](../ERROR_HANDLING.md) - Troubleshooting
- [CONTRIBUTING.md](../CONTRIBUTING.md) - Development setup
