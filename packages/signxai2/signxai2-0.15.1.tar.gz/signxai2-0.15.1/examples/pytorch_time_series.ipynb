{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch ECG Model Tutorial - SignXAI2 Time Series Explainability\n\nThis tutorial demonstrates how to use SignXAI2 to explain PyTorch ECG classification models, following the structure from the official documentation.\n\n## ‚ö†Ô∏è Data Requirements\n\n**This tutorial requires ECG data from the repository:**\n\n```bash\n# Clone the repository to get the complete dataset\ngit clone https://github.com/your-repo/signxai2.git\n```\n\n**Required data files:**\n- ECG records in `examples/data/timeseries/`\n- Pre-trained models in `examples/data/models/pytorch/ECG/`\n- Utility functions for ECG processing\n\nThe PyPI package alone doesn't include ECG data to keep the package size manageable.\n\n## What you'll learn:\n\n1. **PyTorch ECG Models**: Building CNN models for time series classification\n2. **ECG Data Processing**: Loading and preprocessing 12-lead ECG data\n3. **Time Series XAI**: Applying various explainability methods to sequential data\n4. **12-Lead Visualization**: Creating professional medical visualizations\n5. **Multiple Methods**: Comparing different XAI approaches\n6. **Dynamic Method Parsing**: Using the new unified API with embedded parameters\n\n## ECG Signal Components:\nUnderstanding what the model should focus on:\n- **P-wave**: Atrial depolarization (0.08-0.12 sec)\n- **QRS complex**: Ventricular depolarization (0.06-0.10 sec)  \n- **T-wave**: Ventricular repolarization (0.16 sec)\n- **PR interval**: AV conduction time (0.12-0.20 sec)\n- **QT interval**: Total ventricular activity (0.35-0.44 sec)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader\nimport warnings\nimport os\nimport sys\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# SignXAI imports\nfrom signxai import explain, list_methods\n\n# Add project root to path for utility imports\ncurrent_dir = os.getcwd()\nproject_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n# Import ECG utilities (from quickstart files)\ntry:\n    from utils.ecg_data import load_and_preprocess_ecg\n    from utils.ecg_visualization import plot_ecg\n    from utils.ecg_explainability import normalize_ecg_relevancemap\n    print(\"‚úÖ ECG utilities loaded successfully!\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è ECG utilities not available: {e}\")\n    print(\"Please ensure you have the complete repository with utils/ directory\")\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\n\n# ECG lead names for reference\nLEAD_NAMES = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\nprint(f\"\\n12-Lead ECG configuration: {', '.join(LEAD_NAMES)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Generate Synthetic ECG Data\n\nFollowing the documentation approach (lines 75-92), we'll generate synthetic ECG data with characteristic patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate synthetic data (in practice, you would use real ECG datasets)\ndef generate_synthetic_ecg_data(n_samples=1000, seq_length=1000, n_classes=2):\n    X = np.random.randn(n_samples, seq_length, 1) * 0.1\n    # Add synthetic patterns for different classes\n    for i in range(n_samples):\n        if i % n_classes == 0:  # Class 0: Normal\n            # Add normal QRS complex\n            X[i, 400:420, 0] += np.sin(np.linspace(0, np.pi, 20)) * 1.0\n            X[i, 350:370, 0] += np.sin(np.linspace(0, np.pi, 20)) * 0.2  # P wave\n            X[i, 450:480, 0] += np.sin(np.linspace(0, np.pi, 30)) * 0.3  # T wave\n        else:  # Class 1: Abnormal\n            # Add abnormal QRS complex\n            X[i, 380:410, 0] += np.sin(np.linspace(0, np.pi, 30)) * 0.8\n            X[i, 420:460, 0] -= np.sin(np.linspace(0, np.pi, 40)) * 0.4\n            \n    # Create labels\n    y = np.array([i % n_classes for i in range(n_samples)])\n    return X, y\n\n# Generate data\nX_train, y_train = generate_synthetic_ecg_data(800, 1000, 2)\nX_test, y_test = generate_synthetic_ecg_data(200, 1000, 2)\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Test data shape: {X_test.shape}\")\n\n# Visualize some samples\nplt.figure(figsize=(15, 8))\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    sample_idx = i * 50\n    plt.plot(X_test[sample_idx, :, 0])\n    plt.title(f'Sample {sample_idx}, Class: {y_test[sample_idx]}')\n    plt.xlabel('Time (samples)')\n    plt.ylabel('Amplitude')\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Create CNN Model for ECG Classification\n\nFollowing the documentation structure (lines 98-113), we'll create a CNN model for ECG classification."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a PyTorch CNN model for ECG classification\nclass ECG_CNN(nn.Module):\n    def __init__(self, seq_length=1000):\n        super(ECG_CNN, self).__init__()\n        self.conv1 = nn.Conv1d(1, 16, kernel_size=5)\n        self.pool1 = nn.MaxPool1d(2)\n        self.conv2 = nn.Conv1d(16, 32, kernel_size=5)\n        self.pool2 = nn.MaxPool1d(2)\n        self.conv3 = nn.Conv1d(32, 64, kernel_size=5)  # Named for Grad-CAM compatibility\n        self.pool3 = nn.MaxPool1d(2)\n        \n        # Calculate size after convolutions and pooling\n        self.flat_size = 64 * (((seq_length - 4) // 2 - 4) // 2 - 4) // 2\n        \n        self.fc1 = nn.Linear(self.flat_size, 64)\n        self.dropout = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(64, 2)  # No activation (logits)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        # Conv blocks\n        x = self.pool1(self.relu(self.conv1(x)))\n        x = self.pool2(self.relu(self.conv2(x)))\n        x = self.pool3(self.relu(self.conv3(x)))\n        \n        # Flatten\n        x = x.view(-1, self.flat_size)\n        \n        # Fully connected\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        \n        return x\n\n# Create the model\nmodel = ECG_CNN()\nprint(\"Model created successfully!\")\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Train the Model\n\nFollowing the documentation training procedure (lines 115-127)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to PyTorch tensors and prepare data loaders\n# PyTorch expects [batch, channels, time] format\nX_train_pt = torch.tensor(X_train.transpose(0, 2, 1), dtype=torch.float32)\ny_train_pt = torch.tensor(y_train, dtype=torch.long)\nX_test_pt = torch.tensor(X_test.transpose(0, 2, 1), dtype=torch.float32)\ny_test_pt = torch.tensor(y_test, dtype=torch.long)\n\ntrain_dataset = TensorDataset(X_train_pt, y_train_pt)\ntest_dataset = TensorDataset(X_test_pt, y_test_pt)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Initialize loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\n\nprint(\"Training the model...\")\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop\nepochs = 10\ntrain_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    \n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n    \n    # Validation\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss/len(train_loader)\n    epoch_acc = correct_train/total_train\n    val_acc = correct/total\n    \n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_acc)\n    val_accuracies.append(val_acc)\n    \n    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Acc: {val_acc:.4f}')\n\nprint(f'\\n‚úÖ Training completed!')\nprint(f'Test accuracy: {val_acc:.4f}')\n\n# Plot training history\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Training Accuracy')\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Prepare Sample for XAI Analysis\n\nFollowing the documentation approach (lines 129-149), we'll prepare a sample for explanation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the model and sample\necg_sample = X_test[0, :, 0]  # First test sample, remove channel dimension for visualization\n\n# Prepare input with batch dimension - PyTorch expects [batch, channels, time] format\nx = torch.tensor(X_test[0:1].transpose(0, 2, 1), dtype=torch.float32)\n\nprint(f\"Sample shape for XAI: {x.shape}\")\nprint(f\"Sample shape for visualization: {ecg_sample.shape}\")\n\n# Get prediction\nmodel.eval()\nwith torch.no_grad():\n    preds = model(x)\n    predicted_class = torch.argmax(preds, dim=1).item()\n    probabilities = torch.nn.functional.softmax(preds, dim=1)\n    \nclass_names = ['Normal', 'Abnormal']\n\nprint(f\"Predicted class: {predicted_class} ({class_names[predicted_class]})\")\nprint(f\"Confidence: {probabilities[0, predicted_class]:.4f}\")\n\n# Visualize the sample\nplt.figure(figsize=(15, 4))\nplt.plot(ecg_sample)\nplt.title(f'ECG Sample for XAI Analysis\\nPredicted: {class_names[predicted_class]} (confidence: {probabilities[0, predicted_class]:.3f})')\nplt.xlabel('Time (samples)')\nplt.ylabel('Amplitude')\nplt.grid(True)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Generate Explanations with Multiple Methods\n\nFollowing the documentation structure (lines 150-179), we'll calculate explanations with different methods including Grad-CAM."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate explanations with different methods\nmethods = [\n    'gradient',\n    'gradient_x_input',\n    'integrated_gradients',\n    'grad_cam',  # Works for time series too\n    'lrp_z',\n    'lrp_epsilon_0_1',\n    'lrpsign_z'  # The SIGN method\n]\n\nexplanations = {}\nprint(\"Calculating explanations...\")\n\nfor method in methods:\n    try:\n        print(f\"  Computing: {method}\")\n        if method == 'grad_cam':\n            # For PyTorch grad_cam, we need to specify the target layer\n            explanations[method] = explain(\n                model=model,\n                x=x,\n                method_name=method,\n                target_class=predicted_class,\n                layer_name='conv3'  # Target the last conv layer\n            )\n        else:\n            explanations[method] = explain(\n                model=model,\n                x=x,\n                method_name=method,\n                target_class=predicted_class\n            )\n        print(f\"    ‚úÖ Success\")\n    except Exception as e:\n        print(f\"    ‚ùå Failed: {e}\")\n        # Create dummy explanation for visualization\n        explanations[method] = torch.zeros_like(x)\n\nprint(f\"\\n‚úÖ Generated explanations for {len(explanations)} methods\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Visualize Separate Explanations\n\nFollowing the documentation approach (lines 180-201), we'll create separate plots for each method."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize explanations\nfig, axs = plt.subplots(len(methods) + 1, 1, figsize=(15, 3*(len(methods) + 1)))\n\n# Original signal\naxs[0].plot(ecg_sample)\naxs[0].set_title('Original ECG Signal')\naxs[0].set_ylabel('Amplitude')\naxs[0].grid(True)\n\n# Explanations\nfor i, method in enumerate(methods):\n    if method in explanations:\n        # Reshape explanation to 1D (PyTorch format is [batch, channel, time])\n        if isinstance(explanations[method], torch.Tensor):\n            expl = explanations[method].detach().cpu().numpy()[0, 0, :]\n        else:\n            expl = explanations[method][0, 0, :]\n        \n        # Plot explanation\n        axs[i+1].plot(expl)\n        axs[i+1].set_title(f'Method: {method}')\n        axs[i+1].set_ylabel('Attribution')\n        axs[i+1].grid(True)\n    else:\n        axs[i+1].text(0.5, 0.5, 'Method failed', transform=axs[i+1].transAxes, ha='center')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Overlay Visualizations\n\nFollowing the documentation approach (lines 202-222), we'll create overlay visualizations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative visualization: Overlay explanation on signal\nplt.figure(figsize=(15, 10))\n\nfor i, method in enumerate(methods):\n    if method not in explanations:\n        continue\n        \n    plt.subplot(len(methods), 1, i+1)\n    \n    # Original signal\n    plt.plot(ecg_sample, 'gray', alpha=0.5, label='ECG Signal')\n    \n    # Explanation\n    if isinstance(explanations[method], torch.Tensor):\n        expl = explanations[method].detach().cpu().numpy()[0, 0, :]\n    else:\n        expl = explanations[method][0, 0, :]\n        \n    expl_norm = (expl - expl.min()) / (expl.max() - expl.min()) if expl.max() > expl.min() else expl\n    plt.plot(expl_norm, 'r', label='Attribution')\n    \n    plt.title(f'Method: {method}')\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Advanced ECG Component Analysis\n\nFollowing the documentation approach for advanced analysis, we'll analyze specific ECG components."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define characteristic ECG components (these would be expert-identified in real applications)\np_wave_region = slice(350, 370)\nqrs_complex_region = slice(400, 420)\nt_wave_region = slice(450, 480)\n\n# Calculate the mean attribution for each region using LRP-SIGN method\nif 'lrpsign_z' in explanations:\n    if isinstance(explanations['lrpsign_z'], torch.Tensor):\n        lrpsign_expl = explanations['lrpsign_z'].detach().cpu().numpy()[0, 0, :]\n    else:\n        lrpsign_expl = explanations['lrpsign_z'][0, 0, :]\n    \n    p_wave_attr = np.mean(np.abs(lrpsign_expl[p_wave_region]))\n    qrs_complex_attr = np.mean(np.abs(lrpsign_expl[qrs_complex_region]))\n    t_wave_attr = np.mean(np.abs(lrpsign_expl[t_wave_region]))\n    \n    # Visualize with region highlighting\n    plt.figure(figsize=(15, 6))\n    \n    # Plot original ECG\n    plt.subplot(2, 1, 1)\n    plt.plot(ecg_sample)\n    \n    # Highlight ECG components\n    plt.axvspan(350, 370, color='blue', alpha=0.2, label='P-wave')\n    plt.axvspan(400, 420, color='red', alpha=0.2, label='QRS Complex')\n    plt.axvspan(450, 480, color='green', alpha=0.2, label='T-wave')\n    \n    plt.title('ECG Signal with Components')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot explanation with component attribution\n    plt.subplot(2, 1, 2)\n    plt.plot(lrpsign_expl)\n    \n    # Highlight attribution in ECG components\n    plt.axvspan(350, 370, color='blue', alpha=0.2)\n    plt.axvspan(400, 420, color='red', alpha=0.2)\n    plt.axvspan(450, 480, color='green', alpha=0.2)\n    \n    # Add component attribution values\n    plt.text(360, max(lrpsign_expl), f'P-wave: {p_wave_attr:.4f}', \n             horizontalalignment='center', backgroundcolor='white')\n    plt.text(410, max(lrpsign_expl), f'QRS: {qrs_complex_attr:.4f}', \n             horizontalalignment='center', backgroundcolor='white')\n    plt.text(465, max(lrpsign_expl), f'T-wave: {t_wave_attr:.4f}', \n             horizontalalignment='center', backgroundcolor='white')\n    \n    plt.title('LRP-SIGN Attribution')\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nüîç ECG Component Analysis:\")\n    print(f\"  P-wave attribution: {p_wave_attr:.4f}\")\n    print(f\"  QRS complex attribution: {qrs_complex_attr:.4f}\")\n    print(f\"  T-wave attribution: {t_wave_attr:.4f}\")\nelse:\n    print(\"‚ö†Ô∏è LRP-SIGN method not available for component analysis\")"
  },
  {
   "cell_type": "code",
   "source": "# Try to load real ECG data and create 12-lead visualization\ntry:\n    # Load real ECG data from repository\n    record_id = '03509_hr'\n    ecg_src_dir = os.path.join(project_root, 'examples', 'data', 'timeseries', '')\n    \n    print(f\"Loading ECG data for record: {record_id}...\")\n    ecg_data = load_and_preprocess_ecg(\n        record_id=record_id,\n        src_dir=ecg_src_dir,\n        ecg_filters=['BWR', 'BLA', 'AC50Hz', 'LP40Hz'],\n        subsampling_window_size=3000,\n        subsample_start=0\n    )\n    \n    if ecg_data is not None:\n        print(f\"‚úÖ ECG data loaded: {ecg_data.shape}\")\n        \n        # Use single lead for model prediction\n        ecg_single_lead = ecg_data[:, 0:1]  # Shape: (3000, 1)\n        input_tensor = torch.from_numpy(ecg_single_lead).float().permute(1, 0).unsqueeze(0)\n        \n        # Get explanation for real ECG\n        with torch.no_grad():\n            output = model(input_tensor)\n        predicted_idx = torch.argmax(output, dim=1)\n        \n        # Calculate explanation with one method\n        explanation = explain(\n            model,\n            input_tensor,\n            method_name=\"gradient_x_input\",\n            target_class=predicted_idx.item()\n        )\n        \n        # Process for visualization\n        if isinstance(explanation, torch.Tensor):\n            explanation_np = explanation.detach().cpu().numpy()\n        else:\n            explanation_np = explanation\n        \n        # Handle shape and expand to 12 leads\n        if explanation_np.ndim == 3:\n            relevance_map = explanation_np[0].transpose()  # (1, 1, 3000) -> (3000, 1)\n        else:\n            relevance_map = explanation_np.reshape(-1, 1)\n        \n        # Expand to 12 leads for visualization\n        if relevance_map.shape[1] == 1 and ecg_data.shape[1] == 12:\n            relevance_map = np.tile(relevance_map, (1, 12))\n        \n        # Normalize\n        normalized_relevance = normalize_ecg_relevancemap(relevance_map)\n        \n        # Format for visualization\n        ecg_for_visual = ecg_data.transpose()\n        expl_for_visual = normalized_relevance.transpose()\n        \n        # Create 12-lead visualization\n        plot_ecg(\n            ecg=ecg_for_visual,\n            explanation=expl_for_visual,\n            sampling_rate=500,\n            title=f\"PyTorch XAI: gradient_x_input on {record_id}\",\n            show_colorbar=True,\n            cmap='seismic',\n            bubble_size=30,\n            line_width=1.0,\n            style='fancy',\n            save_to=None,\n            clim_min=-1,\n            clim_max=1,\n            colorbar_label='Relevance',\n            shape_switch=False\n        )\n        \n        print(\"\\n‚úÖ 12-lead ECG visualization created!\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Could not create 12-lead visualization: {e}\")\n    print(\"This requires the complete repository with ECG data and utilities.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. 12-Lead ECG Visualization (if ECG utilities available)\n\nUsing the utilities from the quickstart files to create professional ECG visualizations.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Method Comparison Analysis\n\nLet's compare attribution across different methods for ECG components.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare attribution across methods\nmethods_to_compare = ['gradient', 'gradient_x_input', 'lrp_z', 'lrpsign_z']\ncomponents = ['P-wave', 'QRS Complex', 'T-wave']\nregions = [p_wave_region, qrs_complex_region, t_wave_region]\n\n# Calculate attribution for each method and component\ncomponent_attribution = {}\nfor method in methods_to_compare:\n    if method in explanations:\n        if isinstance(explanations[method], torch.Tensor):\n            expl = explanations[method].detach().cpu().numpy()[0, 0, :]\n        else:\n            expl = explanations[method][0, 0, :]\n        component_attribution[method] = [np.mean(np.abs(expl[region])) for region in regions]\n    else:\n        component_attribution[method] = [0, 0, 0]\n\n# Visualize component attribution comparison\nplt.figure(figsize=(12, 6))\n\nx = np.arange(len(components))\nwidth = 0.2\noffsets = np.linspace(-0.3, 0.3, len(methods_to_compare))\n\nfor i, method in enumerate(methods_to_compare):\n    plt.bar(x + offsets[i], component_attribution[method], width, label=method)\n\nplt.xlabel('ECG Component')\nplt.ylabel('Mean Absolute Attribution')\nplt.title('Attribution Comparison Across Methods')\nplt.xticks(x, components)\nplt.legend()\nplt.grid(True, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìä Method Comparison Summary:\")\nfor method in methods_to_compare:\n    if method in component_attribution:\n        attrs = component_attribution[method]\n        print(f\"  {method:20} - P: {attrs[0]:.3f}, QRS: {attrs[1]:.3f}, T: {attrs[2]:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Summary and Key Insights\n\n### What we've learned:\n\n1. **PyTorch Time Series Models**: How to build and train CNN models for ECG classification\n2. **Multiple XAI Methods**: Different approaches provide different insights:\n   - **Gradient**: Shows instantaneous importance\n   - **Gradient √ó Input**: Emphasizes strong signal regions  \n   - **Integrated Gradients**: Provides theoretically grounded attributions\n   - **Grad-CAM**: Adapts convolutional attention for time series\n   - **LRP methods**: Show layer-wise relevance propagation\n   - **LRP-SIGN**: The SIGN method for enhanced attribution\n\n3. **Visualization Techniques**: \n   - Separate plots for clear comparison\n   - Overlay visualizations for intuitive understanding\n   - Component-specific analysis for medical insights\n   - 12-lead medical visualizations for clinical applications\n\n4. **Time Series XAI**: Understanding temporal dependencies and pattern recognition in sequential data\n\n### Clinical Insights:\n- Most methods correctly highlight QRS complexes, which is clinically appropriate\n- Different methods show varying sensitivity to P-waves and T-waves\n- Component-specific analysis helps validate model focus against medical knowledge\n\n### Next Steps:\n- Try different ECG records to see consistency across samples\n- Experiment with different model architectures\n- Compare with TensorFlow implementation\n- Apply to your own time series classification problems\n\n### Method Selection Guide:\n- **`gradient`**: Fast, good for real-time analysis\n- **`gradient_x_input`**: Better for highlighting strong features\n- **`integrated_gradients`**: Most theoretically sound, but slower\n- **`grad_cam`**: Good for understanding convolutional focus\n- **`lrp_*`**: Good for layer-wise understanding\n- **`lrpsign_z`**: Enhanced attribution with sign information\n\nThis tutorial demonstrates how SignXAI2 can provide valuable insights into time series model decisions, particularly important in medical applications where understanding AI reasoning is critical for clinical adoption.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}