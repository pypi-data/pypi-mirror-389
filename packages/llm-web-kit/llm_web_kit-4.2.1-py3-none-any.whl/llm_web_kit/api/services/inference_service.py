# vLLM ä½œä¸ºå¯é€‰ä¾èµ–ï¼šå¯¼å…¥å¤±è´¥æ—¶ä¿æŒæ¨¡å—å¯ç”¨ï¼Œå®é™…ä½¿ç”¨æ—¶å†æŠ¥é”™
import json
import os
import re
import time
from dataclasses import dataclass
from enum import Enum
from typing import List

import torch
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

from llm_web_kit.config.cfg_reader import load_config

from ..dependencies import get_logger

logger = get_logger(__name__)


@dataclass
class InferenceConfig:
    model_path: str = ''
    data_path: str = ''
    output_path: str = ''
    use_logits_processor: bool = True
    num_workers: int = 8
    max_tokens: int = 32768
    temperature: float = 0
    top_p: float = 0.95
    max_output_tokens: int = 8192
    tensor_parallel_size: int = 1
    # æµ‹è¯•ç¯å¢ƒä¿®æ”¹ä¸ºfloat16
    dtype: str = 'bfloat16'
    template: bool = True


config = InferenceConfig(
    model_path='',  # checkpoint-3296è·¯å¾„
    output_path='',
    use_logits_processor=True,  # å¯ç”¨é€»è¾‘å¤„ç†å™¨ç¡®ä¿JSONæ ¼å¼è¾“å‡º
    num_workers=8,  # å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
    max_tokens=26000,  # æœ€å¤§è¾“å…¥tokenæ•°
    temperature=0,  # ç¡®å®šæ€§è¾“å‡º
    top_p=0.95,
    max_output_tokens=8192,  # æœ€å¤§è¾“å‡ºtokenæ•°
    tensor_parallel_size=1,  # å¼ é‡å¹¶è¡Œå¤§å°
    template=True  # å¯ç”¨èŠå¤©æ¨¡æ¿
)

PROMPT = """As a front-end engineering expert in HTML, your task is to analyze the given HTML structure and accurately classify elements with the _item_id attribute as either "main" (primary content) or "other" (supplementary content). Your goal is to precisely extract the primary content of the page, ensuring that only the most relevant information is labeled as "main" while excluding navigation, metadata, and other non-essential elements.
Guidelines for Classification:
Primary Content ("main")
Elements that constitute the core content of the page should be classified as "main". These typically include:
âœ… For Articles, News, and Blogs:
The main text body of the article, blog post, or news content.
Images embedded within the main content that contribute to the article.
âœ… For Forums & Discussion Threads:
The original post in the thread.
Replies and discussions that are part of the main conversation.
âœ… For Q&A Websites:
The question itself posted by a user.
Answers to the question and replies to answers that contribute to the discussion.
âœ… For Other Content-Based Pages:
Any rich text, paragraphs, or media that serve as the primary focus of the page.
Supplementary Content ("other")
Elements that do not contribute to the primary content but serve as navigation, metadata, or supporting information should be classified as "other". These include:
âŒ Navigation & UI Elements:
Menus, sidebars, footers, breadcrumbs, and pagination links.
"Skip to content" links and accessibility-related text.
âŒ Metadata & User Information:
Article titles, author names, timestamps, and view counts.
Like counts, vote counts, and other engagement metrics.
âŒ Advertisements & Promotional Content:
Any section labeled as "Advertisement" or "Sponsored".
Social media sharing buttons, follow prompts, and external links.
âŒ Related & Suggested Content:
"Read More", "Next Article", "Trending Topics", and similar sections.
Lists of related articles, tags, and additional recommendations.
Task Instructions:
You will be provided with a simplified HTML structure containing elements with an _item_id attribute. Your job is to analyze each element's function and determine whether it should be classified as "main" or "other".
Response Format:
Return a JSON object where each key is the _item_id value, and the corresponding value is either "main" or "other", as in the following example:
{{"1": "other","2": "main","3": "other"}}
ğŸš¨ Important Notes:
Do not include any explanations in the outputâ€”only return the JSON.
Ensure high accuracy by carefully distinguishing between primary content and supplementary content.
Err on the side of cautionâ€”if an element seems uncertain, classify it as "other" unless it clearly belongs to the main content.

Input HTML:
{alg_html}

Output format should be a JSON-formatted string representing a dictionary where keys are item_id strings and values are either 'main' or 'other'. Make sure to include ALL item_ids from the input HTML.
"""


def create_prompt(alg_html: str) -> str:
    return PROMPT.format(alg_html=alg_html)


def add_template(prompt: str, tokenizer: AutoTokenizer) -> str:
    messages = [
        {'role': 'user', 'content': prompt}
    ]
    chat_prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=True  # Switches between thinking and non-thinking modes. Default is True.
    )
    return chat_prompt


class State(Enum):
    Left_bracket = 0
    Right_bracket = 1
    Space_quote = 2
    Quote_colon_quote = 3
    Quote_comma = 4
    Main_other = 5
    Number = 6


class Token_state:
    def __init__(self, model_path):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        token_id_map = {
            State.Left_bracket: ['{'],
            State.Right_bracket: ['}'],
            State.Space_quote: [' "'],
            State.Quote_colon_quote: ['":"'],
            State.Quote_comma: ['",'],
            State.Main_other: ['main', 'other'],
            State.Number: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
        }
        self.token_id_map = {k: [self.tokenizer.encode(v)[0] for v in token_id_map[k]] for k in token_id_map}

    def mask_other_logits(self, logits: torch.Tensor, remained_ids: List[int]):
        remained_logits = {ids: logits[ids].item() for ids in remained_ids}
        new_logits = torch.ones_like(logits) * -float('inf')
        for id in remained_ids:
            new_logits[id] = remained_logits[id]
        return new_logits

    def calc_max_count(self, prompt_token_ids: List[int]):
        pattern_list = [716, 1203, 842, 428]
        for idx in range(len(prompt_token_ids) - len(pattern_list), -1, -1):
            if all(prompt_token_ids[idx + i] == pattern_list[i] for i in range(len(pattern_list))):
                num_idx = idx + len(pattern_list)
                num_ids = []
                while num_idx < len(prompt_token_ids) and prompt_token_ids[num_idx] in self.token_id_map[State.Number]:
                    num_ids.append(prompt_token_ids[num_idx])
                    num_idx += 1
                # return int(self.tokenizer.decode(num_ids)) + 1
                return int(self.tokenizer.decode(num_ids))
        return 1

    def find_last_complete_number(self, input_ids: List[int]):
        if not input_ids:
            return -1, 'null', -1

        tail_number_ids = []
        last_idx = len(input_ids) - 1
        while last_idx >= 0 and input_ids[last_idx] in self.token_id_map[State.Number]:
            tail_number_ids.insert(0, input_ids[last_idx])
            last_idx -= 1

        tail_number = int(self.tokenizer.decode(tail_number_ids)) if tail_number_ids else -1

        while last_idx >= 0 and input_ids[last_idx] not in self.token_id_map[State.Number]:
            last_idx -= 1

        if last_idx < 0:
            return tail_number, 'tail', tail_number

        last_number_ids = []
        while last_idx >= 0 and input_ids[last_idx] in self.token_id_map[State.Number]:
            last_number_ids.insert(0, input_ids[last_idx])
            last_idx -= 1

        last_number = int(self.tokenizer.decode(last_number_ids))

        if tail_number == last_number + 1:
            return tail_number, 'tail', tail_number
        return last_number, 'non_tail', tail_number

    def process_logit(self, prompt_token_ids: List[int], input_ids: List[int], logits: torch.Tensor):
        if not input_ids:
            return self.mask_other_logits(logits, self.token_id_map[State.Left_bracket])

        last_token = input_ids[-1]

        if last_token == self.token_id_map[State.Right_bracket][0]:
            return self.mask_other_logits(logits, [151645])
        elif last_token == self.token_id_map[State.Left_bracket][0]:
            return self.mask_other_logits(logits, self.token_id_map[State.Space_quote])
        elif last_token == self.token_id_map[State.Space_quote][0]:
            last_number, _, _ = self.find_last_complete_number(input_ids)
            # next_char = str(last_number + 1)[0]
            if last_number == -1:
                next_char = '1'
            else:
                next_char = str(last_number + 1)[0]

            return self.mask_other_logits(logits, self.tokenizer.encode(next_char))
        elif last_token in self.token_id_map[State.Number]:
            last_number, state, tail_number = self.find_last_complete_number(input_ids)
            if state == 'tail':
                return self.mask_other_logits(logits, self.token_id_map[State.Quote_colon_quote])
            else:
                next_str = str(last_number + 1)
                next_char = next_str[len(str(tail_number))]
                return self.mask_other_logits(logits, self.tokenizer.encode(next_char))
        elif last_token == self.token_id_map[State.Quote_colon_quote][0]:
            return self.mask_other_logits(logits, self.token_id_map[State.Main_other])
        elif last_token in self.token_id_map[State.Main_other]:
            return self.mask_other_logits(logits, self.token_id_map[State.Quote_comma])
        elif last_token == self.token_id_map[State.Quote_comma][0]:
            last_number, _, _ = self.find_last_complete_number(input_ids)
            max_count = self.calc_max_count(prompt_token_ids)
            if last_number >= max_count:
                return self.mask_other_logits(logits, self.token_id_map[State.Right_bracket])
            else:
                return self.mask_other_logits(logits, self.token_id_map[State.Space_quote])

        return logits


def reformat_map(text):
    try:
        data = json.loads(text)
        return {'item_id ' + k: 1 if v == 'main' else 0 for k, v in data.items()}
    except json.JSONDecodeError:
        return {}


def main(simplified_html: str, model: object, tokenizer: object, model_path: str):
    # tokenizer = AutoTokenizer.from_pretrained("/share/liukaiwen/models/qwen3-0.6b/checkpoint-3296", trust_remote_code=True)
    # simplified_html = simplify_html(ori_html)
    # print("sim_html length", len(simplified_html))
    if SamplingParams is None:
        raise RuntimeError(
            'å½“å‰ç¯å¢ƒæœªå®‰è£… vLLM æˆ–å®‰è£…å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œæ¨¡å‹æ¨ç†ã€‚å»ºè®®åœ¨ Linux+NVIDIA GPU ç¯å¢ƒå®‰è£… vLLMï¼Œ' +
            'æˆ–åœ¨ API ä¸­ä½¿ç”¨å ä½/æ›¿ä»£æ¨ç†å®ç°ã€‚åŸå§‹å¯¼å…¥é”™è¯¯: {}'.format('_VLLM_IMPORT_ERROR')
        )
    prompt = create_prompt(simplified_html)
    chat_prompt = add_template(prompt, tokenizer)

    if config.use_logits_processor:
        token_state = Token_state(model_path)
        sampling_params = SamplingParams(
            temperature=config.temperature,
            top_p=config.top_p,
            max_tokens=config.max_output_tokens,
            logits_processors=[token_state.process_logit]
        )
    else:
        sampling_params = SamplingParams(
            temperature=config.temperature,
            top_p=config.top_p,
            max_tokens=config.max_output_tokens
        )

    output = model.generate(chat_prompt, sampling_params)
    output_json = clean_output(output)
    return output_json


def clean_output(output):
    prediction = output[0].outputs[0].text

    # Extract JSON from prediction
    start_idx = prediction.rfind('{')
    end_idx = prediction.rfind('}') + 1

    if start_idx != -1 and end_idx != -1:
        json_str = prediction[start_idx:end_idx]
        json_str = re.sub(r',\s*}', '}', json_str)  # Clean JSON
        try:
            json.loads(json_str)  # Validate
        except Exception:
            json_str = '{}'
    else:
        json_str = '{}'

    return json_str


class InferenceService:
    """å¯¹å¤–æš´éœ²çš„æ¨ç†æœåŠ¡å°è£…ï¼Œä¾› HTMLService è°ƒç”¨ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–æ¨ç†æœåŠ¡ï¼Œå»¶è¿ŸåŠ è½½æ¨¡å‹."""
        self._llm = None
        self._tokenizer = None
        self._initialized = False
        self._init_lock = None  # ç”¨äºå¼‚æ­¥åˆå§‹åŒ–é”
        self._model_path = None

    async def warmup(self):
        """åœ¨æœåŠ¡å¯åŠ¨é˜¶æ®µä¸»åŠ¨é¢„çƒ­æ¨¡å‹ï¼ˆå¼‚æ­¥åˆå§‹åŒ–ï¼‰ã€‚"""
        await self._ensure_initialized()

    async def _ensure_initialized(self):
        """ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆå¼‚æ­¥å®‰å…¨ï¼‰"""
        if not self._initialized:
            if self._init_lock is None:
                import asyncio
                self._init_lock = asyncio.Lock()

            async with self._init_lock:
                if not self._initialized:  # åŒé‡æ£€æŸ¥
                    await self._init_model()
                    self._initialized = True

    async def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer."""
        try:
            llm_config = load_config(suppress_error=True)
            self.model_path = os.environ['MODEL_PATH'] if 'MODEL_PATH' in os.environ else llm_config.get('model_path',
                                                                                                         None)
            if self.model_path is None:
                raise RuntimeError('model_pathä¸ºç©ºï¼Œæœªé…ç½®æ¨¡å‹è·¯å¾„')
            if SamplingParams is None:
                raise RuntimeError(
                    'å½“å‰ç¯å¢ƒæœªå®‰è£… vLLM æˆ–å®‰è£…å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œæ¨¡å‹æ¨ç†ã€‚å»ºè®®åœ¨ Linux+NVIDIA GPU ç¯å¢ƒå®‰è£… vLLMï¼Œ' +
                    'æˆ–åœ¨ API ä¸­ä½¿ç”¨å ä½/æ›¿ä»£æ¨ç†å®ç°ã€‚åŸå§‹å¯¼å…¥é”™è¯¯: {}'.format('_VLLM_IMPORT_ERROR')
                )

            # åˆå§‹åŒ– tokenizer
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )

            # åˆå§‹åŒ– LLM æ¨¡å‹
            self._llm = LLM(
                model=self.model_path,
                trust_remote_code=True,
                dtype=config.dtype,
                tensor_parallel_size=config.tensor_parallel_size,
                # æµ‹è¯•ç¯å¢ƒå–æ¶ˆæ³¨é‡Š
                # max_model_len=config.max_tokens,  # å‡å°‘åºåˆ—é•¿åº¦é¿å…å†…å­˜ä¸è¶³
            )

            logger.info(f'æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.model_path}')

        except Exception as e:
            logger.error(f'æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}')
            # å¦‚æœæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œä¿æŒä¸º Noneï¼Œåç»­è°ƒç”¨ä¼šè¿”å›å ä½ç»“æœ
            self._llm = None
            self._tokenizer = None

    async def inference(self, simplified_html: str, options: dict | None = None) -> dict:
        """æ‰§è¡Œæ¨ç†ï¼Œå¦‚æœæ¨¡å‹æœªåˆå§‹åŒ–åˆ™è¿”å›å ä½ç»“æœ."""
        try:
            await self._ensure_initialized()

            if self._llm is None or self._tokenizer is None:
                logger.error('æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¿”å›å ä½ç»“æœ')
                return self._get_placeholder_result()

            # æ‰§è¡ŒçœŸå®æ¨ç†
            return await self._run_real_inference(simplified_html, options)

        except Exception as e:
            logger.error(f'æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}')
            return self._get_placeholder_result()

    async def _run_real_inference(self, simplified_html: str, options: dict | None = None) -> dict:
        """æ‰§è¡ŒçœŸå®çš„æ¨¡å‹æ¨ç†."""
        try:
            # åˆ›å»º prompt
            prompt = create_prompt(simplified_html)
            chat_prompt = add_template(prompt, self._tokenizer)

            # è®¾ç½®é‡‡æ ·å‚æ•°
            if config.use_logits_processor:
                token_state = Token_state(self.model_path)
                sampling_params = SamplingParams(
                    temperature=config.temperature,
                    top_p=config.top_p,
                    max_tokens=config.max_output_tokens,
                    logits_processors=[token_state.process_logit]
                )
            else:
                sampling_params = SamplingParams(
                    temperature=config.temperature,
                    top_p=config.top_p,
                    max_tokens=config.max_output_tokens
                )

            # æ‰§è¡Œæ¨ç†
            start_time = time.time()
            output = self._llm.generate(chat_prompt, sampling_params)
            end_time = time.time()
            output_json = clean_output(output)

            # æ ¼å¼åŒ–ç»“æœ
            result = reformat_map(output_json)
            logger.info(f'æ¨ç†å®Œæˆï¼Œç»“æœ: {result}, è€—æ—¶: {end_time - start_time}ç§’')
            return result

        except Exception as e:
            logger.error(f'çœŸå®æ¨ç†å¤±è´¥: {e}')
            return self._get_placeholder_result()

    def _get_placeholder_result(self) -> dict:
        """è¿”å›å ä½ç»“æœ."""
        return {}


if __name__ == '__main__':
    config = InferenceConfig(
        model_path='',
        output_path='',
        use_logits_processor=True,
        num_workers=8,
        max_tokens=26000,
        temperature=0,
        top_p=0.95,
        max_output_tokens=8192,
        tensor_parallel_size=1,
        template=True,
    )
    try:
        llm_config = load_config(suppress_error=True)
        model_path = llm_config.get('model_path', None)
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = LLM(model=model_path,
                    trust_remote_code=True,
                    dtype=config.dtype,
                    # è®¾ç½®æœ€å¤§æ¨¡å‹é•¿åº¦
                    max_model_len=config.max_tokens,
                    tensor_parallel_size=config.tensor_parallel_size)

        simplified_html = '<html><body><h1>Hello World</h1></body></html>'
        response_json = main(simplified_html, model, tokenizer)
        llm_response_dict = reformat_map(response_json)
    except Exception:
        raise
    finally:
        import torch.distributed as dist

        # åœ¨ç¨‹åºç»“æŸå‰æ·»åŠ 
        if dist.is_initialized():
            dist.destroy_process_group()
