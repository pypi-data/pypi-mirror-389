{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß† Building & Evaluating Complex Agents with `crewai` and `flotorch-eval`\n",
        "\n",
        "In this notebook, we'll walk through a complete example of evaluating agents using the **`flotorch-eval`** package across key metrics. These metrics help assess both agent quality and system performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Evaluation Metrics\n",
        "\n",
        "- **`AgentGoalAccuracy`**  \n",
        "  Evaluates whether the agent successfully understood and achieved the user's goal.  \n",
        "  - **Binary** (1 = goal achieved, 0 = not achieved)\n",
        "\n",
        "- **`ToolCallAccuracy`**  \n",
        "  Measures the correctness of tool usage by the agent‚Äîi.e., whether the agent called the right tools to complete a task.  \n",
        "  - **Binary** (1 = relevant tools invoked, 0 = relevant tools not invoked)\n",
        "\n",
        "- **`TrajectoryEvalWithLLM`**  \n",
        "  Evaluates whether the trajectory (based on OpenTelemetry spans) is meaningful based on the steps taken by the agent.  \n",
        "  - **Binary** (1 = meaningful, 0 = invalid)\n",
        "\n",
        "- **`TrajectoryEvalWithLLMWithReference`**\n",
        "  Evaluates whether the trajectory is meaningful by comparing it with a reference trajectory\n",
        "  - **Binary** (1 = meaningful, 0 = invalid)\n",
        "\n",
        "\n",
        "- **`LatencyMetric`**  \n",
        "  Measures agent latency‚Äîhow fast the agent responds or completes tasks.  \n",
        "  \n",
        "\n",
        "- **`UsageMetric`**  \n",
        "  Evaluates the cost-efficiency of the agent in terms of compute, tokens, or other usage dimensions.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install numpy pandas langchain-aws -q\n",
        "!pip install flotorch-eval crewai crewai-tools ddgs uv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "# Import required libraries\n",
        "from typing import Dict, Any\n",
        "\n",
        "from crewai.tools import tool\n",
        "from crewai import Crew\n",
        "from ddgs import DDGS\n",
        "\n",
        "\n",
        "from flotorch_eval.agent_eval.core.client import FlotorchEvalClient\n",
        "from flotorch.crewai.agent import FlotorchCrewAIAgent\n",
        "from evaluation_utils import display_evaluation_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Configure Flotorch Credentials\n",
        "\n",
        "Set up the Flotorch base URL and API key here to enable tracing and evaluation by connecting to Flotorch.\n",
        "These credentials allow you to:\n",
        "\n",
        "Access configured agents from the Flotorch console\n",
        "\n",
        "Enable tracing for your runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FLOTORCH_GATEWAY_BASE_URL = \"\"\n",
        "FLOTORCH_API_KEY= \"\"\n",
        "evaluation_llm_model = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Evaluation Logic\n",
        "This helper function manages the evaluation workflow.\n",
        "It accepts the trace ID generated by the agent run and an optional reference trace, which can be provided either as a JSON object or as the ID of a previously created trace to use as a reference.\n",
        "\n",
        "The function creates a client to retrieve the traces and evaluates them across different metrics.\n",
        "By default, the trace is evaluated on all available metrics, but you can customize the evaluation by passing a list of specific metrics to the metrics parameter of the evaluate method.\n",
        "**Example:**  \n",
        "\n",
        "```python\n",
        "metrics = [TrajectoryEvalWithLLM()]\n",
        "client.evaluate(trace_id, metrics)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Reference Trajectory\n",
        "\n",
        "A Reference Trajectory defines the ideal \"golden path\" for an agent's behavior. It outlines not just what the agent should do, but also why it does it, step by step. This structure is used to evaluate if the agent is reasoning and acting correctly.\n",
        "You can create the reference on your own based on how your agent is supposed to work:\n",
        "The structure of a reference trajectory consists of a main object with two key fields:\n",
        "\n",
        "**`input`**: This is the initial prompt or question from the user that kicks off the entire process.\n",
        "\n",
        "**`expected_steps`**: This is an ordered list where each item represents a single step in the agent's thought process and the resulting action.\n",
        "\n",
        "Each step in the \"expected_steps\" list is an object that must contain two parts: a thought and an action.\n",
        "\n",
        "**`thought`**: A string representing the agent's internal reasoning or \"inner monologue.\" This explains why the agent is about to take a specific action.\n",
        "\n",
        "**`action`**: Each step must conclude with exactly one action. The action can be one of two types:\n",
        "\n",
        "A. **tool_call**: The agent decides to use a tool. This object has two parts:\n",
        "\n",
        "**`name`**: The name of the function or tool to be executed.\n",
        "\n",
        "**`arguments`**: A dictionary of the parameters to pass to that tool.\n",
        "\n",
        "B. **final_response**: The agent decides it has enough information to answer the user. This is a string containing the agent's final text reply, which typically occurs as the very last step.\n",
        "\n",
        "**Sample Trajectory**\n",
        "```json\n",
        "{\n",
        "  \"input\": \"What's the weather like in Mumbai?\",\n",
        "  \"expected_steps\": [\n",
        "    {\n",
        "      \"thought\": \"The user is asking for the weather in a specific city. I should use the `get_weather` tool to find this information.\",\n",
        "      \"tool_call\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"arguments\": {\n",
        "          \"city\": \"Mumbai\"\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"thought\": \"I have successfully retrieved the weather information. Now I will formulate the final answer for the user.\",\n",
        "      \"final_response\": \"The weather in Mumbai is currently warm and humid with a temperature of 31¬∞C.\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def evaluate_agent(trace_id: str, reference: Dict[str, Any]=None, reference_id: str=None):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    if reference and reference_id:\n",
        "        raise ValueError(\"Provide either 'reference' or 'reference_trace_id', not both.\")\n",
        "\n",
        "    client = FlotorchEvalClient(\n",
        "        api_key=FLOTORCH_API_KEY,\n",
        "        base_url=FLOTORCH_GATEWAY_BASE_URL,\n",
        "        default_evaluator=evaluation_llm_model # Setting a default evaluator for all metrics that require an LLM.\n",
        "        )\n",
        "\n",
        "    traces = client.fetch_traces(trace_id)\n",
        "    print(f\"Traces: {traces}\")\n",
        "    results = await client.evaluate( # Metrics can be optionally provided as a list\n",
        "        trace=traces,\n",
        "        reference=reference,\n",
        "        reference_trace_id=reference_id\n",
        "    )\n",
        "    \n",
        "    display_evaluation_results(results)\n",
        "    print(results.model_dump_json(indent=4))\n",
        "    end_time = time.time()\n",
        "    time_taken = round(end_time - start_time, 2)\n",
        "    print(f\"Time taken for evaluation: {time_taken} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Using Evaluators for Metrics\n",
        "\n",
        "If a **`default_evaluator`** is set, that model will be used as the LLM judge for all metrics that require an LLM.\n",
        "\n",
        "To evaluate a specific metric with a different model, define the metric individually and specify the desired model using the llm parameter.\n",
        "\n",
        "The model must be one that is configured in the Flotorch console.\n",
        "\n",
        "This will override the default_evaluator only for that metric.\n",
        "\n",
        "Notes:\n",
        "\n",
        "If you provide a metric separately, only that metric will be evaluated.\n",
        "\n",
        "If no default_evaluator is set, you must provide an evaluator for every metric that requires one.\n",
        "\n",
        "```python\n",
        "metrics = [\n",
        "    TrajectoryEvalWithLLM(\n",
        "        llm=\"flotorch/gpt-4o\"\n",
        "    )\n",
        "    ]\n",
        "client.evaluate(trace_id, metrics)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Use Case: AWS Tech Agent**\n",
        "\n",
        "Now, let‚Äôs build, run, and evaluate our agent.\n",
        "\n",
        "#### Build the Agent\n",
        "\n",
        "Flotorch makes it simple to create agents that can be reused across different frameworks.\n",
        "To set up an agent:\n",
        "\n",
        "Go to the **`Flotorch console`** ‚Üí **`Agent Builder`** ‚Üí click **`Create Flotorch Agent`**.\n",
        "\n",
        "Provide a name for your agent.\n",
        "\n",
        "Configure the agent with:\n",
        "\n",
        "- The model you want the agent to use\n",
        "\n",
        "- Input and output structures\n",
        "\n",
        "- Any MCP tools you want to integrate\n",
        "\n",
        "- Agent details such as the system prompt and agent goal\n",
        "\n",
        "Once configured, use the **same agent name here** to set up the **Flotorch CrewAI agent** and run it as a crew.\n",
        "\n",
        "You can also define custom tools in addition to the MCP tools you may have already configured.\n",
        "\n",
        "**`Tools`**:\n",
        "\n",
        "DuckDuckGoSearch ‚Äì to search and retrieve information\n",
        "\n",
        "Salesforce API ‚Äì to fetch Salesforce data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the tools that will be used by the agents\n",
        "@tool('DuckDuckGoSearch')\n",
        "def search_tool(search_query: str):\n",
        "    \"\"\"Search the web for information on a given topic\"\"\"\n",
        "    return DDGS().text(search_query, max_results=5)\n",
        "\n",
        "@tool('SalesforceIntegration')\n",
        "def salesforce_tool(soql_query: str):\n",
        "    \"\"\"Call Salesforce API to get data\"\"\"\n",
        "    return \"Salesforce Integration\"\n",
        "\n",
        "agent_name = \"aws-tech-agent\" # Name of the agent that you have set on the Flotorch console\n",
        "agent_client = FlotorchCrewAIAgent(\n",
        "    agent_name =agent_name,\n",
        "    api_key=FLOTORCH_API_KEY, # Flotorch API key and Base URL is optional here if you have set it in the environment variables\n",
        "    base_url=FLOTORCH_GATEWAY_BASE_URL,\n",
        "    custom_tools= [search_tool, salesforce_tool],\n",
        "    tracer_config={\n",
        "            \"enabled\": True, \n",
        "            \"sampling_rate\": 1\n",
        "        }\n",
        "    )\n",
        "\n",
        "agent = agent_client.get_agent()\n",
        "task = agent_client.get_task()\n",
        "crew = Crew(\n",
        "    agents = [agent],\n",
        "    tasks = [task],\n",
        "    verbose = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run the Agent\n",
        "Let's kickoff the crew to perform its task with a topic. Flotorch Tracing will automatically capture the entire execution in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"You must use the given tools to research and only then write about this topic\"\n",
        "result = crew.kickoff(inputs = {\"query\": query, \"topic\": \"AWS CodeWhisperer\"})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run the Evaluation\n",
        "Finally, we can use the generated trace ID to evaluate_agent function.\n",
        "\n",
        "We can create a reference trajectory for running the **`TrajectoryEvalWithLLMWithReference`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REFERENCE_TRAJECTORY_OUTPUTS = {\n",
        "    \"input\": \"What is AWS Bedrock?\",\n",
        "    \"expected_steps\": [\n",
        "        {\n",
        "            \"thought\": \"To answer this question about Amazon Bedrock, I first need to gather information about what Amazon Bedrock is. I will use the available tool to search for this information.\",\n",
        "            \"tool_call\": {\n",
        "                \"name\": \"Search the web for information on a given topic\",\n",
        "                \"arguments\": \"{\\\"search_query\\\": \\\"Amazon Bedrock\\\"}\"\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"thought\": \"Now that I have the search results, I will synthesize the information to provide a comprehensive answer.\",\n",
        "            \"final_response\": \"Based on the observation, I have learned that Amazon Bedrock is a fully managed service that makes it easy to use foundation models from third-party providers and Amazon. It allows users to build generative AI applications with a choice of foundation models from different AI companies, using a single API. Users can customize these models with their data, orchestrate multistep tasks, trace reasoning, and apply guardrails for responsible AI. Additionally, Amazon Bedrock enables the creation of generative AI workflows by connecting its features with other AWS services.\"\n",
        "        }\n",
        "    ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def main():\n",
        "    trace_ids = agent_client.get_tracer_ids() \n",
        "    for trace_id in trace_ids:\n",
        "        if trace_id:\n",
        "            print(f\"Evaluating trace id: {trace_id}\")\n",
        "            await evaluate_agent(\n",
        "                trace_id=trace_id,\n",
        "                reference=REFERENCE_TRAJECTORY_OUTPUTS\n",
        "            )\n",
        "\n",
        "await main()  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
