Metadata-Version: 2.4
Name: test-flashinfer
Version: 0.0.1
Summary: Fast Attention Algorithms for LLM Inference
Author-email: AMD <support@amd.com>
License: Apache-2.0
Requires-Python: >=3.8
Description-Content-Type: text/markdown

# flashinfer

Fast Attention Algorithms for LLM Inference

## Information

- **License**: Apache-2.0

## Installation

```bash
pip install flashinfer
```
