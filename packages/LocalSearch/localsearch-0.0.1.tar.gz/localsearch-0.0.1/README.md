# LocalSearch

**LocalSearch** is a lightweight, privacy-first **local semantic search engine** powered by embeddings and LLMs.
It lets you index your local files (like `.txt`, `.pdf`, `.html`) and query them using natural language - without sending your data anywhere.

---

## Features

* ğŸ” **Semantic Search** - uses vector embeddings for meaning-based file retrieval
* ğŸ§© **Modular Components** - plug in your own LLM, embedder, vector store, or metadata backend
* âš™ï¸ **Auto Re-Embedding** - detects modified files and re-embeds when needed
* ğŸ’¬ **Context-Aware Q&A** - LLM answers your questions using only local context
* ğŸŒ **FastAPI Web Interface** - browse and query files via a simple local web app
* âœ… **Tested & Typed** - full unit tests and type hints included

---

## Installation

You can install from PyPI:

``` bash
pip install localsearch
```

Or, if youâ€™re developing locally:

``` bash
git clone https://github.com/yourusername/LocalSearch.git
cd LocalSearch
pip install -e .
```

---

## Quick Start

``` python
from LocalSearch.backend.engine import SearchEngine
from your_llm import MyLocalLLM  # implement BaseLLM

# Initialize with your local directory and models
engine = SearchEngine(
    directory_path="/path/to/your/files",
    llm=MyLocalLLM(),
    reembed_policy="modified_only",  # or 'force', 'never'
)

# Run a semantic query
answer = engine.search("What does this project do?")
print(answer)
```

To serve the web interface:

``` python
engine.web(host="127.0.0.1", port=8000)
```

Then open your browser at `http://127.0.0.1:8000`.

---

## How It Works

1. **Embedding Generation**

   * `_generate_embeddings` scans files, extracts text, chunks them, and encodes using your chosen embedder.
2. **Metadata & Vector Storage**

   * Uses `JsonMetadataStore` and `FaissVectorStore` by default (both replaceable).
3. **Querying**

   * Queries are embedded, compared against the local FAISS index, and context is fed into the LLM.
4. **Answering**

   * LLM answers using only retrieved context chunks.

---

## Configuration

| Parameter         | Description                                | Default                         |
| ----------------- | ------------------------------------------ | ------------------------------- |
| `directory_path`  | Path to your data folder                   | *required*                      |
| `embedding_model` | Any model implementing `BaseEmbedder`      | `SentenceTransformerEmbedder()` |
| `metadata_store`  | Metadata persistence                       | `JsonMetadataStore`             |
| `vector_store`    | Vector storage                             | `FaissVectorStore`              |
| `extractor`       | Text extraction logic                      | `DefaultTextExtractor`          |
| `reembed_policy`  | `'force'`, `'modified_only'`, or `'never'` | `'modified_only'`               |
| `recursive`       | Scan subfolders                            | `True`                          |
| `verbose`         | Print logs                                 | `True`                          |


---

## Architecture Overview

```
LocalSearch/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ engine.py                     # Main SearchEngine class
â”‚   â”œâ”€â”€ _doc_loader/
â”‚   â”‚   â”œâ”€â”€ _embedding_processor.py   # Handles embedding and file updates
â”‚   â”‚   â””â”€â”€ _scan.py                  # Directory scanning utilities
â”‚   â”œâ”€â”€ embeddings/                   # BaseEmbedder + implementations
â”‚   â”œâ”€â”€ llms/                         # BaseLLM and custom LLMs
â”‚   â”œâ”€â”€ metadata_store/               # JsonMetadataStore + Base class
â”‚   â””â”€â”€ vector_store/                 # FaissVectorStore + Base class
â””â”€â”€ frontend/                         # For web based interface
```

---

## Extending LocalSearch

You can plug in your own components simply by subclassing the base interfaces:

* **Custom Embedder** â†’ subclass `BaseEmbedder`
* **Custom LLM** â†’ subclass `BaseLLM`
* **Custom Vector Store** â†’ subclass `BaseVectorStore`
* **Custom Metadata Store** â†’ subclass `BaseMetadataStore`

Then pass them to the `SearchEngine` constructor.

---

## Documentation

Docs are hosted on **Read the Docs**:
ğŸ‘‰ [https://localsearchpy.readthedocs.io](https://localsearchpy.readthedocs.io)

To build docs locally:

```bash
cd docs
pip install -r requirements.txt
make html
```

---

## Example Use Cases

* Personal knowledge base search
* Local document assistant for PDFs and notes
* Offline AI-powered research tool
* Privacy-friendly enterprise document retrieval

---

## License

MIT License Â© 2025 [Rick Sanchez]

---

## Contributing

Contributions are welcome!
Please open a pull request or file an issue if youâ€™d like to add features, improve performance, or fix bugs.

---

## A Final Note

> â€œLocalSearch keeps your data *yours*.
> You can use the power of semantic search and LLMs - completely offline.â€

