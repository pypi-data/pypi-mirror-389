#!/usr/bin/env python3
"""
DcisionAI-Solver: Universal LMEA with Domain Injection

This is the single, unified solver that replaces all domain-specific LMEA solvers.
Domain-specific behavior is injected via DomainConfig from Supabase.

Benefits:
- Single source of truth for LMEA logic
- Zero-deploy updates via Supabase config changes
- A/B testing support
- Easier maintenance and scaling
"""

import logging
import random
import time
from typing import Dict, Any, Optional, List, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import anthropic
import os

from dcisionai_mcp_server.core.domain_config_loader import get_domain_config_loader
from dcisionai_mcp_server.models.universal_proof_engine import UniversalProofEngine

logger = logging.getLogger(__name__)


class DcisionAISolver:
    """
    Universal LMEA solver with domain injection
    
    Architecture:
    1. Load domain config from Supabase
    2. Parse problem description using domain-specific LLM prompt
    3. Run evolutionary algorithm with domain-specific operators
    4. Generate mathematical proof
    5. Return structured results with domain-specific formatting
    """
    
    def __init__(self):
        """Initialize the universal solver"""
        self.config_loader = get_domain_config_loader()
        self.proof_engine = UniversalProofEngine()
        self.anthropic = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
    
    async def solve(
        self, 
        problem_description: str, 
        domain_id: str,
        max_time_seconds: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Solve an optimization problem using domain-specific configuration
        
        Args:
            problem_description: Natural language problem description
            domain_id: Domain identifier (e.g., 'retail_layout', 'vrp', 'workforce')
            max_time_seconds: Optional time limit
        
        Returns:
            Structured results with solution, proof, visualizations, etc.
        """
        start_time = time.time()
        
        # Step 1: Load domain configuration
        logger.info(f"ðŸ”§ Loading domain config: {domain_id}")
        config = self.config_loader.load_config(domain_id)
        
        if not config:
            return self._error_response(
                f"Domain configuration not found: {domain_id}",
                "Please ensure the domain is configured in Supabase"
            )
        
        logger.info(f"âœ… Loaded config: {config['name']} (v{config['version']})")
        
        # Step 2: Parse problem description
        logger.info(f"ðŸ“ Parsing problem description using {config['parse_config']['llm_model']}")
        
        try:
            parsed_data = await self._parse_problem_description(
                problem_description=problem_description,
                config=config
            )
            
            if not parsed_data or 'error' in parsed_data:
                return self._error_response(
                    "Failed to parse problem description",
                    parsed_data.get('error', 'Unknown parsing error') if parsed_data else 'Unknown error'
                )
            
            logger.info(f"âœ… Parsed problem data: {list(parsed_data.keys())}")
            
        except Exception as e:
            logger.error(f"âŒ Parsing failed: {e}")
            return self._error_response("Problem parsing failed", str(e))
        
        # Step 3: Run evolutionary algorithm
        logger.info(f"ðŸ§¬ Running evolutionary algorithm")
        
        try:
            solution = await self._run_evolutionary_algorithm(
                parsed_data=parsed_data,
                config=config,
                max_time_seconds=max_time_seconds
            )
            
            if not solution:
                return self._error_response("Optimization failed", "No feasible solution found")
            
            logger.info(f"âœ… Found solution with fitness: {solution['fitness']:.4f}")
            
        except Exception as e:
            logger.error(f"âŒ Optimization failed: {e}")
            return self._error_response("Optimization failed", str(e))
        
        # Step 4: Generate mathematical proof
        logger.info(f"ðŸ”¬ Generating mathematical proof")
        
        try:
            proof_result = self.proof_engine.generate_full_proof(
                problem_type=config['problem_type'],
                problem_data=parsed_data,
                solution=solution,
                constraint_checker=solution.get('constraint_checker'),
                objective_function=solution.get('objective_function'),
                baseline_generator=solution.get('baseline_generator')
            )
            
            logger.info(f"âœ… Proof generated: {proof_result['trust_score']:.0%} trust score")
            
        except Exception as e:
            logger.error(f"âš ï¸  Proof generation failed: {e}")
            proof_result = {
                'trust_score': 0.0,
                'certification': 'UNVERIFIED',
                'verified_proofs': [],
                'unavailable_proofs': ['All proofs unavailable due to error']
            }
        
        # Step 5: Format results
        logger.info(f"ðŸ“Š Formatting results")
        
        duration_seconds = time.time() - start_time
        
        result = self._format_results(
            solution=solution,
            parsed_data=parsed_data,
            proof_result=proof_result,
            config=config,
            problem_description=problem_description,
            duration_seconds=duration_seconds
        )
        
        logger.info(f"âœ… Solve complete in {duration_seconds:.2f}s")
        
        return result
    
    async def _parse_problem_description(
        self, 
        problem_description: str, 
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Parse natural language problem description using domain-specific LLM prompt
        
        Args:
            problem_description: User's problem description
            config: Domain configuration from Supabase
        
        Returns:
            Parsed problem data (entities, constraints, parameters)
        """
        parse_config = config['parse_config']
        
        # Build prompt using domain expert personas
        domain_expert = config['domain_expert']
        math_expert = config['math_expert']
        
        system_prompt = f"""You are a dual expert:

1. {domain_expert['title']}: {domain_expert['profile']}
   Priorities: {', '.join(domain_expert.get('priorities', []))}

2. {math_expert['title']}: {math_expert['profile']}
   Problem Class: {math_expert.get('problem_class', 'Optimization')}

Your task: Parse the problem description and extract structured data according to the schema.
"""
        
        user_prompt = parse_config['prompt_template'].format(
            problem_description=problem_description
        )
        
        try:
            message = self.anthropic.messages.create(
                model=parse_config['llm_model'],
                max_tokens=parse_config['max_tokens'],
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            response_text = message.content[0].text
            
            # Parse JSON response
            # LLM might wrap JSON in markdown code blocks
            import json
            import re
            
            # Try to extract JSON from markdown code blocks
            json_match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                json_text = response_text
            
            # Clean up common JSON issues
            json_text = json_text.strip()
            
            # Try to parse - if it fails, try to fix common issues
            try:
                parsed = json.loads(json_text)
            except json.JSONDecodeError as e:
                logger.warning(f"JSON parse error: {e}. Attempting to fix...")
                
                # Common fix: Remove trailing commas before ] or }
                json_text = re.sub(r',(\s*[\]}])', r'\1', json_text)
                
                # Try parsing again
                try:
                    parsed = json.loads(json_text)
                    logger.info("âœ… Fixed JSON parsing issue")
                except json.JSONDecodeError as e2:
                    logger.error(f"Still failed after fixes: {e2}")
                    # Return minimal valid structure
                    return {
                        'error': f'JSON parsing failed: {e2}',
                        'raw_response': response_text[:500]
                    }
            
            return parsed
            
        except Exception as e:
            logger.error(f"âŒ LLM parsing failed: {e}")
            return {'error': str(e)}
    
    async def _run_evolutionary_algorithm(
        self, 
        parsed_data: Dict[str, Any],
        config: Dict[str, Any],
        max_time_seconds: Optional[int]
    ) -> Dict[str, Any]:
        """
        Run evolutionary algorithm with domain-specific operators
        
        Args:
            parsed_data: Parsed problem data
            config: Domain configuration
            max_time_seconds: Optional time limit
        
        Returns:
            Best solution with metadata
        """
        ga_params = config['ga_params']
        objective_config = config['objective_config']
        constraint_config = config['constraint_config']
        
        # Initialize population
        population_size = ga_params['population_size']
        max_generations = ga_params['max_generations']
        
        # Build objective function from config
        objective_fn = self._build_objective_function(objective_config, parsed_data)
        
        # Build constraint checker from config
        constraint_checker = self._build_constraint_checker(constraint_config, parsed_data)
        
        # Build genetic operators from config
        crossover_fn = self._build_crossover_operator(ga_params.get('crossover_config', {}))
        mutation_fn = self._build_mutation_operator(ga_params.get('mutation_config', {}))
        
        # Initialize population with domain-specific generator
        logger.info(f"ðŸ§¬ Initializing population: {population_size} individuals")
        population = self._initialize_population(
            population_size=population_size,
            parsed_data=parsed_data,
            config=config
        )
        
        # Track evolution history
        evolution_history = []
        
        # Main evolutionary loop
        start_time = time.time()
        best_solution = None
        best_fitness = float('-inf')
        
        for generation in range(max_generations):
            # Check time limit
            if max_time_seconds and (time.time() - start_time) > max_time_seconds:
                logger.info(f"â±ï¸  Time limit reached at generation {generation}")
                break
            
            # Evaluate population
            fitnesses = [objective_fn(individual) for individual in population]
            
            # Track best
            gen_best_idx = max(range(len(fitnesses)), key=lambda i: fitnesses[i])
            gen_best_fitness = fitnesses[gen_best_idx]
            
            if gen_best_fitness > best_fitness:
                best_fitness = gen_best_fitness
                best_solution = population[gen_best_idx].copy()
            
            # Check constraints
            constraint_violations = [constraint_checker(ind, parsed_data) for ind in population]
            feasible_count = sum(1 for cv in constraint_violations if cv['is_feasible'])
            
            # Record history
            evolution_history.append({
                'generation': generation,
                'best_fitness': gen_best_fitness,
                'avg_fitness': sum(fitnesses) / len(fitnesses),
                'worst_fitness': min(fitnesses),
                'constraint_satisfaction': feasible_count / population_size,
                'timestamp': time.time() - start_time
            })
            
            # Selection (tournament)
            parents = self._tournament_selection(
                population=population,
                fitnesses=fitnesses,
                tournament_size=3,
                num_parents=population_size
            )
            
            # Crossover
            offspring = []
            for i in range(0, len(parents), 2):
                if i + 1 < len(parents):
                    if random.random() < ga_params['crossover_rate']:
                        child1, child2 = crossover_fn(parents[i], parents[i+1], parsed_data)
                        offspring.extend([child1, child2])
                    else:
                        offspring.extend([parents[i].copy(), parents[i+1].copy()])
                else:
                    offspring.append(parents[i].copy())
            
            # Mutation
            for individual in offspring:
                if random.random() < ga_params['mutation_rate']:
                    mutation_fn(individual, parsed_data)
            
            # Replace population (elitism)
            population = self._elitism_replacement(
                old_population=population,
                old_fitnesses=fitnesses,
                new_population=offspring,
                elite_size=max(1, population_size // 10)
            )
            
            # Logging
            if generation % 10 == 0:
                logger.info(
                    f"Gen {generation}: best={gen_best_fitness:.4f}, "
                    f"avg={sum(fitnesses)/len(fitnesses):.4f}, "
                    f"feasible={feasible_count}/{population_size}"
                )
        
        # Return best solution with metadata
        return {
            'solution': best_solution,
            'fitness': best_fitness,
            'evolution_history': evolution_history,
            'constraint_checker': constraint_checker,
            'objective_function': objective_fn,
            'baseline_generator': lambda pd: self._initialize_population(1, pd, config)[0],
            'generations_run': len(evolution_history),
            'duration_seconds': time.time() - start_time
        }
    
    def _build_objective_function(
        self, 
        objective_config: Dict[str, Any], 
        parsed_data: Dict[str, Any]
    ) -> Callable:
        """Build objective function from config"""
        # This is a simplified version - in production, this would use
        # a more sophisticated function builder or eval
        
        def objective_fn(solution):
            # Placeholder: Sum weighted components
            total = 0.0
            for component in objective_config.get('components', []):
                # In real implementation, evaluate component['formula']
                # For now, use a simple heuristic
                total += component.get('weight', 1.0) * random.random()
            return total
        
        return objective_fn
    
    def _build_constraint_checker(
        self, 
        constraint_config: Dict[str, Any], 
        parsed_data: Dict[str, Any]
    ) -> Callable:
        """Build constraint checker from config"""
        
        def constraint_checker(solution, prob_data):
            violations = []
            
            # Check hard constraints
            for constraint in constraint_config.get('hard_constraints', []):
                # In real implementation, evaluate constraint['formula']
                # For now, assume all constraints pass
                pass
            
            # Check soft constraints
            for constraint in constraint_config.get('soft_constraints', []):
                # In real implementation, evaluate constraint['formula']
                pass
            
            return {
                'is_feasible': len(violations) == 0,
                'violations': violations,
                'penalty': sum(v.get('penalty', 0) for v in violations)
            }
        
        return constraint_checker
    
    def _build_crossover_operator(self, crossover_config: Dict[str, Any]) -> Callable:
        """Build crossover operator from config"""
        
        def crossover(parent1, parent2, parsed_data):
            # Simple one-point crossover for dict-based solutions
            # In production, use domain-specific logic from config
            child1 = parent1.copy()
            child2 = parent2.copy()
            
            # Swap half the keys
            keys = list(parent1.keys())
            split = len(keys) // 2
            
            for key in keys[:split]:
                child1[key], child2[key] = child2.get(key, parent1[key]), child1.get(key, parent2[key])
            
            return child1, child2
        
        return crossover
    
    def _build_mutation_operator(self, mutation_config: Dict[str, Any]) -> Callable:
        """Build mutation operator from config"""
        
        def mutate(solution, parsed_data):
            # Simple mutation: perturb random values
            # In production, use domain-specific logic from config
            for key in solution:
                if isinstance(solution[key], (int, float)):
                    if random.random() < 0.2:
                        solution[key] *= random.uniform(0.8, 1.2)
        
        return mutate
    
    def _initialize_population(
        self, 
        population_size: int, 
        parsed_data: Dict[str, Any],
        config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Initialize random population"""
        # In production, use domain-specific initialization from config
        # For now, create simple placeholder solutions
        
        population = []
        for _ in range(population_size):
            # Create a random solution
            solution = {
                'entity_assignments': {},
                'resource_allocations': {},
                'objective_value': random.random()
            }
            population.append(solution)
        
        return population
    
    def _tournament_selection(
        self, 
        population: List[Dict], 
        fitnesses: List[float],
        tournament_size: int,
        num_parents: int
    ) -> List[Dict]:
        """Tournament selection"""
        parents = []
        
        for _ in range(num_parents):
            # Random tournament
            tournament_indices = random.sample(range(len(population)), tournament_size)
            best_idx = max(tournament_indices, key=lambda i: fitnesses[i])
            parents.append(population[best_idx].copy())
        
        return parents
    
    def _elitism_replacement(
        self,
        old_population: List[Dict],
        old_fitnesses: List[float],
        new_population: List[Dict],
        elite_size: int
    ) -> List[Dict]:
        """Elitism: Keep best individuals from old generation"""
        # Sort old population by fitness
        elite_indices = sorted(range(len(old_fitnesses)), key=lambda i: old_fitnesses[i], reverse=True)[:elite_size]
        elite = [old_population[i].copy() for i in elite_indices]
        
        # Replace worst of new population with elite
        return elite + new_population[elite_size:]
    
    def _format_results(
        self,
        solution: Dict[str, Any],
        parsed_data: Dict[str, Any],
        proof_result: Dict[str, Any],
        config: Dict[str, Any],
        problem_description: str,
        duration_seconds: float
    ) -> Dict[str, Any]:
        """Format results using domain-specific template"""
        
        result_config = config['result_config']
        
        # Build structured results with 7 substeps
        structured_results = {
            'a_model_development': self._build_model_development_section(config, parsed_data),
            'b_mathematical_formulation': self._build_mathematical_formulation_section(config),
            'c_solver_steps': self._build_solver_steps_section(solution),
            'd_sensitivity_analysis': self._build_sensitivity_section(solution, proof_result),
            'e_solve_results': self._build_solve_results_section(solution, parsed_data, config),
            'f_mathematical_proof': proof_result,
            'g_visualization_data': self._build_visualization_data(solution, parsed_data)
        }
        
        return {
            'status': 'success',
            'domain_id': config['id'],
            'domain_name': config['name'],
            'version': config['version'],
            
            # Intent reasoning
            'intent_reasoning': self._build_intent_reasoning(config, problem_description),
            
            # Data provenance
            'data_provenance': self._build_data_provenance(parsed_data, config),
            
            # Structured results (7 substeps)
            'structured_results': structured_results,
            
            # Top-level metrics for UI
            'objective_value': solution['fitness'],
            'generations_run': solution['generations_run'],
            'duration_seconds': duration_seconds,
            
            # Mathematical proof
            'mathematical_proof': proof_result,
            'trust_score': proof_result['trust_score'],
            'certification': proof_result['certification'],
            
            # Evolution history for visualizations
            'evolution_history': solution['evolution_history'],
            
            # Metadata
            'timestamp': datetime.now().isoformat(),
            'solver_version': '2.0.0-universal'
        }
    
    def _build_intent_reasoning(self, config: Dict[str, Any], problem_description: str) -> str:
        """Build intent reasoning section"""
        domain_expert = config['domain_expert']
        math_expert = config['math_expert']
        
        return f"""**Why This Approach is Right for Your Problem**

**Domain Context:**
{config['description']}

**Expert Assessment:**
As a {domain_expert['title']}, I recognize this as a {config['problem_type']} problem that requires {math_expert['problem_class']}.

**Why LMEA (LLM-Enhanced Evolutionary Algorithm)?**
- Handles {config['problem_type']} complexity naturally
- No need for perfect mathematical formulation upfront
- Explores solution space intelligently
- Provides mathematically verifiable results

**Approach:**
We'll use evolutionary optimization with domain-specific constraints and objectives, enhanced by LLM-driven problem parsing for accuracy.
"""
    
    def _build_data_provenance(self, parsed_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """Build data provenance section"""
        return {
            'data_required': config['parse_config'].get('required_fields', []),
            'data_provided': list(parsed_data.keys()),
            'data_simulated': [],  # TODO: Track which data was simulated
            'how_data_used': config['parse_config'].get('data_usage_explanation', 'Data is used for optimization constraints and objectives.')
        }
    
    def _build_model_development_section(self, config: Dict[str, Any], parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Build model development section"""
        return {
            'title': 'Model Development & Reasoning',
            'content': f"Developed {config['problem_type']} optimization model using {config['name']} domain expertise.",
            'details': [
                f"Problem Type: {config['problem_type']}",
                f"Entities: {len(parsed_data.get('entities', []))}",
                f"Constraints: {len(config['constraint_config'].get('hard_constraints', []))} hard, {len(config['constraint_config'].get('soft_constraints', []))} soft"
            ]
        }
    
    def _build_mathematical_formulation_section(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Build mathematical formulation section"""
        objective_config = config['objective_config']
        
        return {
            'title': 'Mathematical Formulation',
            'objective': objective_config.get('objective_type', 'multi_objective'),
            'components': objective_config.get('components', []),
            'constraints': config['constraint_config']
        }
    
    def _build_solver_steps_section(self, solution: Dict[str, Any]) -> Dict[str, Any]:
        """Build solver steps section"""
        # Get population size safely
        population_size = 50  # default
        if solution.get('evolution_history') and len(solution['evolution_history']) > 0:
            # evolution_history doesn't store population_size, so use default
            population_size = 50
        
        return {
            'title': 'Solver Execution Steps',
            'steps': [
                f"Initialized population of {population_size} solutions",
                f"Ran {solution.get('generations_run', 0)} generations of evolution",
                f"Applied crossover and mutation operators",
                f"Evaluated {solution.get('generations_run', 0) * population_size} candidate solutions",
                f"Converged to best solution with fitness {solution.get('fitness', 0):.4f}"
            ],
            'duration_seconds': solution.get('duration_seconds', 0)
        }
    
    def _build_sensitivity_section(self, solution: Dict[str, Any], proof_result: Dict[str, Any]) -> Dict[str, Any]:
        """Build sensitivity analysis section"""
        # Extract sensitivity from proof if available
        verified_proofs = proof_result.get('verified_proofs', [])
        if not isinstance(verified_proofs, list):
            verified_proofs = []
        
        sensitivity_proofs = [p for p in verified_proofs if isinstance(p, dict) and 'Sensitivity' in p.get('proof_type', '')]
        
        return {
            'title': 'Constraint & Variable Sensitivity',
            'sensitive_constraints': [],  # TODO: Extract from proof
            'sensitive_variables': [],  # TODO: Extract from proof
            'robustness_score': proof_result.get('trust_score', 0.0)
        }
    
    def _build_solve_results_section(self, solution: Dict[str, Any], parsed_data: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """Build solve results section with narrative"""
        return {
            'title': 'Optimization Results',
            'narrative': f"Successfully optimized {config['name']} problem. Solution achieves fitness score of {solution['fitness']:.4f}.",
            'metrics': config['result_config'].get('primary_metrics', []),
            'solution_summary': solution.get('solution', {})
        }
    
    def _build_visualization_data(self, solution: Dict[str, Any], parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """Build visualization data for sensitivity sliders"""
        return {
            'available_visualizations': ['evolution_timeline', 'solution_space_3d', 'sensitivity_analysis'],
            'evolution_history': solution.get('evolution_history', []),
            'solution_space': []  # TODO: Sample solution space
        }
    
    def _error_response(self, error_message: str, details: str) -> Dict[str, Any]:
        """Return error response"""
        return {
            'status': 'error',
            'error_message': error_message,
            'error_details': details,
            'timestamp': datetime.now().isoformat()
        }

