############################################################
# Example Environment (.env example)
############################################################

# Backend
BACKEND=auto
MODEL_NAME=mlx-community/Qwen3-Embedding-4B-4bit-DWQ
MODEL_PATH=
CROSS_ENCODER_MODEL=

# Reranker (Cross-Encoder) â€” Optional
# Choose one of the following ways to enable reranking:
# 1) Torch CrossEncoder (sentence-transformers)
#    RERANKER_BACKEND=torch
#    CROSS_ENCODER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
#
# 2) MLX Reranker (experimental v1; pooled token embeddings + linear head)
#    RERANKER_BACKEND=mlx
#    RERANKER_MODEL_ID=vserifsaglam/Qwen3-Reranker-4B-4bit-MLX
#
# Auto selection prefers Torch for stability (set RERANKER_BACKEND=auto)
RERANKER_BACKEND=auto
RERANKER_MODEL_ID=
# Alias for convenience; same as RERANKER_MODEL_ID
RERANKER_MODEL_NAME=
# Optional overrides
RERANK_MAX_SEQ_LEN=512
RERANK_BATCH_SIZE=16
# MLX-only experimental options:
# - RERANK_POOLING: mean | cls (default: mean)
# - RERANK_SCORE_NORM: none | sigmoid | minmax (default: none)
#   Use sigmoid to bound scores to [0,1] for schema-constrained clients.
RERANK_POOLING=mean
RERANK_SCORE_NORM=none

# OpenAI compatibility (scores normalization on native path for OpenAI clients)
# true | false (default true)
OPENAI_RERANK_AUTO_SIGMOID=true

# Model Cache & Storage
# MODEL_PATH: Custom path for MLX models (overrides auto cache detection)
# If empty, uses Hugging Face cache or environment variables below:
# TRANSFORMERS_CACHE: Override HF transformers cache location
# HF_HOME: Hugging Face cache home directory  
# Default cache location: ~/.cache/huggingface/hub/
# 
# Examples:
# MODEL_PATH=/path/to/local/models/Qwen3-Embedding-4B-4bit-DWQ
# TRANSFORMERS_CACHE=/custom/cache/transformers
# HF_HOME=/custom/huggingface

# Server
HOST=0.0.0.0
PORT=9000
RELOAD=false

# Performance
BATCH_SIZE=32
MAX_BATCH_SIZE=128
MAX_TEXTS_PER_REQUEST=100
MAX_PASSAGES_PER_RERANK=1000
MAX_SEQUENCE_LENGTH=512
DEVICE_MEMORY_FRACTION=0.8
REQUEST_TIMEOUT=300

# ðŸš€ Text Processing Configuration (NEW!)
# Default text processing options for the service
DEFAULT_AUTO_TRUNCATE=true
DEFAULT_TRUNCATION_STRATEGY=smart_truncate
# DEFAULT_MAX_TOKENS_OVERRIDE=2048
DEFAULT_RETURN_PROCESSING_INFO=false

# Text processing strategies:
# - smart_truncate: Preserve sentence boundaries while truncating (recommended)
# - truncate: Simple token-based truncation
# - extract: Extract key sentences only
# - error: Raise error when token limit is exceeded
#
# Token limits (automatically detected from model metadata):
# - Recommended max tokens: 2048 (auto-truncation trigger)
# - Absolute max tokens: 8192 (hard limit, will raise error)
# - Users can override recommended limit via max_tokens_override (up to absolute max)

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Security (optional)
# ALLOWED_HOSTS=["example.com","api.example.com"]
# ALLOWED_ORIGINS=["https://example.com","https://app.example.com"]

# Copy to .env and adjust as needed.
