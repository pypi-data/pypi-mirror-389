"""
ONNX conversion utilities for MedVision.
"""

import os
import glob
import torch
from typing import Dict, Any, List, Tuple


def generate_triton_config(
    model_name: str,
    input_shape: List[int],
    output_shape: List[int] = None,
    max_batch_size: int = 8,
    data_type: str = "TYPE_FP32"
) -> str:
    """
    ç”Ÿæˆ Triton Inference Server çš„ config.pbtxt é…ç½®æ–‡ä»¶å†…å®¹
    
    Args:
        model_name: æ¨¡å‹åç§°
        input_shape: è¾“å…¥å¼ é‡å½¢çŠ¶ (åŒ…å«batchç»´åº¦)
        output_shape: è¾“å‡ºå¼ é‡å½¢çŠ¶ (åŒ…å«batchç»´åº¦)ï¼Œå¦‚æœä¸ºNoneåˆ™å‡è®¾ä¸è¾“å…¥ç›¸åŒ
        max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
        data_type: æ•°æ®ç±»å‹
        
    Returns:
        str: config.pbtxt é…ç½®æ–‡ä»¶å†…å®¹
    """
    # å»æ‰batchç»´åº¦ï¼ŒTritoné…ç½®ä¸­ä¸åŒ…å«batchç»´åº¦
    input_dims = input_shape[1:]  # å»æ‰ç¬¬ä¸€ä¸ªç»´åº¦(batch)
    output_dims = output_shape[1:] if output_shape else input_dims
    
    config_content = f'''name: "{model_name}"
platform: "onnxruntime_onnx"
max_batch_size: {max_batch_size}

input [
  {{
    name: "input"
    data_type: {data_type}
    dims: [{", ".join(map(str, input_dims))}]
  }}
]

output [
  {{
    name: "output"
    data_type: {data_type}
    dims: [{", ".join(map(str, output_dims))}]
  }}
]

version_policy: {{
  all: {{}}
}}

instance_group [
  {{
    count: 1
    kind: KIND_GPU
  }}
]

optimization: {{
  execution_accelerators: {{
    gpu_execution_accelerator: [
      {{
        name: "tensorrt"
        parameters: {{
          key: "precision_mode"
          value: "FP16"
        }}
        parameters: {{
          key: "max_workspace_size_bytes"
          value: "1073741824"
        }}
      }}
    ]
  }}
}}
'''
    return config_content


def convert_models_to_onnx(
    checkpoint_callback, 
    model_class, 
    config: Dict[str, Any], 
    datamodule
) -> Tuple[List[Dict], str]:
    """
    å°†ä¿å­˜çš„top-kæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
    
    Args:
        checkpoint_callback: ModelCheckpointå›è°ƒå¯¹è±¡
        model_class: æ¨¡å‹ç±»
        config: é…ç½®å­—å…¸
        datamodule: æ•°æ®æ¨¡å—
        
    Returns:
        Tuple[List[Dict], str]: è½¬æ¢æˆåŠŸçš„æ¨¡å‹åˆ—è¡¨å’ŒONNXç›®å½•è·¯å¾„
    """
    # é…ç½®å‚æ•°
    opset_version = config.get("onnx_opset_version", 18)
    
    checkpoint_dir = checkpoint_callback.dirpath
    onnx_dir = os.path.join(os.path.dirname(checkpoint_dir), "onnx_models")
    os.makedirs(onnx_dir, exist_ok=True)
    
    # è·å–æœ€ä½³æ¨¡å‹è·¯å¾„
    best_model_path = checkpoint_callback.best_model_path
    
    if not best_model_path or not os.path.exists(best_model_path):
        print("âŒ No best model found!")
        return [], onnx_dir
    
    checkpoint_files = [best_model_path]  # åªå¤„ç†æœ€ä½³æ¨¡å‹
    
    # è·å–ç¤ºä¾‹è¾“å…¥
    datamodule.setup('fit')
    sample_batch = next(iter(datamodule.train_dataloader()))
    sample_input = sample_batch[0][:1]  # å–ä¸€ä¸ªæ ·æœ¬
    
    converted_models = []
    
    print(f"Converting best model based on monitored metric: {os.path.basename(best_model_path)}")
    print(f"Best model score: {checkpoint_callback.best_model_score}")
    
    # æ£€æŸ¥ç¤ºä¾‹è¾“å…¥çš„è®¾å¤‡
    print(f"Sample input device: {sample_input.device}")
    
    for ckpt_path in checkpoint_files:
        ckpt_name = os.path.splitext(os.path.basename(ckpt_path))[0]
        print(f"\nConverting {ckpt_name}...")
        
        try:
            # åŠ è½½æ¨¡å‹
            model = model_class.load_from_checkpoint(ckpt_path, config=config)
            model.eval()
            
            # æ£€æŸ¥æ¨¡å‹å‚æ•°çš„è®¾å¤‡
            model_device = next(model.parameters()).device
            print(f"  Model loaded on device: {model_device}")
            
            # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPUè¿›è¡ŒONNXè½¬æ¢
            if model_device.type == 'cuda':
                print(f"  Moving model from {model_device} to CPU for ONNX export...")
                model = model.cpu()
            
            # ç¡®ä¿ç¤ºä¾‹è¾“å…¥åœ¨CPUä¸Š
            sample_input_cpu = sample_input.cpu()
            print(f"  Using sample input on device: {sample_input_cpu.device}")
            
            # ONNXæ–‡ä»¶è·¯å¾„
            onnx_path = os.path.join(onnx_dir, f"{ckpt_name}.onnx")
            
            # è·å–è¾“å…¥shapeä¿¡æ¯
            input_shape = sample_input_cpu.shape
            
            # è½¬æ¢ä¸ºONNX
            with torch.no_grad():
                torch.onnx.export(
                    model.net,
                    sample_input_cpu,
                    onnx_path,
                    export_params=True,
                    opset_version=opset_version,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={
                        'input': {0: 'batch_size'},
                        'output': {0: 'batch_size'}
                    },
                    verbose=False,
                    external_data=False
                )
            
            # éªŒè¯ONNXæ¨¡å‹
            output_shape = None
            try:
                import onnx
                onnx_model = onnx.load(onnx_path)
                onnx.checker.check_model(onnx_model)
                print(f"âœ“ ONNX model validation passed: {ckpt_name}")
                
                # è·å–è¾“å‡ºå½¢çŠ¶ä¿¡æ¯
                try:
                    with torch.no_grad():
                        dummy_output = model.net(sample_input_cpu)
                        output_shape = list(dummy_output.shape)
                except Exception as e:
                    print(f"  âš  Could not infer output shape: {e}")
                
            except ImportError:
                print(f"âš  ONNX validation skipped (onnx package not installed): {ckpt_name}")
            except Exception as e:
                print(f"âš  ONNX validation failed: {ckpt_name}, error: {e}")
            
            # ç”Ÿæˆ Triton config.pbtxt
            triton_model_dir = None
            triton_config_path = None
            try:
                # åˆ›å»ºæ¨¡å‹ç›®å½•ï¼ˆTritonè¦æ±‚æ¯ä¸ªæ¨¡å‹æœ‰è‡ªå·±çš„ç›®å½•ï¼‰
                triton_model_dir = os.path.join(onnx_dir, ckpt_name)
                os.makedirs(triton_model_dir, exist_ok=True)
                
                # ç§»åŠ¨ONNXæ–‡ä»¶åˆ°æ¨¡å‹ç›®å½•çš„1ç‰ˆæœ¬å­ç›®å½•
                triton_version_dir = os.path.join(triton_model_dir, "1")
                os.makedirs(triton_version_dir, exist_ok=True)
                
                # å¤åˆ¶ONNXæ–‡ä»¶åˆ°ç‰ˆæœ¬ç›®å½•
                import shutil
                triton_onnx_path = os.path.join(triton_version_dir, "model.onnx")
                shutil.copy2(onnx_path, triton_onnx_path)
                
                # ç”Ÿæˆconfig.pbtxt
                config_content = generate_triton_config(
                    model_name=ckpt_name,
                    input_shape=list(input_shape),
                    output_shape=output_shape,
                    max_batch_size=config.get("triton_max_batch_size", 8),
                    data_type=config.get("triton_data_type", "TYPE_FP32")
                )
                
                triton_config_path = os.path.join(triton_model_dir, "config.pbtxt")
                with open(triton_config_path, 'w', encoding='utf-8') as f:
                    f.write(config_content)
                
                print(f"  âœ“ Generated Triton config: {triton_config_path}")
                
            except Exception as e:
                print(f"  âš  Failed to generate Triton config for {ckpt_name}: {e}")
            
            converted_models.append({
                "checkpoint_path": ckpt_path,
                "onnx_path": onnx_path,
                "triton_model_dir": triton_model_dir,
                "triton_config_path": triton_config_path,
                "model_name": ckpt_name,
                "input_shape": list(input_shape),
                "output_shape": output_shape,
                "original_device": str(model_device) if 'model_device' in locals() else "unknown"
            })
            
            print(f"  âœ“ Successfully converted {ckpt_name} to ONNX with Triton config")
            
        except Exception as e:
            print(f"  âŒ Failed to convert {ckpt_name}: {str(e)}")
            import traceback
            print(f"  Full error traceback:")
            traceback.print_exc()
    
    print(f"\nğŸ“ Best model ONNX and Triton config saved to: {onnx_dir}")
    if converted_models:
        print(f"ğŸ“Š Successfully converted best model: {converted_models[0]['model_name']}")
        print(f"ğŸ“ˆ Best model score: {checkpoint_callback.best_model_score}")
        if converted_models[0].get('triton_config_path'):
            print(f"ğŸš€ Triton config generated: {converted_models[0]['triton_config_path']}")
    else:
        print("âŒ No models were successfully converted")
    
    return converted_models, onnx_dir


def convert_single_model_to_onnx(
    checkpoint_path: str,
    model_class,
    config: Dict[str, Any],
    sample_input: torch.Tensor,
    output_path: str,
    opset_version: int = 18
) -> Dict[str, Any]:
    """
    å°†å•ä¸ªæ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼
    
    Args:
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        model_class: æ¨¡å‹ç±»
        config: æ¨¡å‹é…ç½®å­—å…¸
        sample_input: ç¤ºä¾‹è¾“å…¥å¼ é‡
        output_path: è¾“å‡ºONNXæ–‡ä»¶è·¯å¾„
        opset_version: ONNX opsetç‰ˆæœ¬
        
    Returns:
        Dict[str, Any]: è½¬æ¢ç»“æœä¿¡æ¯
    """
    try:
        # åŠ è½½æ¨¡å‹
        model = model_class.load_from_checkpoint(checkpoint_path, config=config)
        model.eval()
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°çš„è®¾å¤‡
        model_device = next(model.parameters()).device
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPUè¿›è¡ŒONNXè½¬æ¢
        if model_device.type == 'cuda':
            model = model.cpu()
        
        # ç¡®ä¿ç¤ºä¾‹è¾“å…¥åœ¨CPUä¸Š
        sample_input_cpu = sample_input.cpu()
        
        # è·å–è¾“å…¥shapeä¿¡æ¯
        input_shape = sample_input_cpu.shape
        
        # è½¬æ¢ä¸ºONNX
        with torch.no_grad():
            torch.onnx.export(
                model.net,
                sample_input_cpu,
                output_path,
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                },
                verbose=False
            )
        
        # éªŒè¯ONNXæ¨¡å‹
        validation_passed = False
        validation_error = None
        
        try:
            import onnx
            onnx_model = onnx.load(output_path)
            onnx.checker.check_model(onnx_model)
            validation_passed = True
        except ImportError:
            validation_error = "ONNX package not installed"
        except Exception as e:
            validation_error = str(e)
        
        return {
            "success": True,
            "checkpoint_path": checkpoint_path,
            "onnx_path": output_path,
            "input_shape": list(input_shape),
            "validation_passed": validation_passed,
            "validation_error": validation_error
        }
        
    except Exception as e:
        return {
            "success": False,
            "checkpoint_path": checkpoint_path,
            "onnx_path": output_path,
            "error": str(e)
        }


def validate_onnx_model(onnx_path: str) -> Tuple[bool, str]:
    """
    éªŒè¯ONNXæ¨¡å‹çš„æœ‰æ•ˆæ€§
    
    Args:
        onnx_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
        
    Returns:
        Tuple[bool, str]: (æ˜¯å¦éªŒè¯é€šè¿‡, é”™è¯¯ä¿¡æ¯)
    """
    try:
        import onnx
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)
        return True, "Validation passed"
    except ImportError:
        return False, "ONNX package not installed"
    except Exception as e:
        return False, str(e)


def get_onnx_model_info(onnx_path: str) -> Dict[str, Any]:
    """
    è·å–ONNXæ¨¡å‹çš„ä¿¡æ¯
    
    Args:
        onnx_path: ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ¨¡å‹ä¿¡æ¯å­—å…¸
    """
    try:
        import onnx
        model = onnx.load(onnx_path)
        
        # è·å–è¾“å…¥ä¿¡æ¯
        inputs = []
        for input_tensor in model.graph.input:
            input_info = {
                "name": input_tensor.name,
                "type": input_tensor.type.tensor_type.elem_type,
                "shape": [dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim]
            }
            inputs.append(input_info)
        
        # è·å–è¾“å‡ºä¿¡æ¯
        outputs = []
        for output_tensor in model.graph.output:
            output_info = {
                "name": output_tensor.name,
                "type": output_tensor.type.tensor_type.elem_type,
                "shape": [dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim]
            }
            outputs.append(output_info)
        
        return {
            "file_path": onnx_path,
            "file_size": os.path.getsize(onnx_path),
            "opset_version": model.opset_import[0].version if model.opset_import else None,
            "inputs": inputs,
            "outputs": outputs,
            "node_count": len(model.graph.node)
        }
        
    except ImportError:
        return {"error": "ONNX package not installed"}
    except Exception as e:
        return {"error": str(e)}
