"""
Analysis API endpoints.
"""

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from ...core.database import get_db
from ...core.auth import verify_api_key
from ...models.schemas import (
    ToxicityRequest, ToxicityResponse,
    BiasRequest, BiasResponse,
    JailbreakRequest, JailbreakResponse,
    ExplainRequest, ExplainResponse,
    RemediateRequest, RemediateResponse,
    HallucinationRequest, HallucinationResponse,
)
from ...services.toxicity import ToxicityDetector  # Legacy fallback
from ...services.custom_toxicity import get_toxicity_detector  # New custom detector
from ...services.bias import BiasDetector  # Legacy fallback
from ...services.enhanced_bias import get_bias_detector  # New enhanced detector
from ...services.jailbreak import JailbreakDetector  # Legacy fallback
from ...services.advanced_jailbreak import get_jailbreak_detector  # New advanced detector
from ...services.hallucination import get_hallucination_detector  # New hallucination detector
from ...services.credit_manager import CreditManager  # Credit management
from ...services.explainability import ExplainabilityEngine
from ...services.remediation import RemediationEngine
from ...services.custom_classifiers import classifier_registry
from ...services.policy_engine import policy_engine
from ...services.advanced_bias import bias_test_suite
from ...exceptions import InsufficientCreditsException

router = APIRouter()

# Initialize services (use new detectors)
custom_toxicity_detector = get_toxicity_detector()
enhanced_bias_detector = get_bias_detector()
advanced_jailbreak_detector = get_jailbreak_detector()
hallucination_detector = get_hallucination_detector()

# Legacy detectors (fallback)
legacy_toxicity_detector = ToxicityDetector()
legacy_bias_detector = BiasDetector()
legacy_jailbreak_detector = JailbreakDetector()
explainability_engine = ExplainabilityEngine()
remediation_engine = RemediationEngine()


@router.post("/toxicity", response_model=ToxicityResponse)
async def analyze_toxicity(
    request: ToxicityRequest,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Analyze text for toxicity using custom transformer models.
    
    Uses open-source models instead of OpenAI Moderation API.
    Returns toxicity score, flagged status, and categories.
    """
    # Check credits before processing
    credit_manager = CreditManager(db)
    try:
        await credit_manager.check_credits(tenant_id, operation_type="toxicity")
    except InsufficientCreditsException as e:
        raise HTTPException(
            status_code=402,  # Payment Required
            detail={
                "error": "Insufficient credits",
                "required": e.required,
                "available": e.available,
                "balance": e.balance,
            }
        )
    
    result = await custom_toxicity_detector.analyze(request.text)
    
    # Deduct credits after successful processing
    try:
        await credit_manager.deduct_credits(
            tenant_id=tenant_id,
            amount=credit_manager.CREDIT_COSTS["toxicity"],
            operation_type="toxicity",
            reference_type="api_call",
            description="Toxicity analysis",
        )
    except InsufficientCreditsException as e:
        # Should not happen as we checked above, but handle gracefully
        raise HTTPException(
            status_code=402,
            detail={
                "error": "Insufficient credits",
                "required": e.required,
                "available": e.available,
            }
        )
    
    return ToxicityResponse(
        toxicity_score=result["toxicity_score"],
        flagged=result["flagged"],
        categories=result["categories"],
    )


@router.post("/bias", response_model=BiasResponse)
async def analyze_bias(
    request: BiasRequest,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Analyze text for demographic bias using enhanced detection.
    
    Uses stereotype association, fairness metrics, and demographic parity testing.
    """
    # Check credits before processing
    credit_manager = CreditManager(db)
    await credit_manager.check_credits(tenant_id, operation_type="bias")
    
    result = await enhanced_bias_detector.analyze_comprehensive(
        prompt=request.prompt,
        response=request.response,
        demographic_variants=request.variants,
    )
    
    # Deduct credits after successful processing
    await credit_manager.deduct_credits(
        tenant_id=tenant_id,
        amount=credit_manager.CREDIT_COSTS["bias"],
        operation_type="bias",
        reference_type="api_call",
        description="Bias analysis",
    )
    
    return BiasResponse(
        bias_score=result["bias_score"],
        flags=result["flags"],
        details=result.get("details"),
    )


@router.post("/jailbreak", response_model=JailbreakResponse)
async def detect_jailbreak(
    request: JailbreakRequest,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Detect jailbreak attempts and prompt injections using advanced detection.
    
    Uses pattern matching, semantic analysis, behavioral indicators, and adversarial detection.
    """
    # Check credits before processing
    credit_manager = CreditManager(db)
    await credit_manager.check_credits(tenant_id, operation_type="jailbreak")
    
    # Get context if available (user history, etc.)
    context = request.__dict__.get("context", None)
    
    result = await advanced_jailbreak_detector.detect(request.text, context)
    
    # Deduct credits after successful processing
    await credit_manager.deduct_credits(
        tenant_id=tenant_id,
        amount=credit_manager.CREDIT_COSTS["jailbreak"],
        operation_type="jailbreak",
        reference_type="api_call",
        description="Jailbreak detection",
    )
    
    return JailbreakResponse(
        jailbreak_flag=result["jailbreak_flag"],
        confidence=result["confidence"],
        patterns_detected=result.get("components", {}).get("patterns_found", []),
    )


@router.post("/hallucination", response_model=HallucinationResponse)
async def detect_hallucination(
    request: HallucinationRequest,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Detect hallucinations in LLM responses.
    
    Uses semantic consistency, contradiction detection, factuality checking,
    pattern analysis, and confidence calibration.
    """
    # Check credits before processing
    credit_manager = CreditManager(db)
    await credit_manager.check_credits(tenant_id, operation_type="hallucination")
    
    result = await hallucination_detector.detect(
        prompt=request.prompt,
        response=request.response,
        context=request.context,
    )
    
    # Deduct credits after successful processing
    await credit_manager.deduct_credits(
        tenant_id=tenant_id,
        amount=credit_manager.CREDIT_COSTS["hallucination"],
        operation_type="hallucination",
        reference_type="api_call",
        description="Hallucination detection",
    )
    
    return HallucinationResponse(
        hallucination_score=result["hallucination_score"],
        flagged=result["flagged"],
        confidence=result["confidence"],
        recommendation=result["recommendation"],
        components=result.get("components", {}),
    )


@router.post("/explain", response_model=ExplainResponse)
async def explain_issues(
    request: ExplainRequest,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Generate explanation for flagged content.
    
    Highlights problematic spans and provides rationale.
    """
    result = await explainability_engine.explain(
        text=request.text,
        issues=request.issues,
    )
    
    return ExplainResponse(
        explanation=result["explanation"],
        highlighted_spans=result["highlighted_spans"],
    )


@router.post("/remediate", response_model=RemediateResponse)
async def remediate_content(
    request: RemediateRequest,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Remediate unsafe content.
    
    Returns a safer version of the text while preserving intent.
    """
    result = await remediation_engine.remediate(
        text=request.text,
        issue=request.issue,
    )
    
    return RemediateResponse(
        remediated_text=result["remediated_text"],
        original_score=result["original_score"],
        new_score=result["new_score"],
        changes_made=result["changes_made"],
    )


@router.post("/custom-classify")
async def custom_classify(
    request: dict,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Run custom classifiers (PII, misinformation, compliance, etc.).
    
    Goes beyond standard toxicity to detect:
    - PII (email, phone, SSN, credit cards)
    - Misinformation patterns
    - Unprofessional content
    - Compliance violations (HIPAA, GDPR)
    """
    text = request.get("text", "")
    enabled_classifiers = request.get("classifiers", None)
    
    result = await classifier_registry.classify_all(text, enabled_classifiers)
    
    return {
        "overall_score": result["overall_score"],
        "flagged": result["flagged"],
        "results": result["classifier_results"],
        "available_classifiers": classifier_registry.list_classifiers(),
    }


@router.post("/policy-check")
async def check_policy(
    request: dict,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Check text against tenant-specific policy rules.
    
    Supports custom rules for:
    - Industry-specific compliance (HIPAA, SEC, FINRA)
    - Corporate communication standards
    - Domain-specific keywords/patterns
    - Confidential information detection
    """
    text = request.get("text", "")
    
    result = policy_engine.evaluate_policy(tenant_id, text)
    
    return result


@router.post("/advanced-bias-test")
async def advanced_bias_test(
    request: dict,
    tenant_id: str = Depends(verify_api_key),
    db: AsyncSession = Depends(get_db),
):
    """
    Run comprehensive paired demographic bias testing.
    
    Tests actual LLM responses across:
    - Gender variants (he/she/they)
    - Race/ethnicity variants
    - Age variants
    - Professional context pronoun usage
    
    Returns detailed bias analysis with variance scores.
    """
    base_prompt = request.get("prompt", "")
    test_demographics = request.get("demographics", ["gender"])
    model = request.get("model", "gpt-3.5-turbo")
    
    result = await bias_test_suite.run_comprehensive_bias_tests(
        base_prompt,
        test_demographics,
        model,
    )
    
    return result


@router.get("/classifiers")
async def list_classifiers(
    tenant_id: str = Depends(verify_api_key),
):
    """
    List all available custom classifiers.
    
    Returns:
        - PII detector
        - Misinformation detector
        - Professional context classifier
        - Compliance classifier
        - Any custom tenant classifiers
    """
    return {
        "classifiers": classifier_registry.list_classifiers(),
        "description": {
            "pii_detector": "Detects personally identifiable information (email, phone, SSN, etc.)",
            "misinfo_detector": "Detects misinformation patterns and claims",
            "professional_context": "Flags unprofessional language in business contexts",
            "compliance": "Detects compliance-sensitive content (HIPAA, legal, financial)",
        },
    }


@router.get("/policy/{tenant_id}")
async def get_tenant_policy(
    tenant_id: str,
    auth_tenant_id: str = Depends(verify_api_key),
):
    """
    Get tenant's custom policy configuration.
    
    Returns configured rules, thresholds, and enabled features.
    """
    # Verify tenant access
    if tenant_id != auth_tenant_id:
        raise HTTPException(status_code=403, detail="Access denied to this tenant's policy")
    
    policy = policy_engine.get_policy(tenant_id)
    
    if not policy:
        return {
            "message": "No custom policy configured",
            "default_policy": True,
        }
    
    return {
        "tenant_id": policy.tenant_id,
        "name": policy.name,
        "description": policy.description,
        "enabled": policy.enabled,
        "rules": [
            {
                "id": rule.id,
                "name": rule.name,
                "description": rule.description,
                "action": rule.action,
                "severity": rule.severity,
                "enabled": rule.enabled,
            }
            for rule in policy.rules
        ],
        "thresholds": {
            "toxicity": policy.toxicity_threshold,
            "bias": policy.bias_threshold,
            "pii": policy.pii_threshold,
        },
        "features": {
            "toxicity": policy.enable_toxicity,
            "bias": policy.enable_bias,
            "jailbreak": policy.enable_jailbreak,
            "pii": policy.enable_pii,
            "custom_classifiers": policy.enable_custom_classifiers,
        },
    }
