"""
Comprehensive Tool Calling Benchmark with Response Validation

Tests 105 tool calling scenarios with:
- Real tool execution
- Response correctness validation
- Routing pattern analysis
- Small model accuracy measurement
- Ground truth comparison

Validates:
1. Tool selection correctness (did model pick right tool?)
2. Parameter extraction accuracy (are params correct?)
3. Routing effectiveness (cascade vs direct)
4. Small model reliability by complexity level
"""

import asyncio
import json
import time
from collections import defaultdict
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# Import tool infrastructure
from tool_call_dataset import ALL_TOOL_CALLS, ToolCallQuery
from tools_real_world import execute_tool_call, validate_tool_call


@dataclass
class ToolCallResult:
    """Result of a single tool call test."""
    query_id: str
    complexity: str
    category: str
    query: str

    # Expected (ground truth)
    expected_tool: str
    expected_routing: str

    # Actual results
    model_used: str
    provider_used: str
    actual_tool_called: Optional[str]
    actual_parameters: Optional[Dict[str, Any]]

    # Correctness validation
    tool_selection_correct: bool
    parameters_valid: bool
    tool_executed: bool
    execution_error: Optional[str]

    # Routing validation
    routing_correct: bool
    cascade_attempts: int

    # Performance
    latency_ms: float
    cost_usd: float
    confidence: float

    # Response quality
    response_text: str
    response_length: int


@dataclass
class ToolBenchmarkSummary:
    """Summary statistics for tool benchmark."""
    total_queries: int = 0

    # Tool selection accuracy
    tool_selection_correct: int = 0
    tool_selection_accuracy: float = 0.0

    # Parameter extraction
    parameters_valid: int = 0
    parameter_accuracy: float = 0.0

    # Execution success
    successful_executions: int = 0
    execution_success_rate: float = 0.0

    # Routing correctness
    routing_correct: int = 0
    routing_accuracy: float = 0.0

    # By complexity
    accuracy_by_complexity: Dict[str, Dict[str, float]] = None

    # By model
    accuracy_by_model: Dict[str, Dict[str, float]] = None

    # Cost analysis
    total_cost: float = 0.0
    avg_cost: float = 0.0
    cost_if_always_premium: float = 0.0
    cost_savings_pct: float = 0.0

    # Performance
    avg_latency: float = 0.0
    cascade_vs_direct: Dict[str, Any] = None


class ComprehensiveToolBenchmark:
    """Comprehensive tool calling benchmark with validation."""

    def __init__(self, test_subset: Optional[int] = None):
        """
        Initialize benchmark.

        Args:
            test_subset: If set, only test first N queries
        """
        self.queries = ALL_TOOL_CALLS[:test_subset] if test_subset else ALL_TOOL_CALLS
        self.results: List[ToolCallResult] = []

        print(f"\n{'='*80}")
        print("COMPREHENSIVE TOOL CALLING BENCHMARK")
        print(f"{'='*80}")
        print(f"\nInitialized with {len(self.queries)} tool calling scenarios:")

        by_complexity = defaultdict(int)
        by_category = defaultdict(int)
        for q in self.queries:
            by_complexity[q.complexity] += 1
            by_category[q.category] += 1

        print(f"\nBy Complexity:")
        for complexity in ["trivial", "simple", "moderate", "hard", "expert"]:
            count = by_complexity[complexity]
            if count > 0:
                print(f"  {complexity.capitalize():10s}: {count:3d} queries")

        print(f"\nBy Category:")
        for category, count in sorted(by_category.items()):
            print(f"  {category:20s}: {count:3d} queries")
        print()

    async def run_benchmark(self):
        """Run comprehensive tool calling benchmark."""
        try:
            from cascadeflow import CascadeAgent
            from cascadeflow.schema.config import ModelConfig
        except ImportError as e:
            print(f"âŒ Could not import cascadeflow: {e}")
            return

        # Setup cascade with tool-capable models
        print("ðŸ“Š Setting up cascade configuration...")
        cascade = CascadeAgent(
            models=[
                # Tier 1: Cheap (for trivial tool calls)
                ModelConfig(
                    name="llama-3.1-8b-instant",
                    provider="groq",
                    cost=0.05,
                    supports_tools=True
                ),
                # Tier 2: Balanced (for simple tool calls)
                ModelConfig(
                    name="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                    provider="together",
                    cost=0.10,
                    supports_tools=True
                ),
                # Tier 3: Quality (for moderate/complex)
                ModelConfig(
                    name="gpt-4o-mini",
                    provider="openai",
                    cost=0.60,
                    supports_tools=True
                ),
            ]
        )

        print("\nâœ… Cascade configured:")
        print("  Tier 1: Groq Llama-3.1-8B (trivial)")
        print("  Tier 2: Together Llama-3.1-8B (simple)")
        print("  Tier 3: OpenAI GPT-4o-mini (complex/expert)")
        print()

        # Test each complexity level
        for complexity in ["trivial", "simple", "moderate", "hard", "expert"]:
            queries = [q for q in self.queries if q.complexity == complexity]
            if not queries:
                continue

            print(f"\n{'='*80}")
            print(f"Testing {complexity.upper()} tool calls ({len(queries)} queries)")
            print(f"{'='*80}\n")

            for i, query in enumerate(queries, 1):
                await self._test_tool_call(cascade, query, i, len(queries))

        # Analyze results
        self.summary = self._analyze_results()
        self._generate_report()

    async def _test_tool_call(
        self,
        cascade: Any,
        query: ToolCallQuery,
        index: int,
        total: int
    ):
        """Test a single tool call."""
        try:
            start = time.time()

            # Run query through cascade with tools
            result = await cascade.run(
                query.query,
                tools=query.tools,
                max_tokens=query.max_tokens
            )

            latency_ms = (time.time() - start) * 1000

            # Extract tool call information
            model_used = getattr(result, 'model_used', 'unknown')
            provider_used = self._extract_provider(model_used)

            # Check if tool was called
            tool_calls = getattr(result, 'tool_calls', None)
            actual_tool = None
            actual_params = None
            tool_executed = False
            execution_error = None

            if tool_calls and len(tool_calls) > 0:
                # Get first tool call
                first_call = tool_calls[0]
                actual_tool = first_call.get('name') or first_call.get('function', {}).get('name')
                # FIX: cascadeflow returns 'arguments' not 'parameters'
                actual_params = first_call.get('arguments') or first_call.get('parameters') or first_call.get('function', {}).get('arguments', {})

                # Try to execute the tool
                if actual_tool and actual_params:
                    # Validate before execution
                    is_valid, error_msg = validate_tool_call(actual_tool, actual_params)
                    if is_valid:
                        # Execute tool
                        tool_result = execute_tool_call(actual_tool, actual_params)
                        if tool_result.get('success', False) or 'error' not in tool_result:
                            tool_executed = True
                        else:
                            execution_error = tool_result.get('error')
                    else:
                        execution_error = error_msg

            # Validate correctness
            tool_selection_correct = (actual_tool == query.expected_tool)
            parameters_valid = bool(actual_params and not execution_error)

            # Validate routing
            routing_correct = self._validate_routing(query.complexity, query.expected_routing, model_used)
            cascade_attempts = getattr(result, 'cascade_attempts', 1)

            # Create result
            test_result = ToolCallResult(
                query_id=query.id,
                complexity=query.complexity,
                category=query.category,
                query=query.query,
                expected_tool=query.expected_tool,
                expected_routing=query.expected_routing,
                model_used=model_used,
                provider_used=provider_used,
                actual_tool_called=actual_tool,
                actual_parameters=actual_params,
                tool_selection_correct=tool_selection_correct,
                parameters_valid=parameters_valid,
                tool_executed=tool_executed,
                execution_error=execution_error,
                routing_correct=routing_correct,
                cascade_attempts=cascade_attempts,
                latency_ms=latency_ms,
                cost_usd=getattr(result, 'total_cost', 0.0),
                confidence=getattr(result, 'confidence', 0.0),
                response_text=result.content[:200] if hasattr(result, 'content') else "",
                response_length=len(result.content) if hasattr(result, 'content') else 0
            )

            self.results.append(test_result)

            # Print progress
            status = "âœ“" if (tool_selection_correct and parameters_valid) else "âœ—"
            tool_status = f"{actual_tool[:15]:15s}" if actual_tool else "NO_TOOL        "
            print(
                f"  {status} [{index:3d}/{total:3d}] {query.id[:25]:25s} "
                f"â†’ {tool_status} {model_used[:20]:20s} {latency_ms:6.0f}ms"
            )

        except Exception as e:
            print(f"  âœ— [{index:3d}/{total:3d}] {query.id[:25]:25s} ERROR: {str(e)[:50]}")

    def _extract_provider(self, model_name: str) -> str:
        """Extract provider from model name."""
        model_lower = model_name.lower()
        if "groq" in model_lower or "llama-3.1-8b-instant" in model_lower:
            return "groq"
        elif "together" in model_lower:
            return "together"
        elif "gpt" in model_lower or "openai" in model_lower:
            return "openai"
        elif "claude" in model_lower or "anthropic" in model_lower:
            return "anthropic"
        return "unknown"

    def _validate_routing(self, complexity: str, expected_routing: str, model_used: str) -> bool:
        """Validate if routing decision was correct."""
        model_lower = model_used.lower()

        # Check expected routing
        if expected_routing == "direct_premium":
            # Should use premium model (GPT-4o-mini)
            return "gpt" in model_lower or "openai" in model_lower
        else:  # cascade
            # Can use any model, cascade will validate
            return True

    def _analyze_results(self):
        """Analyze benchmark results."""
        if not self.results:
            return ToolBenchmarkSummary()  # FIX: Return empty summary

        summary = ToolBenchmarkSummary()
        summary.total_queries = len(self.results)

        # Tool selection accuracy
        summary.tool_selection_correct = sum(1 for r in self.results if r.tool_selection_correct)
        summary.tool_selection_accuracy = (summary.tool_selection_correct / summary.total_queries * 100)

        # Parameter validation
        summary.parameters_valid = sum(1 for r in self.results if r.parameters_valid)
        summary.parameter_accuracy = (summary.parameters_valid / summary.total_queries * 100)

        # Execution success
        summary.successful_executions = sum(1 for r in self.results if r.tool_executed)
        summary.execution_success_rate = (summary.successful_executions / summary.total_queries * 100)

        # Routing correctness
        summary.routing_correct = sum(1 for r in self.results if r.routing_correct)
        summary.routing_accuracy = (summary.routing_correct / summary.total_queries * 100)

        # By complexity
        accuracy_by_complexity = {}
        for complexity in ["trivial", "simple", "moderate", "hard", "expert"]:
            complexity_results = [r for r in self.results if r.complexity == complexity]
            if complexity_results:
                accuracy_by_complexity[complexity] = {
                    "total": len(complexity_results),
                    "tool_selection": sum(1 for r in complexity_results if r.tool_selection_correct) / len(complexity_results) * 100,
                    "parameters": sum(1 for r in complexity_results if r.parameters_valid) / len(complexity_results) * 100,
                    "execution": sum(1 for r in complexity_results if r.tool_executed) / len(complexity_results) * 100,
                    "routing": sum(1 for r in complexity_results if r.routing_correct) / len(complexity_results) * 100,
                }
        summary.accuracy_by_complexity = accuracy_by_complexity

        # By model
        accuracy_by_model = {}
        for provider in ["groq", "together", "openai"]:
            model_results = [r for r in self.results if r.provider_used == provider]
            if model_results:
                accuracy_by_model[provider] = {
                    "total": len(model_results),
                    "tool_selection": sum(1 for r in model_results if r.tool_selection_correct) / len(model_results) * 100,
                    "parameters": sum(1 for r in model_results if r.parameters_valid) / len(model_results) * 100,
                    "execution": sum(1 for r in model_results if r.tool_executed) / len(model_results) * 100,
                }
        summary.accuracy_by_model = accuracy_by_model

        # Cost analysis
        summary.total_cost = sum(r.cost_usd for r in self.results)
        summary.avg_cost = summary.total_cost / summary.total_queries if summary.total_queries > 0 else 0
        # Estimate if always premium (GPT-4o-mini at $0.60/1M)
        summary.cost_if_always_premium = summary.total_cost * 12  # Conservative 12x multiplier
        if summary.cost_if_always_premium > 0:
            summary.cost_savings_pct = ((summary.cost_if_always_premium - summary.total_cost) /
                                       summary.cost_if_always_premium * 100)

        # Performance
        summary.avg_latency = sum(r.latency_ms for r in self.results) / summary.total_queries if summary.total_queries > 0 else 0

        # Cascade vs direct
        cascade_results = [r for r in self.results if r.cascade_attempts > 1 or r.provider_used in ["groq", "together"]]
        direct_results = [r for r in self.results if r.provider_used == "openai"]

        summary.cascade_vs_direct = {
            "cascade": {
                "count": len(cascade_results),
                "avg_latency": sum(r.latency_ms for r in cascade_results) / len(cascade_results) if cascade_results else 0,
                "accuracy": sum(1 for r in cascade_results if r.tool_selection_correct and r.parameters_valid) / len(cascade_results) * 100 if cascade_results else 0,
            },
            "direct": {
                "count": len(direct_results),
                "avg_latency": sum(r.latency_ms for r in direct_results) / len(direct_results) if direct_results else 0,
                "accuracy": sum(1 for r in direct_results if r.tool_selection_correct and r.parameters_valid) / len(direct_results) * 100 if direct_results else 0,
            }
        }

        return summary  # FIX: Return the summary object

    def _generate_report(self):
        """Generate comprehensive report."""
        print(f"\n{'='*80}")
        print("TOOL CALLING BENCHMARK RESULTS")
        print(f"{'='*80}\n")

        s = self.summary

        print("ðŸ“Š Overall Statistics")
        print("-" * 80)
        print(f"Total queries tested:        {s.total_queries}")
        print(f"Tool selection correct:      {s.tool_selection_correct:3d} ({s.tool_selection_accuracy:.1f}%)")
        print(f"Parameters valid:            {s.parameters_valid:3d} ({s.parameter_accuracy:.1f}%)")
        print(f"Successful executions:       {s.successful_executions:3d} ({s.execution_success_rate:.1f}%)")
        print(f"Routing correct:             {s.routing_correct:3d} ({s.routing_accuracy:.1f}%)")
        print()

        print("ðŸŽ¯ Accuracy by Complexity Level")
        print("-" * 80)
        print(f"{'Complexity':<12} {'Total':>6} {'Tool':>8} {'Params':>8} {'Exec':>8} {'Routing':>8}")
        print("-" * 80)
        for complexity in ["trivial", "simple", "moderate", "hard", "expert"]:
            if complexity in s.accuracy_by_complexity:
                stats = s.accuracy_by_complexity[complexity]
                print(
                    f"{complexity.capitalize():<12} {stats['total']:>6} "
                    f"{stats['tool_selection']:>7.1f}% {stats['parameters']:>7.1f}% "
                    f"{stats['execution']:>7.1f}% {stats['routing']:>7.1f}%"
                )
        print()

        print("ðŸ¤– Accuracy by Model/Provider")
        print("-" * 80)
        print(f"{'Provider':<12} {'Total':>6} {'Tool Selection':>15} {'Parameters':>12} {'Execution':>12}")
        print("-" * 80)
        for provider in ["groq", "together", "openai"]:
            if provider in s.accuracy_by_model:
                stats = s.accuracy_by_model[provider]
                print(
                    f"{provider.capitalize():<12} {stats['total']:>6} "
                    f"{stats['tool_selection']:>14.1f}% {stats['parameters']:>11.1f}% "
                    f"{stats['execution']:>11.1f}%"
                )
        print()

        print("ðŸ’° Cost Analysis")
        print("-" * 80)
        print(f"Total cost (cascade):        ${s.total_cost:.6f}")
        print(f"Avg cost per query:          ${s.avg_cost:.6f}")
        print(f"Est. cost (always premium):  ${s.cost_if_always_premium:.6f}")
        print(f"Cost savings:                ${s.cost_if_always_premium - s.total_cost:.6f} ({s.cost_savings_pct:.1f}%)")
        print()

        print("âš¡ Performance Analysis")
        print("-" * 80)
        print(f"Average latency:             {s.avg_latency:.0f}ms")
        if s.cascade_vs_direct:
            print(f"\nCascade routing ({s.cascade_vs_direct['cascade']['count']} queries):")
            print(f"  Avg latency: {s.cascade_vs_direct['cascade']['avg_latency']:.0f}ms")
            print(f"  Accuracy:    {s.cascade_vs_direct['cascade']['accuracy']:.1f}%")
            print(f"\nDirect routing ({s.cascade_vs_direct['direct']['count']} queries):")
            print(f"  Avg latency: {s.cascade_vs_direct['direct']['avg_latency']:.0f}ms")
            print(f"  Accuracy:    {s.cascade_vs_direct['direct']['accuracy']:.1f}%")
        print()

        # Save results
        self._save_results()

    def _save_results(self):
        """Save detailed results to files."""
        output_dir = Path("benchmark_results")
        output_dir.mkdir(exist_ok=True)

        # Save detailed results
        with open(output_dir / "tool_calling_results.json", "w") as f:
            json.dump([asdict(r) for r in self.results], f, indent=2)

        # Save summary
        summary_dict = asdict(self.summary)
        with open(output_dir / "tool_calling_summary.json", "w") as f:
            json.dump(summary_dict, f, indent=2)

        print(f"âœ… Results saved to: {output_dir}")
        print(f"   - tool_calling_results.json")
        print(f"   - tool_calling_summary.json")


async def main():
    """Run comprehensive tool calling benchmark."""
    import argparse

    parser = argparse.ArgumentParser(description="Comprehensive Tool Calling Benchmark")
    parser.add_argument("--subset", type=int, help="Test only first N queries")
    args = parser.parse_args()

    benchmark = ComprehensiveToolBenchmark(test_subset=args.subset)
    await benchmark.run_benchmark()


if __name__ == "__main__":
    asyncio.run(main())
