# SPDX-FileCopyrightText: Copyright 2022-2025, Arm Limited and/or its affiliates.
# SPDX-License-Identifier: Apache-2.0
"""Module for backend integration."""
from __future__ import annotations

import base64
import json
import logging
import re
import subprocess  # nosec
from dataclasses import dataclass
from pathlib import Path

from mlia.backend.errors import BackendExecutionFailed
from mlia.backend.repo import get_backend_repository
from mlia.utils.filesystem import get_mlia_resources
from mlia.utils.proc import Command
from mlia.utils.proc import OutputLogger
from mlia.utils.proc import process_command_output


logger = logging.getLogger(__name__)

TARGET_METRIC_MAPS = {
    "default": [
        "NPU ACTIVE",
        "NPU IDLE",
        "NPU TOTAL",
        "NPU AXI0_RD_DATA_BEAT_RECEIVED",
        "NPU AXI0_WR_DATA_BEAT_WRITTEN",
        "NPU AXI1_RD_DATA_BEAT_RECEIVED",
    ],
    "corstone-320": [
        "NPU ACTIVE",
        "NPU IDLE",
        "NPU TOTAL",
        "NPU ETHOSU_PMU_SRAM_RD_DATA_BEAT_RECEIVED",
        "NPU ETHOSU_PMU_SRAM_WR_DATA_BEAT_WRITTEN",
        "NPU ETHOSU_PMU_EXT_RD_DATA_BEAT_RECEIVED",
        "NPU ETHOSU_PMU_EXT_WR_DATA_BEAT_WRITTEN",
    ],
}


@dataclass
class PerformanceMetrics:
    """Performance metrics parsed from generic inference output."""

    npu_active_cycles: int
    npu_idle_cycles: int
    npu_total_cycles: int
    npu_axi0_rd_data_beat_received: int
    npu_axi0_wr_data_beat_written: int
    npu_axi1_rd_data_beat_received: int
    npu_axi1_wr_data_beat_written: int | None = None


class GenericInferenceOutputParser:
    """Generic inference runner output parser."""

    pattern = re.compile(r"<metrics>(.*)</metrics>")

    def __init__(self) -> None:
        """Init parser."""
        self.base64_data: list[str] = []

    def __call__(self, line: str) -> None:
        """Extract base64 strings from the app output."""
        if res_b64 := self.pattern.search(line):
            self.base64_data.append(res_b64.group(1))

    def get_metrics(self, target: str = "default") -> PerformanceMetrics:
        """Parse the collected data and return perf metrics."""
        try:
            parsed_metrics = self._parse_data()

            metric_names = TARGET_METRIC_MAPS.get(target, TARGET_METRIC_MAPS["default"])

            metrics_kwargs = {}
            field_names = [
                f.name
                for f in PerformanceMetrics.__dataclass_fields__.values()  # pylint: disable=no-member
            ]
            for idx, metric_name in enumerate(metric_names):
                # Only add if metric exists in parsed_metrics and field exists
                if metric_name in parsed_metrics and idx < len(field_names):
                    metrics_kwargs[field_names[idx]] = parsed_metrics[metric_name]
                else:
                    raise KeyError(f"Metric {metric_name} not found in parsed data.")

            return PerformanceMetrics(**metrics_kwargs)
        except Exception as err:
            raise ValueError("Unable to parse output and get metrics.") from err

    def _parse_data(self) -> dict[str, int]:
        """Parse the data."""
        parsed_metrics: dict[str, int] = {}

        for base64_item in self.base64_data:
            res_json = base64.b64decode(base64_item, validate=True)

            for profiling_group in json.loads(res_json):
                for metric in profiling_group["samples"]:
                    metric_name = metric["name"]
                    metric_value = int(metric["value"][0])

                    if metric_name in parsed_metrics:
                        raise KeyError(f"Duplicate key {metric_name}")

                    parsed_metrics[metric_name] = metric_value

        return parsed_metrics


@dataclass
class FVPMetadata:
    """Metadata for FVP."""

    executable: str
    generic_inf_app: Path


def get_generic_inference_app_path(fvp: str, target: str) -> Path:
    """Return path to the generic inference runner binary."""
    apps_path = get_mlia_resources() / "backends/applications"

    fvp_mapping = {"corstone-300": "300", "corstone-310": "310", "corstone-320": "320"}
    target_mapping = {"ethos-u55": "U55", "ethos-u65": "U65", "ethos-u85": "U85"}

    fvp_version = f"sse-{fvp_mapping[fvp]}"
    app_version = f"22.08.02-ethos-{target_mapping[target]}-Default-noTA"

    app_dir = f"inference_runner-{fvp_version}-{app_version}"
    return apps_path.joinpath(app_dir, "ethos-u-inference_runner.axf")


def get_executable_name(fvp: str, profile: str, target: str) -> str:
    """Return name of the executable for selected FVP and profile."""
    executable_name_mapping = {
        ("corstone-300", "AVH", "ethos-u55"): "VHT_Corstone_SSE-300_Ethos-U55",
        ("corstone-300", "AVH", "ethos-u65"): "VHT_Corstone_SSE-300_Ethos-U65",
        ("corstone-300", "default", "ethos-u55"): "FVP_Corstone_SSE-300_Ethos-U55",
        ("corstone-300", "default", "ethos-u65"): "FVP_Corstone_SSE-300_Ethos-U65",
        ("corstone-310", "AVH", "ethos-u55"): "VHT_Corstone_SSE-310",
        ("corstone-310", "AVH", "ethos-u65"): "VHT_Corstone_SSE-310_Ethos-U65",
        ("corstone-310", "default", "ethos-u55"): "FVP_Corstone_SSE-310",
        ("corstone-310", "default", "ethos-u65"): "FVP_Corstone_SSE-310_Ethos-U65",
        ("corstone-320", "AVH", "ethos-u85"): "VHT_Corstone_SSE-320",
        ("corstone-320", "default", "ethos-u85"): "FVP_Corstone_SSE-320",
    }

    return executable_name_mapping[(fvp, profile, target)]


def get_fvp_metadata(fvp: str, profile: str, target: str) -> FVPMetadata:
    """Return metadata for selected Corstone backend."""
    executable_name = get_executable_name(fvp, profile, target)

    app = get_generic_inference_app_path(fvp, target)

    return FVPMetadata(executable_name, app)


@dataclass
class CorstoneRunConfig:
    """Configuration for running Corstone FVP generic inference."""

    backend_path: Path
    fvp: str
    target: str
    mac: int
    model: Path
    profile: str = "default"


def build_corstone_command(cfg: CorstoneRunConfig) -> Command:
    """Build command to run Corstone FVP."""
    fvp_metadata = get_fvp_metadata(cfg.fvp, cfg.profile, cfg.target)

    if cfg.fvp == "corstone-320":
        cmd = [
            cfg.backend_path.joinpath(fvp_metadata.executable).as_posix(),
            "-a",
            fvp_metadata.generic_inf_app.as_posix(),
            "--data",
            f"{cfg.model}@0x90000000",
            "-C",
            f"mps4_board.subsystem.ethosu.num_macs={cfg.mac}",
            "-C",
            "mps4_board.telnetterminal0.start_telnet=0",
            "-C",
            "mps4_board.uart0.out_file='-'",
            "-C",
            "mps4_board.uart0.shutdown_on_eot=1",
            "-C",
            "mps4_board.visualisation.disable-visualisation=1",
            "-C",
            "vis_hdlcd.disable_visualisation=1",
            "--stat",
        ]
    else:
        cmd = [
            cfg.backend_path.joinpath(fvp_metadata.executable).as_posix(),
            "-a",
            fvp_metadata.generic_inf_app.as_posix(),
            "--data",
            f"{cfg.model}@0x90000000",
            "-C",
            f"ethosu.num_macs={cfg.mac}",
            "-C",
            "mps3_board.telnetterminal0.start_telnet=0",
            "-C",
            "mps3_board.uart0.out_file='-'",
            "-C",
            "mps3_board.uart0.shutdown_on_eot=1",
            "-C",
            "mps3_board.visualisation.disable-visualisation=1",
            "--stat",
        ]
    return Command(cmd)


def get_metrics(cfg: CorstoneRunConfig) -> PerformanceMetrics:
    """Run generic inference and return perf metrics."""
    try:
        command = build_corstone_command(cfg)
    except Exception as err:  # noqa: BLE001 - we want to wrap any construction errors
        raise BackendExecutionFailed(
            f"Unable to construct a command line for {cfg.fvp}"
        ) from err

    output_parser = GenericInferenceOutputParser()
    output_logger = OutputLogger(logger)

    try:
        process_command_output(
            command,
            [output_parser, output_logger],
        )
    except subprocess.CalledProcessError as err:
        raise BackendExecutionFailed("Backend execution failed.") from err

    return output_parser.get_metrics(cfg.fvp)


def estimate_performance(
    target: str, mac: int, model: Path, backend: str
) -> PerformanceMetrics:
    """Get performance estimations."""
    backend_repo = get_backend_repository()
    backend_path, settings = backend_repo.get_backend_settings(backend)

    if not settings or "profile" not in settings:
        raise BackendExecutionFailed(f"Unable to configure backend {backend}.")

    cfg = CorstoneRunConfig(
        backend_path=backend_path,
        fvp=backend,
        target=target,
        mac=mac,
        model=model,
        profile=settings["profile"],
    )
    return get_metrics(cfg)
